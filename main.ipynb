{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb886d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b019739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a884230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c36fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dc2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41faf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01120acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6628e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pre_train_data.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b2f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ddeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a1d71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "        text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None # don't return anything if image cannot be loaded\n",
    "    def encode_text(self, example):\n",
    "        text = self.tokenizer(example, padding='max_length', max_length=100, add_special_tokens=True) # hard-coded max_length for now\n",
    "        bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "         # add a bos token as well\n",
    "        text = {\n",
    "            \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "            \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "00981577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "812a3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    input_ids = [t[\"input_ids\"] for t in texts]\n",
    "    attention_mask = [t[\"attention_mask\"] for t in texts]\n",
    "    \n",
    "    return images, {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f3b9ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=2, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "835b015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7137, 0.7176, 0.5176,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.6431, 0.6902, 0.5686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.5490, 0.5961, 0.6000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.6392, 0.6275, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.7216, 0.6471, 0.5725,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.7725, 0.6510, 0.6353,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[0.9529, 0.9725, 0.8510,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9451, 0.9843, 0.9255,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9451, 0.9961, 0.9765,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9725, 0.9843, 0.9059,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9804, 0.9451, 0.9176,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9333, 0.8588, 0.9294,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[0.7922, 0.8157, 0.6471,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.7725, 0.8392, 0.7137,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.7373, 0.8118, 0.8000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.7804, 0.7922, 0.6863,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.8039, 0.7529, 0.7059,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.8000, 0.6941, 0.7373,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1451, 0.1529, 0.1176,  ..., 0.3333, 0.3412, 0.3529],\n",
      "          [0.1569, 0.1569, 0.1176,  ..., 0.3216, 0.3216, 0.3333],\n",
      "          [0.1686, 0.1686, 0.1255,  ..., 0.3216, 0.3176, 0.3412],\n",
      "          ...,\n",
      "          [0.2000, 0.1961, 0.1922,  ..., 0.6627, 0.7412, 0.6588],\n",
      "          [0.2000, 0.1961, 0.1922,  ..., 0.6196, 0.6941, 0.8118],\n",
      "          [0.1961, 0.1922, 0.1922,  ..., 0.5647, 0.5961, 0.7176]],\n",
      "\n",
      "         [[0.1804, 0.1882, 0.1569,  ..., 0.3412, 0.3490, 0.3608],\n",
      "          [0.1922, 0.1922, 0.1529,  ..., 0.3294, 0.3294, 0.3412],\n",
      "          [0.1922, 0.1922, 0.1490,  ..., 0.3294, 0.3255, 0.3490],\n",
      "          ...,\n",
      "          [0.2000, 0.1961, 0.1922,  ..., 0.7216, 0.8039, 0.6863],\n",
      "          [0.2000, 0.1961, 0.1922,  ..., 0.6824, 0.7647, 0.8431],\n",
      "          [0.1961, 0.1922, 0.1922,  ..., 0.6235, 0.6706, 0.7608]],\n",
      "\n",
      "         [[0.1765, 0.1686, 0.1216,  ..., 0.3373, 0.3451, 0.3451],\n",
      "          [0.1882, 0.1804, 0.1333,  ..., 0.3255, 0.3255, 0.3294],\n",
      "          [0.1922, 0.1922, 0.1490,  ..., 0.3294, 0.3255, 0.3412],\n",
      "          ...,\n",
      "          [0.1922, 0.1882, 0.1843,  ..., 0.7608, 0.8196, 0.6745],\n",
      "          [0.1961, 0.1922, 0.1882,  ..., 0.7373, 0.7882, 0.8314],\n",
      "          [0.1961, 0.1922, 0.1922,  ..., 0.6902, 0.7020, 0.7490]]]])\n",
      "{'input_ids': tensor([[    2,  9565,   638,   920, 17706,  6070,   460,  1752,  1808,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    2,  1799,     3,    18,   350,  5108, 21438,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "for i, j in train_loader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "#     print(tokenizer.decode(j[0]['input_ids']))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4461936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f0ca6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.001, weight_decay=0.00001, num_epochs=20, checkpoint_path='../checkpoints/'):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    model\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "\n",
    "    batch_size = 2\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "       \n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32)\n",
    "            text = batch[1]['input_ids'].type(torch.long)\n",
    "\n",
    "            if len(imgs) < batch_size:\n",
    "                # Last batch will have less images, text pairs since it will be the\n",
    "                # remainder of Total images / batch_size.\n",
    "\n",
    "                # Adjust the learning rate of the last batch by \n",
    "                # (size(last_batch) / batch_size) to account \n",
    "                # for the smaller size.\n",
    "                adj_lr = lr * (len(inp) / batch_size)\n",
    "                optimizer = optim.AdamW(model.parameters(),\n",
    "                    lr=adj_lr,\n",
    "                    weight_decay=weight_decay)\n",
    "\n",
    "            text_labels = text[:, 1:] # labels are the same text just with the <s> token removed\n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            n += 1\n",
    "            iters.append(n)\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss\n",
    "            t_contrastive_loss += contrastive_loss\n",
    "            t_generative_loss += generative_loss\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss / len(loader))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(loader))\n",
    "        train_generative_losses.append(t_generative_loss / len(loader))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "        if epoch % 5 == 0: # save model every 5th epoch\n",
    "            torch.save(model.state_dict(), checkpoint_path.format(epoch))\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(loader), t_contrastive_loss / len(loader), t_generative_loss / len(loader)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(loader), val_contrastive_loss / len(loader), val_generative_loss / len(loader)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bc0d34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img torch.Size([2, 3, 224, 224])\n",
      "feats torch.Size([2, 256, 1280])\n",
      "output shape torch.Size([2, 102, 512])\n",
      "output shape torch.Size([2, 102, 512])\n",
      "output shape torch.Size([2, 102, 512])\n",
      "output shape torch.Size([2, 102, 512])\n",
      "output shape torch.Size([2, 102, 512])\n",
      "output shape torch.Size([2, 102, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x512 and 1280x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[194], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, lr, weight_decay, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     48\u001b[0m         lr\u001b[38;5;241m=\u001b[39madj_lr,\n\u001b[1;32m     49\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     51\u001b[0m text_labels \u001b[38;5;241m=\u001b[39m text[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;66;03m# labels are the same text just with the <s> token removed\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m total_loss, contrastive_loss, generative_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[208], line 234\u001b[0m, in \u001b[0;36mMaMMUT.forward\u001b[0;34m(self, image, text, text_labels)\u001b[0m\n\u001b[1;32m    230\u001b[0m constrastive_text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_text_features(constrastive_text_features)\n\u001b[1;32m    231\u001b[0m contrastive_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrastive_loss(vision_features, constrastive_text_features)\n\u001b[0;32m--> 234\u001b[0m generative_text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerative_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m text_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_output_features_to_text_tokens(generative_text_features)\n\u001b[1;32m    236\u001b[0m generative_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerative_loss(text_logits, text_labels)\n",
      "Cell \u001b[0;32mIn[208], line 183\u001b[0m, in \u001b[0;36mMaMMUT.generative_text_features\u001b[0;34m(self, text_embeds, vision_features)\u001b[0m\n\u001b[1;32m    180\u001b[0m         output \u001b[38;5;241m=\u001b[39m layer(output, vision_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enable_cross_attn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, causal_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask, padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# enable cross-attention for even numbered layers\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_cross_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[204], line 46\u001b[0m, in \u001b[0;36mTextDecoderLayer.forward\u001b[0;34m(self, x, vision_features, enable_cross_attn, causal_mask, padding_mask, attn_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m out_layer_norm1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(out)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_cross_attn:\n\u001b[0;32m---> 46\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_cross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_cross_attn(x)\n\u001b[1;32m     48\u001b[0m     v2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_cross_attn(vision_features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x512 and 1280x512)"
     ]
    }
   ],
   "source": [
    "train(model=model, data=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "aa9f7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.long).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "\n",
    "        text_labels = text[:, 1:] # labels are the same text just with the <s> token removed\n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss\n",
    "        val_contrastive_loss += contrastive_loss\n",
    "        val_generative_loss += generative_loss\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d26c56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextDecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                num_heads_mha, \n",
    "                num_heads_cross_attn, \n",
    "                d_feedforward, \n",
    "                d_k,\n",
    "                d_v, \n",
    "                vit_dim):\n",
    "        super(TextDecoderLayer, self).__init__()\n",
    "\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.MHA_1 = nn.MultiheadAttention(d_model, num_heads_mha, batch_first =True)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.k_cross_attn = nn.Linear(vit_dim, d_model)\n",
    "        self.q_cross_attn = nn.Linear(d_model, d_model)\n",
    "        self.v_cross_attn = nn.Linear(vit_dim, d_model)\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, num_heads_cross_attn, batch_first =True)\n",
    "        self.layer_norm_cross_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_feedforward)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_feedforward, d_model)\n",
    "        self.layer_norm_ff = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, vision_features=None, enable_cross_attn=True, causal_mask=False, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"Forward method for decoder layer with added option to disable cross attention and causal masking\"\"\"\n",
    "        k1 = self.k(x)\n",
    "        q1 = self.q(x)\n",
    "        v1 = self.v(x)\n",
    "\n",
    "        out = self.MHA_1(q1, k1, v1, is_causal=causal_mask, key_padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "#         print(out.shape)\n",
    "        out = self.layer_norm1(out[0] + x)\n",
    "        out_layer_norm1 = torch.clone(out)\n",
    "\n",
    "        if enable_cross_attn:\n",
    "            k2 = self.k_cross_attn(vision_features)\n",
    "            q2 = self.q_cross_attn(x)\n",
    "            v2 = self.v_cross_attn(vision_features)\n",
    "            out = self.cross_attn(q2, k2, v2)\n",
    "            out = self.layer_norm_cross_attn(out[0] + out_layer_norm1)\n",
    "            out_layer_norm_cross_attn = torch.clone(out)\n",
    "        \n",
    "        out = self.fc2(self.relu(self.fc1(out)))\n",
    "        if enable_cross_attn:\n",
    "            out = self.layer_norm_ff(out + out_layer_norm_cross_attn)\n",
    "        else:\n",
    "            out = self.layer_norm_ff(out + out_layer_norm1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "afa5b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as PIL_Image\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "from torchvision.transforms import v2\n",
    "# from text_decoder import TextDecoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaMMUT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 14,\n",
    "                 vit_num_layers: int = 32,\n",
    "                 vit_num_heads: int = 16,\n",
    "                 vit_hidden_dim: int = 1280,\n",
    "                 vit_mlp_dim: int = 5120,\n",
    "                 vit_dropout: float = 0.0, # Potential ablation / extension to add to the replication\n",
    "                 vit_attention_dropout: float = 0.0, # Potential ablation / extension to add to the replication\n",
    "                 contrastive_loss_weight: float = 1.0,\n",
    "                 generative_loss_weight: float = 1.0,\n",
    "                 text_decoder_depth: int = 6,\n",
    "                 text_decoder_embed_dim: int = 512,\n",
    "                 text_decoder_sub_layer_heads: int = 8,\n",
    "                 text_decoder_feedforward_dim: int = 2048,\n",
    "                 text_decoder_dk: int = 128,\n",
    "                 vocab_size: int = 1000,\n",
    "                 latent_dim: int = 512,\n",
    "                 contrastive_loss_temp: float = 0.5,\n",
    "                 contrastive_loss_gamma: float = 1.0):\n",
    "                 \n",
    "        super(MaMMUT, self).__init__()         \n",
    "        self.vit = VisionTransformer(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_layers=vit_num_layers,\n",
    "            num_heads=vit_num_heads,\n",
    "            hidden_dim=vit_hidden_dim,\n",
    "            mlp_dim=vit_mlp_dim,\n",
    "            dropout=vit_dropout,\n",
    "            attention_dropout=vit_attention_dropout,\n",
    "            num_classes=1000\n",
    "        )\n",
    "        \n",
    "        self.token_size = vocab_size\n",
    "        self.text_decoder_embed_dim = text_decoder_embed_dim\n",
    "        self.text_decoder_sub_layer_heads = text_decoder_sub_layer_heads\n",
    "        \n",
    "        self.contrastive_loss_weight = contrastive_loss_weight\n",
    "        self.generative_loss_weight = generative_loss_weight\n",
    "        \n",
    "        self.ifs2tfs = nn.Linear(vit_hidden_dim, text_decoder_embed_dim)\n",
    "        \n",
    "        self.text_decoder_depth = text_decoder_depth\n",
    "        self.text_decoder_layers = []\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (image_size // patch_size)**2 + 1, vit_hidden_dim))\n",
    "        \n",
    "        self.final_layernorm = nn.LayerNorm(self.token_size)\n",
    "\n",
    "        self.latent_text_features = nn.Linear(text_decoder_embed_dim, text_decoder_embed_dim) # for contrastive loss\n",
    "        self.pad_token_id = 0 # we can set this in the SentencePiece tokenizer\n",
    "\n",
    "        self.text_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=text_decoder_embed_dim, padding_idx=self.pad_token_id)\n",
    "\n",
    "        self.text_cls_token = nn.Parameter(torch.randn(text_decoder_embed_dim))\n",
    "        self.contrastive_layernorm = nn.LayerNorm(text_decoder_embed_dim)\n",
    "\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.contrastive_loss_temp = contrastive_loss_temp\n",
    "        self.contrastive_loss_gamma = contrastive_loss_gamma\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        \n",
    "        # Changing logic for the decoder layer. This way we can disable cross-attention during the forward pass and keep everything else the same\n",
    "        for i in range(text_decoder_depth):\n",
    "            self.text_decoder_layers.append(TextDecoderLayer(d_model=text_decoder_embed_dim, num_heads_mha=text_decoder_sub_layer_heads, \\\n",
    "                                                            num_heads_cross_attn=text_decoder_sub_layer_heads, d_feedforward=text_decoder_feedforward_dim, \\\n",
    "                                                             d_k=text_decoder_dk, d_v=(text_decoder_embed_dim // text_decoder_sub_layer_heads), vit_dim=vit_hidden_dim)\n",
    "                                                             )\n",
    "        \n",
    "        self.decoder_output_features_to_text_tokens_layer = nn.Linear(self.text_decoder_embed_dim, self.token_size) # for captioning loss\n",
    "            \n",
    "        \n",
    "    def cropped_positional_encoding(self, feats):\n",
    "        # feats shape: N x (H_p x W_p) x Hidden\n",
    "        n, h_w, hidden = feats.shape\n",
    "\n",
    "\n",
    "        # take out cls token before upsampling\n",
    "        cls_pos_embed = self.pos_embedding[:, 0, :]\n",
    "        pos_embeddings = self.pos_embedding[:, 1: :]\n",
    "\n",
    "        pos_embeddings = pos_embeddings.reshape(1, self.image_size // self.patch_size, self.image_size // self.patch_size, hidden).permute(0, -1, 1, 2)\n",
    "\n",
    "        # Upsample using bilinear interpolation\n",
    "        upsample_layer = nn.Upsample(mode='bilinear', scale_factor=4)\n",
    "        upsampled_pos_embeddings = upsample_layer(pos_embeddings)\n",
    "        random_crop = v2.RandomCrop(pos_embeddings.shape[2])\n",
    "        cropped_pos_encoding = random_crop(upsampled_pos_embeddings)\n",
    "\n",
    "        # cropped_pos_encoding shape: N x (H_p x W_p) x Hidden. Reshape to align with feats\n",
    "        cropped_pos_encoding = cropped_pos_encoding.reshape(1, h_w-1, hidden)\n",
    "        \n",
    "        cropped_pos_encoding = torch.cat([cropped_pos_encoding, cls_pos_embed.reshape(1, 1, hidden)], dim=1)\n",
    "\n",
    "\n",
    "        return feats + cropped_pos_encoding\n",
    "\n",
    "        \n",
    "    def get_vision_features(self, img: torch.tensor):\n",
    "        # image has shape N x C x H x W where\n",
    "        # N is the batch size\n",
    "        # C is the channel size\n",
    "        # H is the image height\n",
    "        # W is the image width\n",
    "#         preprocessing = v2.Compose([\n",
    "#             v2.ToImage(),\n",
    "#             v2.Resize((272,272)),\n",
    "#             v2.RandomCrop(224)\n",
    "#         ])\n",
    "\n",
    "#         img = PIL_Image.open(\"example_2353642598754.jpeg\")\n",
    "#         img = preprocessing(img)\n",
    "\n",
    "        # Add batch dimension - for testing on one image, remove for training\n",
    "#         img = img.unsqueeze(0)\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w), converts into patches\n",
    "        print(\"img\", img.shape)\n",
    "        feats = self.vit._process_input(img)\n",
    "        print(\"feats\", feats.shape)\n",
    "        # Expand the CLS token to the full batch\n",
    "        batch_class_token = self.vit.class_token.expand(img.shape[0], -1, -1)\n",
    "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
    "        \n",
    "        feats = self.cropped_positional_encoding(feats)\n",
    "\n",
    "        feats = self.vit.encoder(feats)\n",
    "\n",
    "        # Fetch pre-prended CLS token at position 0 in dimension 1\n",
    "        feats = feats[:, 0]\n",
    "        \n",
    "#         print(feats.shape)\n",
    "        \n",
    "        return feats\n",
    "    \n",
    "    def img_feat_size_to_txt_feat_size(self, vision_features: torch.tensor):\n",
    "        return self.ifs2tfs(vision_features)\n",
    "    \n",
    "    def contrastive_text_features(self, text_embeds: torch.Tensor):\n",
    "        # text has shape N x S\n",
    "        # Remember to pass bidirectional mask (as far as I understand, a mask that allows attention to all non-padded areas or maybe just all non-CLS areas and maybe stops cls from attending to padding TODO: Clarify)\n",
    "        # Remember to perform residual additions         \n",
    "        # expand to match dimensions\n",
    "        cls_tokens = self.text_cls_token.expand(text_embeds.shape[0], 1, self.text_decoder_embed_dim)\n",
    "        # Add cls tokens to start of the sequences\n",
    "        text_embeds = torch.cat([cls_tokens, text_embeds], dim=1)\n",
    "        cls_padding_mask = (text_embeds == 0).all(dim=-1) # From nn.Embedding, padding tokens are embedded as vector of 0s. Result should be shape N x S.\n",
    "\n",
    "        output = text_embeds.clone()\n",
    "        for i, layer in enumerate(self.text_decoder_layers):\n",
    "            # Disable cross-attention for contrastive features\n",
    "            print(\"output shape\", output.shape)\n",
    "            output = layer(output, enable_cross_attn=False, padding_mask=cls_padding_mask)\n",
    "\n",
    "        output = output[:, 0]\n",
    "        output = self.contrastive_layernorm(output)\n",
    "        return output\n",
    "    \n",
    "    def generative_text_features(self, text_embeds: torch.tensor, vision_features: torch.tensor):\n",
    "        # Remember to toggle causal in forward pass\n",
    "        # Remember to perform residual additions\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones((text_embeds.shape[1], text_embeds.shape[1]))).bool() # Assuming shape[1] is the sequence dim\n",
    "        output = text_embeds.clone()\n",
    "        padding_mask = (text_embeds == 0).all(dim=-1)\n",
    "        for i, layer in enumerate(self.text_decoder_layers):\n",
    "            # Disable cross-attention for odd numbered layers\n",
    "            if i % 2 != 0:\n",
    "                output = layer(output, vision_features=None, enable_cross_attn=False, causal_mask=True, attn_mask=attn_mask, padding_mask=padding_mask)\n",
    "            else:\n",
    "                # enable cross-attention for even numbered layers\n",
    "                output = layer(output, vision_features=vision_features, enable_cross_attn=True, causal_mask=True, attn_mask=attn_mask, padding_mask=padding_mask)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def contrastive_loss(self, vision_features: torch.tensor, constrastive_text_features: torch.tensor):\n",
    "        \"\"\"Implement Focal-contrastive loss as in the paper\"\"\"\n",
    "        similarity = (vision_features @ constrastive_text_features.T) / self.contrastive_loss_temp\n",
    "        # In contrastive learning we aim to minimize loss for between the matching image and text pairs, and maximize loss \n",
    "        # for mismatching image text pairs.\n",
    "        # after the matrix multipication, shape will be N x N\n",
    "        # each row represents image i, and each column would represent each caption\n",
    "        # Therefore, the matching pairs will be across the diagonal (0,0), (1, 1) ... and we can treat this as a classification task\n",
    "        # where we compute the loss between the text_logits and its matching image and vice-versa for the image loss\n",
    "    \n",
    "        # We can construct the labels by just creating a diagonal matrix\n",
    "        labels = torch.arange(similarity.shape[0])\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=similarity.shape[0])\n",
    "\n",
    "        probs_imgs = F.softmax(similarity, dim=1) # using softmax instead of sigmoid\n",
    "        loss_i2t = ((1 - probs_imgs) ** self.contrastive_loss_gamma) * (torch.log(probs_imgs))\n",
    "\n",
    "        probs_texts = F.softmax(similarity, dim=0)\n",
    "        loss_t2i = ((1 - probs_texts) ** self.contrastive_loss_gamma) * (torch.log(probs_texts))\n",
    "\n",
    "        total_contrastive_loss = loss_i2t + loss_t2i\n",
    "\n",
    "        return total_contrastive_loss\n",
    "\n",
    "\n",
    "    def generative_loss(self, generative_text_features: torch.tensor, text_labels: torch.tensor):\n",
    "        generative_text_features = generative_text_features.permute(0, -1, 1) # cross-entropy expects N x C as first two dims\n",
    "        loss = self.loss_criterion(generative_text_features, text_labels, ignore_index=self.pad_token_id)\n",
    "        return loss\n",
    "\n",
    "    def decoder_output_features_to_text_tokens(self, text_features: torch.tensor):\n",
    "        return self.final_layernorm(self.decoder_output_features_to_text_tokens_layer(text_features))\n",
    "        \n",
    "    \n",
    "    def forward(self, image, text, text_labels):\n",
    "        # Pseudocode for now, need to fully implement and test\n",
    "        # TODO: Implement average pooling over spatial dimension and sequence where appropriate\n",
    "        # TODO: Add tokenizer & params ------- Tokenizer would be added in training pipeline\n",
    "        text_embeds = self.text_embeddings(text)\n",
    "        vision_features = self.get_vision_features(image)\n",
    "        vision_features = self.img_feat_size_to_txt_feat_size(vision_features) # projects image feature dim to text feature dim\n",
    "        \n",
    "        constrastive_text_features = self.contrastive_text_features(text_embeds)\n",
    "        constrastive_text_features = self.latent_text_features(constrastive_text_features)\n",
    "        contrastive_loss = self.contrastive_loss(vision_features, constrastive_text_features)\n",
    "        \n",
    "        \n",
    "        generative_text_features = self.generative_text_features(text_embeds, vision_features)\n",
    "        text_logits = self.decoder_output_features_to_text_tokens(generative_text_features)\n",
    "        generative_loss = self.generative_loss(text_logits, text_labels)\n",
    "        \n",
    "        loss = self.contrastive_loss_weight * contrastive_loss + self.generative_loss_weight * generative_loss\n",
    "        \n",
    "        return loss, contrastive_loss, generative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "43828f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaMMUT(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2989d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/hfaisal8/CS7643/project/RepliMaMMUT\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
