{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb886d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b019739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a884230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c36fe84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7dc2a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41faf9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b72784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01120acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6628e321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04191de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using only subset of data for now\n",
    "pre_train_data = pre_train_data.with_format(\"torch\")\n",
    "test_data = pre_train_data.select(range(6000, 7000))\n",
    "val_data = pre_train_data.select(range(5000, 6000))\n",
    "pre_train_data = pre_train_data.select(range(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e9a51cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b2f6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddeab91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d71fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "#         text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=1)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None\n",
    "#     def encode_text(self, example):\n",
    "#         text = self.tokenizer(example, padding='max_length', max_length=max_seq_len, add_special_tokens=True) # hard-coded max_length for now\n",
    "#         bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "#          # add a bos token as well\n",
    "#         text = {\n",
    "#             \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "#             \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "#         }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00981577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812a3ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)\n",
    "    return images, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b9ec14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "val_dataset_cleaned = PreTrainDataset(val_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=32, shuffle=True, collate_fn=remove_none_fn)\n",
    "val_loader = DataLoader(val_dataset_cleaned, batch_size=10, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4461936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4c36d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_grad_norms(model, norm_type=2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            grad_norm = param.grad.norm(norm_type).item()\n",
    "            print(f\"{name}: grad norm = {grad_norm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150e5013-8069-4109-8c36-4e49b9e55de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0ca6d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, val_data, opt=None, lr=0.0001, weight_decay=0.00000, num_epochs=20, checkpoint_path='../checkpoints/', warmup_steps=100):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "#     n = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    batch_size = 32\n",
    "    n = 0\n",
    "    accumulation_steps = 4\n",
    "    if opt is not None:\n",
    "        optimizer = opt\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "    # scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, total_iters=warmup_steps)\n",
    "#     main_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "#     schedule = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_steps])\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "\n",
    "\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32).to(device)\n",
    "            text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "#             print(text)\n",
    "\n",
    "#             if len(imgs) < batch_size:\n",
    "#                 # Last batch will have less images, text pairs since it will be the\n",
    "#                 # remainder of Total images / batch_size.\n",
    "\n",
    "#                 # Adjust the learning rate of the last batch by \n",
    "#                 # (size(last_batch) / batch_size) to account \n",
    "#                 # for the smaller size.\n",
    "#                 adj_lr = lr * (len(imgs) / batch_size)\n",
    "#                 optimizer = optim.AdamW(model.parameters(),\n",
    "#                     lr=adj_lr,\n",
    "#                     weight_decay=weight_decay)\n",
    "            # Since task is to predict next token, the labels will start form position 1\n",
    "            text_labels = text[:, 1:] \n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            total_loss = total_loss / accumulation_steps\n",
    "            \n",
    "            n += 1\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            print(f\"Iter: {n}   Total Loss: {total_loss.item() * accumulation_steps}   Gen Loss: {generative_loss.item()}   Contr Loss: {contrastive_loss.item()}\")\n",
    "            total_loss.backward(retain_graph=True)\n",
    "#             contrastive_loss.backward(retain_graph=True)\n",
    "#             print(\"contrastive_norms\")\n",
    "#             log_grad_norms(model)\n",
    "#             generative_loss.backward(retain_graph=True)\n",
    "#             print(\"generative_norms\")\n",
    "#             log_grad_norms(model)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n",
    "            i = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.grad is not None:\n",
    "#                     print(f\"{name}: grad norm = {param.grad.norm().item():.4f}\")\n",
    "#                 i += 1\n",
    "#                 if i > 10:\n",
    "#                     break\n",
    "            if n % accumulation_steps == 0: # accumulate gradients to artifically increase batch size for learning\n",
    "               \n",
    "                optimizer.step()\n",
    "                # scheduler.step(total_loss)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss.detach()\n",
    "            t_contrastive_loss += contrastive_loss.detach()\n",
    "            t_generative_loss += generative_loss.detach()\n",
    "            del imgs\n",
    "            del text\n",
    "            if n % 100 == 0:\n",
    "#                 torch.save(model.state_dict(), f\"{checkpoint_path}_iter_{n}\")\n",
    "                torch.save({\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        }, f\"{checkpoint_path}_model_checkpoint.pt\")\n",
    "                with open(f\"{checkpoint_path}_train_loss.pkl\", 'wb') as f:\n",
    "                    pickle.dump(train_losses, f)\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss * accumulation_steps / len(data))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(data))\n",
    "        train_generative_losses.append(t_generative_loss / len(data))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "#         if epoch % 5 == 0: # save model every 5th epoch\n",
    "#         torch.save(model.state_dict(), f\"{checkpoint_path}_epoch_{epoch}\")\n",
    "        torch.save({\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }, f\"_model_checkpoint.pt\")\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(data), t_contrastive_loss / len(data), t_generative_loss / len(data)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(val_data), val_contrastive_loss / len(val_data), val_generative_loss / len(val_data)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f87bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.float32).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "        # Since task is to predict next token, the labels will start form position 1\n",
    "        text_labels = text[:, 1:] \n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss.detach()\n",
    "        val_contrastive_loss += contrastive_loss.detach()\n",
    "        val_generative_loss += generative_loss.detach()\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3732c220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0db509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import MaMMUT\n",
    "model = MaMMUT(vocab_size=tokenizer.vocab_size,\n",
    "                image_size= 224,\n",
    "                patch_size = 16,\n",
    "                vit_num_layers= 6,\n",
    "                vit_num_heads= 4,\n",
    "                vit_hidden_dim = 768,\n",
    "                vit_mlp_dim = 2048,\n",
    "                vit_dropout = 0.0, # Potential ablation / extension to add to the replication\n",
    "                vit_attention_dropout = 0.0, # Potential ablation / extension to add to the replication\n",
    "                contrastive_loss_weight = 1.25,\n",
    "                generative_loss_weight = 1.0,\n",
    "                text_decoder_depth = 4,\n",
    "                text_decoder_embed_dim = 256,\n",
    "                text_decoder_sub_layer_heads = 4,\n",
    "                text_decoder_feedforward_dim = 2048,\n",
    "                text_decoder_dk = 128,\n",
    "                latent_dim = 512,\n",
    "                contrastive_loss_gamma = 1.0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a8f045a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = torch.load('../checkpoints/_model_checkpoint.pt', weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10002b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "013cf1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(c['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263105a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = optim.AdamW(model.parameters(),\n",
    "                lr=0.01,\n",
    "                weight_decay=0)\n",
    "opt.load_state_dict(c['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d34f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 1   Total Loss: 10.513639450073242   Gen Loss: 6.381706237792969   Contr Loss: 3.3055460453033447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 2   Total Loss: 10.763405799865723   Gen Loss: 7.047630310058594   Contr Loss: 2.9726204872131348\n",
      "-----------------------------------------------------------\n",
      "Iter: 3   Total Loss: 10.812945365905762   Gen Loss: 7.086523532867432   Contr Loss: 2.981137275695801\n",
      "-----------------------------------------------------------\n",
      "Iter: 4   Total Loss: 10.941603660583496   Gen Loss: 7.328691005706787   Contr Loss: 2.8903303146362305\n",
      "-----------------------------------------------------------\n",
      "Iter: 5   Total Loss: 11.884536743164062   Gen Loss: 7.01146936416626   Contr Loss: 3.898453712463379\n",
      "-----------------------------------------------------------\n",
      "Iter: 6   Total Loss: 11.98591136932373   Gen Loss: 7.101951599121094   Contr Loss: 3.907167673110962\n",
      "-----------------------------------------------------------\n",
      "Iter: 7   Total Loss: 12.360930442810059   Gen Loss: 7.07823371887207   Contr Loss: 4.226157188415527\n",
      "-----------------------------------------------------------\n",
      "Iter: 8   Total Loss: 12.448683738708496   Gen Loss: 6.954749584197998   Contr Loss: 4.395147323608398\n",
      "-----------------------------------------------------------\n",
      "Iter: 9   Total Loss: 12.52770709991455   Gen Loss: 7.197288513183594   Contr Loss: 4.264334678649902\n",
      "-----------------------------------------------------------\n",
      "Iter: 10   Total Loss: 12.575323104858398   Gen Loss: 6.931310653686523   Contr Loss: 4.515210151672363\n",
      "-----------------------------------------------------------\n",
      "Iter: 11   Total Loss: 12.600114822387695   Gen Loss: 7.237364292144775   Contr Loss: 4.290200710296631\n",
      "-----------------------------------------------------------\n",
      "Iter: 12   Total Loss: 12.974018096923828   Gen Loss: 6.678192615509033   Contr Loss: 5.036660671234131\n",
      "-----------------------------------------------------------\n",
      "Iter: 13   Total Loss: 13.669240951538086   Gen Loss: 6.937936305999756   Contr Loss: 5.385043144226074\n",
      "-----------------------------------------------------------\n",
      "Iter: 14   Total Loss: 11.83061408996582   Gen Loss: 7.322380065917969   Contr Loss: 3.6065871715545654\n",
      "-----------------------------------------------------------\n",
      "Iter: 15   Total Loss: 12.03720474243164   Gen Loss: 7.33690071105957   Contr Loss: 3.7602434158325195\n",
      "-----------------------------------------------------------\n",
      "Iter: 16   Total Loss: 13.411947250366211   Gen Loss: 6.411391735076904   Contr Loss: 5.600444793701172\n",
      "-----------------------------------------------------------\n",
      "Iter: 17   Total Loss: 11.831851959228516   Gen Loss: 7.442592620849609   Contr Loss: 3.5114071369171143\n",
      "-----------------------------------------------------------\n",
      "Iter: 18   Total Loss: 11.777681350708008   Gen Loss: 7.000401496887207   Contr Loss: 3.821824073791504\n",
      "-----------------------------------------------------------\n",
      "Iter: 19   Total Loss: 11.9364652633667   Gen Loss: 6.9982099533081055   Contr Loss: 3.950604200363159\n",
      "-----------------------------------------------------------\n",
      "Iter: 20   Total Loss: 11.899872779846191   Gen Loss: 7.401697158813477   Contr Loss: 3.5985403060913086\n",
      "-----------------------------------------------------------\n",
      "Iter: 21   Total Loss: 12.208538055419922   Gen Loss: 7.825306415557861   Contr Loss: 3.5065855979919434\n",
      "-----------------------------------------------------------\n",
      "Iter: 22   Total Loss: 11.24828815460205   Gen Loss: 6.784778594970703   Contr Loss: 3.570807456970215\n",
      "-----------------------------------------------------------\n",
      "Iter: 23   Total Loss: 11.882497787475586   Gen Loss: 7.566761016845703   Contr Loss: 3.4525890350341797\n",
      "-----------------------------------------------------------\n",
      "Iter: 24   Total Loss: 11.746797561645508   Gen Loss: 7.173861503601074   Contr Loss: 3.658348798751831\n",
      "-----------------------------------------------------------\n",
      "Iter: 25   Total Loss: 11.535699844360352   Gen Loss: 7.208989143371582   Contr Loss: 3.461369037628174\n",
      "-----------------------------------------------------------\n",
      "Iter: 26   Total Loss: 11.77267837524414   Gen Loss: 7.321380138397217   Contr Loss: 3.561038017272949\n",
      "-----------------------------------------------------------\n",
      "Iter: 27   Total Loss: 11.975892066955566   Gen Loss: 7.71681547164917   Contr Loss: 3.407261371612549\n",
      "-----------------------------------------------------------\n",
      "Iter: 28   Total Loss: 12.144158363342285   Gen Loss: 7.613403797149658   Contr Loss: 3.624603748321533\n",
      "-----------------------------------------------------------\n",
      "Iter: 29   Total Loss: 11.866012573242188   Gen Loss: 7.575089454650879   Contr Loss: 3.4327383041381836\n",
      "-----------------------------------------------------------\n",
      "Iter: 30   Total Loss: 11.297555923461914   Gen Loss: 7.06873083114624   Contr Loss: 3.383059501647949\n",
      "-----------------------------------------------------------\n",
      "Iter: 31   Total Loss: 11.779155731201172   Gen Loss: 7.490909099578857   Contr Loss: 3.4305975437164307\n",
      "-----------------------------------------------------------\n",
      "Iter: 32   Total Loss: 11.324560165405273   Gen Loss: 7.067148208618164   Contr Loss: 3.4059300422668457\n",
      "-----------------------------------------------------------\n",
      "Iter: 33   Total Loss: 10.804200172424316   Gen Loss: 6.464824199676514   Contr Loss: 3.4715006351470947\n",
      "-----------------------------------------------------------\n",
      "Iter: 34   Total Loss: 11.741361618041992   Gen Loss: 7.47128438949585   Contr Loss: 3.416062355041504\n",
      "-----------------------------------------------------------\n",
      "Iter: 35   Total Loss: 11.317113876342773   Gen Loss: 7.023332595825195   Contr Loss: 3.435025215148926\n",
      "-----------------------------------------------------------\n",
      "Iter: 36   Total Loss: 11.487768173217773   Gen Loss: 7.22001314163208   Contr Loss: 3.4142038822174072\n",
      "-----------------------------------------------------------\n",
      "Iter: 37   Total Loss: 11.70008373260498   Gen Loss: 7.48248291015625   Contr Loss: 3.3740806579589844\n",
      "-----------------------------------------------------------\n",
      "Iter: 38   Total Loss: 11.458208084106445   Gen Loss: 7.236985206604004   Contr Loss: 3.3769781589508057\n",
      "-----------------------------------------------------------\n",
      "Iter: 39   Total Loss: 11.570110321044922   Gen Loss: 7.1641693115234375   Contr Loss: 3.5247530937194824\n",
      "-----------------------------------------------------------\n",
      "Iter: 40   Total Loss: 11.186483383178711   Gen Loss: 6.867361545562744   Contr Loss: 3.4552969932556152\n",
      "-----------------------------------------------------------\n",
      "Iter: 41   Total Loss: 11.737749099731445   Gen Loss: 7.530908584594727   Contr Loss: 3.3654727935791016\n",
      "-----------------------------------------------------------\n",
      "Iter: 42   Total Loss: 11.128039360046387   Gen Loss: 6.97152042388916   Contr Loss: 3.3252151012420654\n",
      "-----------------------------------------------------------\n",
      "Iter: 43   Total Loss: 10.8284330368042   Gen Loss: 6.646728992462158   Contr Loss: 3.345363140106201\n",
      "-----------------------------------------------------------\n",
      "Iter: 44   Total Loss: 11.7584228515625   Gen Loss: 7.624885559082031   Contr Loss: 3.3068294525146484\n",
      "-----------------------------------------------------------\n",
      "Iter: 45   Total Loss: 11.572150230407715   Gen Loss: 7.304859161376953   Contr Loss: 3.413832664489746\n",
      "-----------------------------------------------------------\n",
      "Iter: 46   Total Loss: 11.97523307800293   Gen Loss: 7.884187698364258   Contr Loss: 3.272836208343506\n",
      "-----------------------------------------------------------\n",
      "Iter: 47   Total Loss: 11.440170288085938   Gen Loss: 7.197887420654297   Contr Loss: 3.3938257694244385\n",
      "-----------------------------------------------------------\n",
      "Iter: 48   Total Loss: 11.933479309082031   Gen Loss: 7.588979244232178   Contr Loss: 3.475600004196167\n",
      "-----------------------------------------------------------\n",
      "Iter: 49   Total Loss: 11.695184707641602   Gen Loss: 7.40806245803833   Contr Loss: 3.429697275161743\n",
      "-----------------------------------------------------------\n",
      "Iter: 50   Total Loss: 11.886359214782715   Gen Loss: 7.556053638458252   Contr Loss: 3.4642446041107178\n",
      "-----------------------------------------------------------\n",
      "Iter: 51   Total Loss: 11.378822326660156   Gen Loss: 6.984076023101807   Contr Loss: 3.5157971382141113\n",
      "-----------------------------------------------------------\n",
      "Iter: 52   Total Loss: 11.571722984313965   Gen Loss: 7.380974769592285   Contr Loss: 3.3525986671447754\n",
      "-----------------------------------------------------------\n",
      "Iter: 53   Total Loss: 11.69914436340332   Gen Loss: 7.443621635437012   Contr Loss: 3.404418468475342\n",
      "-----------------------------------------------------------\n",
      "Iter: 54   Total Loss: 11.463905334472656   Gen Loss: 7.124760150909424   Contr Loss: 3.471315622329712\n",
      "-----------------------------------------------------------\n",
      "Iter: 55   Total Loss: 11.613338470458984   Gen Loss: 7.263256072998047   Contr Loss: 3.4800662994384766\n",
      "-----------------------------------------------------------\n",
      "Iter: 56   Total Loss: 11.252933502197266   Gen Loss: 7.239981651306152   Contr Loss: 3.2103610038757324\n",
      "-----------------------------------------------------------\n",
      "Iter: 57   Total Loss: 11.100008964538574   Gen Loss: 6.893227577209473   Contr Loss: 3.3654251098632812\n",
      "-----------------------------------------------------------\n",
      "Iter: 58   Total Loss: 11.56035041809082   Gen Loss: 7.240543365478516   Contr Loss: 3.455845355987549\n",
      "-----------------------------------------------------------\n",
      "Iter: 59   Total Loss: 11.22020149230957   Gen Loss: 6.983767509460449   Contr Loss: 3.3891472816467285\n",
      "-----------------------------------------------------------\n",
      "Iter: 60   Total Loss: 11.578981399536133   Gen Loss: 7.328226089477539   Contr Loss: 3.400604248046875\n",
      "-----------------------------------------------------------\n",
      "Iter: 61   Total Loss: 11.992936134338379   Gen Loss: 7.797947883605957   Contr Loss: 3.355990409851074\n",
      "-----------------------------------------------------------\n",
      "Iter: 62   Total Loss: 11.531098365783691   Gen Loss: 7.3737077713012695   Contr Loss: 3.3259124755859375\n",
      "-----------------------------------------------------------\n",
      "Iter: 63   Total Loss: 11.166141510009766   Gen Loss: 6.910002708435059   Contr Loss: 3.4049110412597656\n",
      "-----------------------------------------------------------\n",
      "Iter: 64   Total Loss: 11.183862686157227   Gen Loss: 6.995088577270508   Contr Loss: 3.3510191440582275\n",
      "-----------------------------------------------------------\n",
      "Iter: 65   Total Loss: 11.411310195922852   Gen Loss: 7.184267520904541   Contr Loss: 3.381633996963501\n",
      "-----------------------------------------------------------\n",
      "Iter: 66   Total Loss: 11.988965034484863   Gen Loss: 7.707766056060791   Contr Loss: 3.424959182739258\n",
      "-----------------------------------------------------------\n",
      "Iter: 67   Total Loss: 11.86097526550293   Gen Loss: 7.574845314025879   Contr Loss: 3.428903579711914\n",
      "-----------------------------------------------------------\n",
      "Iter: 68   Total Loss: 11.863920211791992   Gen Loss: 7.643831729888916   Contr Loss: 3.376070499420166\n",
      "-----------------------------------------------------------\n",
      "Iter: 69   Total Loss: 12.12498664855957   Gen Loss: 7.938148498535156   Contr Loss: 3.349470615386963\n",
      "-----------------------------------------------------------\n",
      "Iter: 70   Total Loss: 11.427284240722656   Gen Loss: 7.1178975105285645   Contr Loss: 3.447509288787842\n",
      "-----------------------------------------------------------\n",
      "Iter: 71   Total Loss: 10.861148834228516   Gen Loss: 6.628940582275391   Contr Loss: 3.3857665061950684\n",
      "-----------------------------------------------------------\n",
      "Iter: 72   Total Loss: 10.566835403442383   Gen Loss: 5.850493907928467   Contr Loss: 3.7730727195739746\n",
      "-----------------------------------------------------------\n",
      "Iter: 73   Total Loss: 11.651458740234375   Gen Loss: 7.381248474121094   Contr Loss: 3.416168212890625\n",
      "-----------------------------------------------------------\n",
      "Iter: 74   Total Loss: 11.61471939086914   Gen Loss: 7.552250385284424   Contr Loss: 3.2499752044677734\n",
      "-----------------------------------------------------------\n",
      "Iter: 75   Total Loss: 11.893330574035645   Gen Loss: 7.6011457443237305   Contr Loss: 3.4337477684020996\n",
      "-----------------------------------------------------------\n",
      "Iter: 76   Total Loss: 11.235834121704102   Gen Loss: 7.172041416168213   Contr Loss: 3.2510340213775635\n",
      "-----------------------------------------------------------\n",
      "Iter: 77   Total Loss: 11.296317100524902   Gen Loss: 7.11565637588501   Contr Loss: 3.3445284366607666\n",
      "-----------------------------------------------------------\n",
      "Iter: 78   Total Loss: 11.333621978759766   Gen Loss: 7.103949069976807   Contr Loss: 3.3837385177612305\n",
      "-----------------------------------------------------------\n",
      "Iter: 79   Total Loss: 11.026606559753418   Gen Loss: 6.804365158081055   Contr Loss: 3.377793312072754\n",
      "-----------------------------------------------------------\n",
      "Iter: 80   Total Loss: 11.515493392944336   Gen Loss: 7.281336784362793   Contr Loss: 3.387324810028076\n",
      "-----------------------------------------------------------\n",
      "Iter: 81   Total Loss: 12.075982093811035   Gen Loss: 8.032607078552246   Contr Loss: 3.2347002029418945\n",
      "-----------------------------------------------------------\n",
      "Iter: 82   Total Loss: 11.255661010742188   Gen Loss: 6.966400146484375   Contr Loss: 3.431408405303955\n",
      "-----------------------------------------------------------\n",
      "Iter: 83   Total Loss: 11.394718170166016   Gen Loss: 7.174309253692627   Contr Loss: 3.3763267993927\n",
      "-----------------------------------------------------------\n",
      "Iter: 84   Total Loss: 11.538829803466797   Gen Loss: 7.064907550811768   Contr Loss: 3.5791373252868652\n",
      "-----------------------------------------------------------\n",
      "Iter: 85   Total Loss: 11.27366828918457   Gen Loss: 7.157690048217773   Contr Loss: 3.2927823066711426\n",
      "-----------------------------------------------------------\n",
      "Iter: 86   Total Loss: 10.613823890686035   Gen Loss: 6.512991905212402   Contr Loss: 3.280665636062622\n",
      "-----------------------------------------------------------\n",
      "Iter: 87   Total Loss: 11.203413009643555   Gen Loss: 7.205124855041504   Contr Loss: 3.1986303329467773\n",
      "-----------------------------------------------------------\n",
      "Iter: 88   Total Loss: 11.480561256408691   Gen Loss: 7.332729339599609   Contr Loss: 3.318265438079834\n",
      "-----------------------------------------------------------\n",
      "Iter: 89   Total Loss: 11.010663986206055   Gen Loss: 6.900332450866699   Contr Loss: 3.2882657051086426\n",
      "-----------------------------------------------------------\n",
      "Iter: 90   Total Loss: 11.768108367919922   Gen Loss: 7.454518795013428   Contr Loss: 3.450871467590332\n",
      "-----------------------------------------------------------\n",
      "Iter: 91   Total Loss: 11.44141960144043   Gen Loss: 7.1072845458984375   Contr Loss: 3.4673078060150146\n",
      "-----------------------------------------------------------\n",
      "Iter: 92   Total Loss: 11.66099739074707   Gen Loss: 7.657109260559082   Contr Loss: 3.203110456466675\n",
      "-----------------------------------------------------------\n",
      "Iter: 93   Total Loss: 11.013507843017578   Gen Loss: 6.9449591636657715   Contr Loss: 3.2548394203186035\n",
      "-----------------------------------------------------------\n",
      "Iter: 94   Total Loss: 11.85507869720459   Gen Loss: 7.534584045410156   Contr Loss: 3.4563956260681152\n",
      "-----------------------------------------------------------\n",
      "Iter: 95   Total Loss: 11.418846130371094   Gen Loss: 7.3663763999938965   Contr Loss: 3.2419753074645996\n",
      "-----------------------------------------------------------\n",
      "Iter: 96   Total Loss: 11.720695495605469   Gen Loss: 7.500147342681885   Contr Loss: 3.376439094543457\n",
      "-----------------------------------------------------------\n",
      "Iter: 97   Total Loss: 11.1013765335083   Gen Loss: 7.016858100891113   Contr Loss: 3.2676146030426025\n",
      "-----------------------------------------------------------\n",
      "Iter: 98   Total Loss: 11.715572357177734   Gen Loss: 7.71092414855957   Contr Loss: 3.203718423843384\n",
      "-----------------------------------------------------------\n",
      "Iter: 99   Total Loss: 11.74157428741455   Gen Loss: 7.481144905090332   Contr Loss: 3.4083433151245117\n",
      "-----------------------------------------------------------\n",
      "Iter: 100   Total Loss: 11.169652938842773   Gen Loss: 7.042900085449219   Contr Loss: 3.3014018535614014\n",
      "-----------------------------------------------------------\n",
      "Iter: 101   Total Loss: 10.348175048828125   Gen Loss: 6.28037691116333   Contr Loss: 3.254239082336426\n",
      "-----------------------------------------------------------\n",
      "Iter: 102   Total Loss: 11.723633766174316   Gen Loss: 7.622562885284424   Contr Loss: 3.2808566093444824\n",
      "-----------------------------------------------------------\n",
      "Iter: 103   Total Loss: 11.840923309326172   Gen Loss: 7.783833980560303   Contr Loss: 3.2456717491149902\n",
      "-----------------------------------------------------------\n",
      "Iter: 104   Total Loss: 11.18056583404541   Gen Loss: 7.015542030334473   Contr Loss: 3.332019090652466\n",
      "-----------------------------------------------------------\n",
      "Iter: 105   Total Loss: 11.080911636352539   Gen Loss: 7.029893398284912   Contr Loss: 3.240814685821533\n",
      "-----------------------------------------------------------\n",
      "Iter: 106   Total Loss: 10.761613845825195   Gen Loss: 6.473907947540283   Contr Loss: 3.4301652908325195\n",
      "-----------------------------------------------------------\n",
      "Iter: 107   Total Loss: 11.83169937133789   Gen Loss: 7.561207294464111   Contr Loss: 3.416393756866455\n",
      "-----------------------------------------------------------\n",
      "Iter: 108   Total Loss: 10.957378387451172   Gen Loss: 6.800252437591553   Contr Loss: 3.325700283050537\n",
      "-----------------------------------------------------------\n",
      "Iter: 109   Total Loss: 11.405945777893066   Gen Loss: 7.368244171142578   Contr Loss: 3.230161190032959\n",
      "-----------------------------------------------------------\n",
      "Iter: 110   Total Loss: 11.612115859985352   Gen Loss: 6.987574100494385   Contr Loss: 3.6996331214904785\n",
      "-----------------------------------------------------------\n",
      "Iter: 111   Total Loss: 11.291467666625977   Gen Loss: 7.17075252532959   Contr Loss: 3.296572208404541\n",
      "-----------------------------------------------------------\n",
      "Iter: 112   Total Loss: 11.688312530517578   Gen Loss: 7.716273307800293   Contr Loss: 3.17763090133667\n",
      "-----------------------------------------------------------\n",
      "Iter: 113   Total Loss: 11.688241958618164   Gen Loss: 7.497336387634277   Contr Loss: 3.352724075317383\n",
      "-----------------------------------------------------------\n",
      "Iter: 114   Total Loss: 11.091226577758789   Gen Loss: 7.040744304656982   Contr Loss: 3.2403852939605713\n",
      "-----------------------------------------------------------\n",
      "Iter: 115   Total Loss: 11.046215057373047   Gen Loss: 6.883703708648682   Contr Loss: 3.3300085067749023\n",
      "-----------------------------------------------------------\n",
      "Iter: 116   Total Loss: 11.33283519744873   Gen Loss: 7.310874938964844   Contr Loss: 3.2175681591033936\n",
      "-----------------------------------------------------------\n",
      "Iter: 117   Total Loss: 11.083990097045898   Gen Loss: 6.8790693283081055   Contr Loss: 3.3639371395111084\n",
      "-----------------------------------------------------------\n",
      "Iter: 118   Total Loss: 11.086877822875977   Gen Loss: 7.034698009490967   Contr Loss: 3.24174427986145\n",
      "-----------------------------------------------------------\n",
      "Iter: 119   Total Loss: 11.167367935180664   Gen Loss: 6.800950050354004   Contr Loss: 3.493134021759033\n",
      "-----------------------------------------------------------\n",
      "Iter: 120   Total Loss: 11.842437744140625   Gen Loss: 7.0059661865234375   Contr Loss: 3.8691768646240234\n",
      "-----------------------------------------------------------\n",
      "Iter: 121   Total Loss: 11.228809356689453   Gen Loss: 7.15605354309082   Contr Loss: 3.2582051753997803\n",
      "-----------------------------------------------------------\n",
      "Iter: 122   Total Loss: 11.790454864501953   Gen Loss: 7.627975940704346   Contr Loss: 3.3299827575683594\n",
      "-----------------------------------------------------------\n",
      "Iter: 123   Total Loss: 11.863224029541016   Gen Loss: 7.303344249725342   Contr Loss: 3.6479034423828125\n",
      "-----------------------------------------------------------\n",
      "Iter: 124   Total Loss: 10.438279151916504   Gen Loss: 6.337396144866943   Contr Loss: 3.2807064056396484\n",
      "-----------------------------------------------------------\n",
      "Iter: 125   Total Loss: 11.731477737426758   Gen Loss: 7.52573823928833   Contr Loss: 3.3645918369293213\n",
      "-----------------------------------------------------------\n",
      "Iter: 126   Total Loss: 11.298389434814453   Gen Loss: 7.230721950531006   Contr Loss: 3.254133701324463\n",
      "-----------------------------------------------------------\n",
      "Iter: 127   Total Loss: 11.231419563293457   Gen Loss: 7.0551300048828125   Contr Loss: 3.341031551361084\n",
      "-----------------------------------------------------------\n",
      "Iter: 128   Total Loss: 11.382547378540039   Gen Loss: 7.3493475914001465   Contr Loss: 3.226560354232788\n",
      "-----------------------------------------------------------\n",
      "Iter: 129   Total Loss: 11.045867919921875   Gen Loss: 6.86661958694458   Contr Loss: 3.3433985710144043\n",
      "-----------------------------------------------------------\n",
      "Iter: 130   Total Loss: 11.20318603515625   Gen Loss: 7.0164594650268555   Contr Loss: 3.3493809700012207\n",
      "-----------------------------------------------------------\n",
      "Iter: 131   Total Loss: 11.268524169921875   Gen Loss: 6.9010491371154785   Contr Loss: 3.4939799308776855\n",
      "-----------------------------------------------------------\n",
      "Iter: 132   Total Loss: 11.730586051940918   Gen Loss: 7.542915344238281   Contr Loss: 3.3501367568969727\n",
      "-----------------------------------------------------------\n",
      "Iter: 133   Total Loss: 11.448678016662598   Gen Loss: 7.3587775230407715   Contr Loss: 3.2719204425811768\n",
      "-----------------------------------------------------------\n",
      "Iter: 134   Total Loss: 11.232826232910156   Gen Loss: 7.153680801391602   Contr Loss: 3.2633159160614014\n",
      "-----------------------------------------------------------\n",
      "Iter: 135   Total Loss: 11.28979778289795   Gen Loss: 7.275263786315918   Contr Loss: 3.2116270065307617\n",
      "-----------------------------------------------------------\n",
      "Iter: 136   Total Loss: 11.778142929077148   Gen Loss: 7.858972072601318   Contr Loss: 3.135336399078369\n",
      "-----------------------------------------------------------\n",
      "Iter: 137   Total Loss: 11.390841484069824   Gen Loss: 7.15782356262207   Contr Loss: 3.3864145278930664\n",
      "-----------------------------------------------------------\n",
      "Iter: 138   Total Loss: 11.502481460571289   Gen Loss: 7.467321395874023   Contr Loss: 3.2281277179718018\n",
      "-----------------------------------------------------------\n",
      "Iter: 139   Total Loss: 12.126218795776367   Gen Loss: 7.535978317260742   Contr Loss: 3.672191858291626\n",
      "-----------------------------------------------------------\n",
      "Iter: 140   Total Loss: 11.27157211303711   Gen Loss: 7.118686676025391   Contr Loss: 3.32230806350708\n",
      "-----------------------------------------------------------\n",
      "Iter: 141   Total Loss: 11.512641906738281   Gen Loss: 7.490421295166016   Contr Loss: 3.217776298522949\n",
      "-----------------------------------------------------------\n",
      "Iter: 142   Total Loss: 11.370532989501953   Gen Loss: 7.3319172859191895   Contr Loss: 3.2308928966522217\n",
      "-----------------------------------------------------------\n",
      "Iter: 143   Total Loss: 11.880317687988281   Gen Loss: 7.724816799163818   Contr Loss: 3.3244009017944336\n",
      "-----------------------------------------------------------\n",
      "Iter: 144   Total Loss: 11.273133277893066   Gen Loss: 7.210389614105225   Contr Loss: 3.250195026397705\n",
      "-----------------------------------------------------------\n",
      "Iter: 145   Total Loss: 9.772939682006836   Gen Loss: 5.714200496673584   Contr Loss: 3.2469918727874756\n",
      "-----------------------------------------------------------\n",
      "Iter: 146   Total Loss: 11.831920623779297   Gen Loss: 7.462802886962891   Contr Loss: 3.4952938556671143\n",
      "-----------------------------------------------------------\n",
      "Iter: 147   Total Loss: 11.642821311950684   Gen Loss: 7.419033050537109   Contr Loss: 3.379030704498291\n",
      "-----------------------------------------------------------\n",
      "Iter: 148   Total Loss: 11.085387229919434   Gen Loss: 7.1754374504089355   Contr Loss: 3.127959728240967\n",
      "-----------------------------------------------------------\n",
      "Iter: 149   Total Loss: 11.79305648803711   Gen Loss: 7.47927188873291   Contr Loss: 3.4510278701782227\n",
      "-----------------------------------------------------------\n",
      "Iter: 150   Total Loss: 11.754404067993164   Gen Loss: 6.773470401763916   Contr Loss: 3.9847469329833984\n",
      "-----------------------------------------------------------\n",
      "Iter: 151   Total Loss: 13.225927352905273   Gen Loss: 7.624665260314941   Contr Loss: 4.4810099601745605\n",
      "-----------------------------------------------------------\n",
      "Iter: 152   Total Loss: 11.432411193847656   Gen Loss: 7.384641170501709   Contr Loss: 3.238215446472168\n",
      "-----------------------------------------------------------\n",
      "Iter: 153   Total Loss: 11.166613578796387   Gen Loss: 7.076149940490723   Contr Loss: 3.2723708152770996\n",
      "-----------------------------------------------------------\n",
      "Iter: 154   Total Loss: 11.472765922546387   Gen Loss: 7.371457099914551   Contr Loss: 3.2810468673706055\n",
      "-----------------------------------------------------------\n",
      "Iter: 155   Total Loss: 11.106874465942383   Gen Loss: 6.9361891746521   Contr Loss: 3.336548328399658\n",
      "-----------------------------------------------------------\n",
      "Iter: 156   Total Loss: 11.56922435760498   Gen Loss: 7.416111946105957   Contr Loss: 3.3224899768829346\n",
      "-----------------------------------------------------------\n",
      "Iter: 157   Total Loss: 9.543373107910156   Gen Loss: 7.073355674743652   Contr Loss: 1.9760140180587769\n",
      "Epoch 1:  Train loss: 2.880535840988159   Train Contrastive Loss: 3.452144145965576   Train Generative Loss: 7.206961154937744]\n",
      "Epoch 1:  Val loss: 10.326748847961426   Val Contrastive Loss: 2.293469190597534   Val Generative Loss: 7.45991325378418]\n",
      "-----------------------------------------------------------\n",
      "Iter: 158   Total Loss: 11.951688766479492   Gen Loss: 7.91336727142334   Contr Loss: 3.2306575775146484\n",
      "-----------------------------------------------------------\n",
      "Iter: 159   Total Loss: 11.545370101928711   Gen Loss: 7.58278751373291   Contr Loss: 3.1700661182403564\n",
      "-----------------------------------------------------------\n",
      "Iter: 160   Total Loss: 11.570760726928711   Gen Loss: 7.168988227844238   Contr Loss: 3.5214180946350098\n",
      "-----------------------------------------------------------\n",
      "Iter: 161   Total Loss: 11.060272216796875   Gen Loss: 6.9177350997924805   Contr Loss: 3.314030170440674\n",
      "-----------------------------------------------------------\n",
      "Iter: 162   Total Loss: 10.834224700927734   Gen Loss: 7.109375   Contr Loss: 2.979879379272461\n",
      "-----------------------------------------------------------\n",
      "Iter: 163   Total Loss: 11.654218673706055   Gen Loss: 6.729981899261475   Contr Loss: 3.939389944076538\n",
      "-----------------------------------------------------------\n",
      "Iter: 164   Total Loss: 11.426351547241211   Gen Loss: 7.3774824142456055   Contr Loss: 3.2390949726104736\n",
      "-----------------------------------------------------------\n",
      "Iter: 165   Total Loss: 11.410476684570312   Gen Loss: 7.124244689941406   Contr Loss: 3.428986072540283\n",
      "-----------------------------------------------------------\n",
      "Iter: 166   Total Loss: 11.359901428222656   Gen Loss: 7.100956916809082   Contr Loss: 3.4071555137634277\n",
      "-----------------------------------------------------------\n",
      "Iter: 167   Total Loss: 11.117420196533203   Gen Loss: 7.315385341644287   Contr Loss: 3.041628360748291\n",
      "-----------------------------------------------------------\n",
      "Iter: 168   Total Loss: 11.625387191772461   Gen Loss: 7.336871147155762   Contr Loss: 3.4308128356933594\n",
      "-----------------------------------------------------------\n",
      "Iter: 169   Total Loss: 11.504413604736328   Gen Loss: 7.345790386199951   Contr Loss: 3.3268990516662598\n",
      "-----------------------------------------------------------\n",
      "Iter: 170   Total Loss: 11.2867431640625   Gen Loss: 6.902939319610596   Contr Loss: 3.5070433616638184\n",
      "-----------------------------------------------------------\n",
      "Iter: 171   Total Loss: 11.651592254638672   Gen Loss: 7.015235424041748   Contr Loss: 3.7090859413146973\n",
      "-----------------------------------------------------------\n",
      "Iter: 172   Total Loss: 10.71306037902832   Gen Loss: 6.706672668457031   Contr Loss: 3.2051100730895996\n",
      "-----------------------------------------------------------\n",
      "Iter: 173   Total Loss: 10.199919700622559   Gen Loss: 6.326252460479736   Contr Loss: 3.098933696746826\n",
      "-----------------------------------------------------------\n",
      "Iter: 174   Total Loss: 11.10162353515625   Gen Loss: 7.2149977684021   Contr Loss: 3.1093008518218994\n",
      "-----------------------------------------------------------\n",
      "Iter: 175   Total Loss: 11.201902389526367   Gen Loss: 7.120259761810303   Contr Loss: 3.2653145790100098\n",
      "-----------------------------------------------------------\n",
      "Iter: 176   Total Loss: 11.038972854614258   Gen Loss: 7.0486063957214355   Contr Loss: 3.1922926902770996\n",
      "-----------------------------------------------------------\n",
      "Iter: 177   Total Loss: 11.17487907409668   Gen Loss: 7.251468658447266   Contr Loss: 3.138728141784668\n",
      "-----------------------------------------------------------\n",
      "Iter: 178   Total Loss: 11.728775024414062   Gen Loss: 7.640840530395508   Contr Loss: 3.2703471183776855\n",
      "-----------------------------------------------------------\n",
      "Iter: 179   Total Loss: 11.515848159790039   Gen Loss: 7.319734573364258   Contr Loss: 3.35689115524292\n",
      "-----------------------------------------------------------\n",
      "Iter: 180   Total Loss: 10.792176246643066   Gen Loss: 6.699982643127441   Contr Loss: 3.2737550735473633\n",
      "-----------------------------------------------------------\n",
      "Iter: 181   Total Loss: 11.711054801940918   Gen Loss: 7.5728278160095215   Contr Loss: 3.3105814456939697\n",
      "-----------------------------------------------------------\n",
      "Iter: 182   Total Loss: 11.130605697631836   Gen Loss: 7.109158992767334   Contr Loss: 3.2171578407287598\n",
      "-----------------------------------------------------------\n",
      "Iter: 183   Total Loss: 10.939231872558594   Gen Loss: 6.848311901092529   Contr Loss: 3.272735834121704\n",
      "-----------------------------------------------------------\n",
      "Iter: 184   Total Loss: 11.469992637634277   Gen Loss: 7.324806213378906   Contr Loss: 3.3161489963531494\n",
      "-----------------------------------------------------------\n",
      "Iter: 185   Total Loss: 11.59537124633789   Gen Loss: 7.413601875305176   Contr Loss: 3.3454155921936035\n",
      "-----------------------------------------------------------\n",
      "Iter: 186   Total Loss: 10.94180679321289   Gen Loss: 6.889523983001709   Contr Loss: 3.2418267726898193\n",
      "-----------------------------------------------------------\n",
      "Iter: 187   Total Loss: 11.159757614135742   Gen Loss: 7.137829780578613   Contr Loss: 3.2175421714782715\n",
      "-----------------------------------------------------------\n",
      "Iter: 188   Total Loss: 11.205785751342773   Gen Loss: 7.126036167144775   Contr Loss: 3.2637996673583984\n",
      "-----------------------------------------------------------\n",
      "Iter: 189   Total Loss: 11.38205623626709   Gen Loss: 7.390939235687256   Contr Loss: 3.1928935050964355\n"
     ]
    }
   ],
   "source": [
    "train(model=model, opt=None, data=train_loader, val_data=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
