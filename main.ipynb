{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb886d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b019739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a884230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c36fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7dc2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41faf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01120acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6628e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b04191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only subset of data for now\n",
    "pre_train_data = pre_train_data.with_format(\"torch\")\n",
    "test_data = pre_train_data.select(range(11000, 12000))\n",
    "val_data = pre_train_data.select(range(10000, 11000))\n",
    "pre_train_data = pre_train_data.select(range(10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e9a51cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b2f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ddeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "#         text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=1)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None\n",
    "#     def encode_text(self, example):\n",
    "#         text = self.tokenizer(example, padding='max_length', max_length=max_seq_len, add_special_tokens=True) # hard-coded max_length for now\n",
    "#         bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "#          # add a bos token as well\n",
    "#         text = {\n",
    "#             \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "#             \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "#         }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00981577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "812a3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)\n",
    "    return images, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3b9ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "val_dataset_cleaned = PreTrainDataset(val_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=64, shuffle=True, collate_fn=remove_none_fn)\n",
    "val_loader = DataLoader(val_dataset_cleaned, batch_size=10, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4461936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1943702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_grad_norms(model, norm_type=2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            grad_norm = param.grad.norm(norm_type).item()\n",
    "            print(f\"{name}: grad norm = {grad_norm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ca6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, val_data, opt=None, lr=0.0001, weight_decay=0.00000, num_epochs=20, checkpoint_path='../checkpoints/'):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "#     n = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    batch_size = 64\n",
    "    n = 0\n",
    "    accumulation_steps = 4\n",
    "    if opt is not None:\n",
    "        optimizer = opt\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "\n",
    "\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32).to(device)\n",
    "            text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "#             print(text)\n",
    "\n",
    "#             if len(imgs) < batch_size:\n",
    "#                 # Last batch will have less images, text pairs since it will be the\n",
    "#                 # remainder of Total images / batch_size.\n",
    "\n",
    "#                 # Adjust the learning rate of the last batch by \n",
    "#                 # (size(last_batch) / batch_size) to account \n",
    "#                 # for the smaller size.\n",
    "#                 adj_lr = lr * (len(imgs) / batch_size)\n",
    "#                 optimizer = optim.AdamW(model.parameters(),\n",
    "#                     lr=adj_lr,\n",
    "#                     weight_decay=weight_decay)\n",
    "            # Since task is to predict next token, the labels will start form position 1\n",
    "            text_labels = text[:, 1:] \n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            total_loss = total_loss / accumulation_steps\n",
    "            \n",
    "            n += 1\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            print(f\"Iter: {n}   Total Loss: {total_loss.item() * accumulation_steps}   Gen Loss: {generative_loss.item()}   Contr Loss: {contrastive_loss.item()}\")\n",
    "#             total_loss.backward()\n",
    "            contrastive_loss.backward(retain_graph=True)\n",
    "            print(\"contrastive_norms\")\n",
    "            log_grad_norms(model)\n",
    "            generative_loss.backward(retain_graph=True)\n",
    "            print(\"generative_norms\")\n",
    "            log_grad_norms(model)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            i = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.grad is not None:\n",
    "#                     print(f\"{name}: grad norm = {param.grad.norm().item():.4f}\")\n",
    "#                 i += 1\n",
    "#                 if i > 10:\n",
    "#                     break\n",
    "            if n % accumulation_steps == 0: # accumulate gradients to artifically increase batch size for learning\n",
    "                optimizer.step()\n",
    "                scheduler.step(total_loss)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss.detach()\n",
    "            t_contrastive_loss += contrastive_loss.detach()\n",
    "            t_generative_loss += generative_loss.detach()\n",
    "            del imgs\n",
    "            del text\n",
    "            if n % 100 == 0:\n",
    "#                 torch.save(model.state_dict(), f\"{checkpoint_path}_iter_{n}\")\n",
    "                torch.save({\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        }, f\"{checkpoint_path}_model_checkpoint.pt\")\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss / len(data))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(data))\n",
    "        train_generative_losses.append(t_generative_loss / len(data))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "#         if epoch % 5 == 0: # save model every 5th epoch\n",
    "#         torch.save(model.state_dict(), f\"{checkpoint_path}_epoch_{epoch}\")\n",
    "        torch.save({\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }, f\"_model_checkpoint.pt\")\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(data), t_contrastive_loss / len(data), t_generative_loss / len(data)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(val_data), val_contrastive_loss / len(val_data), val_generative_loss / len(val_data)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3732c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df0db509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaMMUT\n",
    "model = MaMMUT(vocab_size=tokenizer.vocab_size, contrastive_loss_weight=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f6ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.load('../checkpoints/_model_checkpoint.pt', weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1019a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(c['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d93d5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(model.parameters(),\n",
    "                lr=0.0001,\n",
    "                weight_decay=0)\n",
    "opt.load_state_dict(c['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc0d34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 1   Total Loss: 18.854232788085938   Gen Loss: 10.431276321411133   Contr Loss: 4.211477756500244\n",
      "contrastive_norms\n",
      "pos_embedding: grad norm = 0.035248\n",
      "text_cls_token: grad norm = 0.244320\n",
      "vit.class_token: grad norm = 0.044123\n",
      "vit.conv_proj.weight: grad norm = 0.380109\n",
      "vit.conv_proj.bias: grad norm = 0.075046\n",
      "vit.encoder.pos_embedding: grad norm = 0.044734\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.020298\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.034181\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.711618\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.039051\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 1.448126\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias: grad norm = 0.056747\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.weight: grad norm = 0.036612\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.bias: grad norm = 0.028284\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.weight: grad norm = 1.323341\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.bias: grad norm = 0.037284\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.weight: grad norm = 1.339025\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.bias: grad norm = 0.031597\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.weight: grad norm = 0.016068\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.bias: grad norm = 0.011385\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight: grad norm = 0.602900\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias: grad norm = 0.014255\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight: grad norm = 0.929690\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias: grad norm = 0.023947\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.weight: grad norm = 0.028886\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.bias: grad norm = 0.017747\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.weight: grad norm = 1.038745\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.bias: grad norm = 0.023040\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.weight: grad norm = 0.982646\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.bias: grad norm = 0.013753\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.weight: grad norm = 0.010801\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.bias: grad norm = 0.006488\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight: grad norm = 0.412682\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias: grad norm = 0.007998\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight: grad norm = 0.565632\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias: grad norm = 0.012949\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.weight: grad norm = 0.017416\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.bias: grad norm = 0.011059\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.weight: grad norm = 0.624935\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.bias: grad norm = 0.014569\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.weight: grad norm = 0.626969\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.bias: grad norm = 0.011734\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.weight: grad norm = 0.008940\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.bias: grad norm = 0.005820\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight: grad norm = 0.329419\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias: grad norm = 0.007152\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight: grad norm = 0.449996\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias: grad norm = 0.011506\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.weight: grad norm = 0.013367\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.bias: grad norm = 0.009337\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.weight: grad norm = 0.488271\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.bias: grad norm = 0.012362\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.weight: grad norm = 0.496043\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.bias: grad norm = 0.011215\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.weight: grad norm = 0.007488\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.bias: grad norm = 0.005252\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight: grad norm = 0.275906\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias: grad norm = 0.006606\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight: grad norm = 0.399694\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias: grad norm = 0.011125\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.weight: grad norm = 0.011607\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.bias: grad norm = 0.008967\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.weight: grad norm = 0.434293\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.bias: grad norm = 0.011859\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.weight: grad norm = 0.435859\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.bias: grad norm = 0.011038\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.weight: grad norm = 0.007231\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.bias: grad norm = 0.005205\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight: grad norm = 0.262485\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias: grad norm = 0.006589\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight: grad norm = 0.352935\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias: grad norm = 0.011037\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.weight: grad norm = 0.011474\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.bias: grad norm = 0.008448\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.weight: grad norm = 0.397112\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.bias: grad norm = 0.011082\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.weight: grad norm = 0.403384\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.bias: grad norm = 0.011005\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.weight: grad norm = 0.006409\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.bias: grad norm = 0.004975\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight: grad norm = 0.247355\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias: grad norm = 0.006531\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight: grad norm = 0.341069\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias: grad norm = 0.011004\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.weight: grad norm = 0.010966\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.bias: grad norm = 0.008014\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.weight: grad norm = 0.379172\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.bias: grad norm = 0.010607\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.weight: grad norm = 0.401227\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.bias: grad norm = 0.011021\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.weight: grad norm = 0.006563\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.bias: grad norm = 0.005021\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight: grad norm = 0.237768\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias: grad norm = 0.006608\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight: grad norm = 0.329137\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias: grad norm = 0.011024\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.weight: grad norm = 0.009991\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.bias: grad norm = 0.007888\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.weight: grad norm = 0.359396\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.bias: grad norm = 0.010352\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.weight: grad norm = 0.371153\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.bias: grad norm = 0.011040\n",
      "vit.encoder.ln.weight: grad norm = 0.162447\n",
      "vit.encoder.ln.bias: grad norm = 0.117663\n",
      "ifs2tfs.weight: grad norm = 8.670148\n",
      "ifs2tfs.bias: grad norm = 0.204658\n",
      "latent_text_features.weight: grad norm = 6.222881\n",
      "latent_text_features.bias: grad norm = 0.372588\n",
      "text_embeddings.weight: grad norm = 0.041702\n",
      "contrastive_layernorm.weight: grad norm = 0.247491\n",
      "contrastive_layernorm.bias: grad norm = 0.237947\n",
      "text_decoder_layers.0.k.weight: grad norm = 0.088931\n",
      "text_decoder_layers.0.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q.weight: grad norm = 0.082871\n",
      "text_decoder_layers.0.q.bias: grad norm = 0.005273\n",
      "text_decoder_layers.0.v.weight: grad norm = 0.646140\n",
      "text_decoder_layers.0.v.bias: grad norm = 0.100101\n",
      "text_decoder_layers.0.MHA_1.in_proj_weight: grad norm = 0.559490\n",
      "text_decoder_layers.0.MHA_1.in_proj_bias: grad norm = 0.139541\n",
      "text_decoder_layers.0.MHA_1.out_proj.weight: grad norm = 0.744211\n",
      "text_decoder_layers.0.MHA_1.out_proj.bias: grad norm = 0.263492\n",
      "text_decoder_layers.0.layer_norm1.weight: grad norm = 0.242400\n",
      "text_decoder_layers.0.layer_norm1.bias: grad norm = 0.255327\n",
      "text_decoder_layers.0.fc1.weight: grad norm = 1.578779\n",
      "text_decoder_layers.0.fc1.bias: grad norm = 0.103123\n",
      "text_decoder_layers.0.fc2.weight: grad norm = 4.358873\n",
      "text_decoder_layers.0.fc2.bias: grad norm = 0.250665\n",
      "text_decoder_layers.0.layer_norm_ff.weight: grad norm = 0.245917\n",
      "text_decoder_layers.0.layer_norm_ff.bias: grad norm = 0.258063\n",
      "text_decoder_layers.1.k.weight: grad norm = 0.086945\n",
      "text_decoder_layers.1.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.1.q.weight: grad norm = 0.086047\n",
      "text_decoder_layers.1.q.bias: grad norm = 0.005803\n",
      "text_decoder_layers.1.v.weight: grad norm = 0.737462\n",
      "text_decoder_layers.1.v.bias: grad norm = 0.099093\n",
      "text_decoder_layers.1.MHA_1.in_proj_weight: grad norm = 0.622504\n",
      "text_decoder_layers.1.MHA_1.in_proj_bias: grad norm = 0.134457\n",
      "text_decoder_layers.1.MHA_1.out_proj.weight: grad norm = 0.744126\n",
      "text_decoder_layers.1.MHA_1.out_proj.bias: grad norm = 0.246500\n",
      "text_decoder_layers.1.layer_norm1.weight: grad norm = 0.243154\n",
      "text_decoder_layers.1.layer_norm1.bias: grad norm = 0.246450\n",
      "text_decoder_layers.1.fc1.weight: grad norm = 1.584586\n",
      "text_decoder_layers.1.fc1.bias: grad norm = 0.099465\n",
      "text_decoder_layers.1.fc2.weight: grad norm = 4.206179\n",
      "text_decoder_layers.1.fc2.bias: grad norm = 0.238768\n",
      "text_decoder_layers.1.layer_norm_ff.weight: grad norm = 0.253134\n",
      "text_decoder_layers.1.layer_norm_ff.bias: grad norm = 0.245851\n",
      "text_decoder_layers.2.k.weight: grad norm = 0.082651\n",
      "text_decoder_layers.2.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q.weight: grad norm = 0.112504\n",
      "text_decoder_layers.2.q.bias: grad norm = 0.007341\n",
      "text_decoder_layers.2.v.weight: grad norm = 0.767929\n",
      "text_decoder_layers.2.v.bias: grad norm = 0.095276\n",
      "text_decoder_layers.2.MHA_1.in_proj_weight: grad norm = 0.676190\n",
      "text_decoder_layers.2.MHA_1.in_proj_bias: grad norm = 0.136335\n",
      "text_decoder_layers.2.MHA_1.out_proj.weight: grad norm = 0.813624\n",
      "text_decoder_layers.2.MHA_1.out_proj.bias: grad norm = 0.243317\n",
      "text_decoder_layers.2.layer_norm1.weight: grad norm = 0.252312\n",
      "text_decoder_layers.2.layer_norm1.bias: grad norm = 0.244202\n",
      "text_decoder_layers.2.fc1.weight: grad norm = 1.551553\n",
      "text_decoder_layers.2.fc1.bias: grad norm = 0.097338\n",
      "text_decoder_layers.2.fc2.weight: grad norm = 4.154553\n",
      "text_decoder_layers.2.fc2.bias: grad norm = 0.239549\n",
      "text_decoder_layers.2.layer_norm_ff.weight: grad norm = 0.238118\n",
      "text_decoder_layers.2.layer_norm_ff.bias: grad norm = 0.242684\n",
      "text_decoder_layers.3.k.weight: grad norm = 0.077798\n",
      "text_decoder_layers.3.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.3.q.weight: grad norm = 0.087840\n",
      "text_decoder_layers.3.q.bias: grad norm = 0.005488\n",
      "text_decoder_layers.3.v.weight: grad norm = 0.780032\n",
      "text_decoder_layers.3.v.bias: grad norm = 0.095465\n",
      "text_decoder_layers.3.MHA_1.in_proj_weight: grad norm = 0.641497\n",
      "text_decoder_layers.3.MHA_1.in_proj_bias: grad norm = 0.136506\n",
      "text_decoder_layers.3.MHA_1.out_proj.weight: grad norm = 0.753067\n",
      "text_decoder_layers.3.MHA_1.out_proj.bias: grad norm = 0.232660\n",
      "text_decoder_layers.3.layer_norm1.weight: grad norm = 0.240968\n",
      "text_decoder_layers.3.layer_norm1.bias: grad norm = 0.233800\n",
      "text_decoder_layers.3.fc1.weight: grad norm = 1.517184\n",
      "text_decoder_layers.3.fc1.bias: grad norm = 0.093773\n",
      "text_decoder_layers.3.fc2.weight: grad norm = 4.335126\n",
      "text_decoder_layers.3.fc2.bias: grad norm = 0.228918\n",
      "text_decoder_layers.3.layer_norm_ff.weight: grad norm = 0.243221\n",
      "text_decoder_layers.3.layer_norm_ff.bias: grad norm = 0.236033\n",
      "generative_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embedding: grad norm = 0.037735\n",
      "text_cls_token: grad norm = 0.244320\n",
      "vit.class_token: grad norm = 0.046358\n",
      "vit.conv_proj.weight: grad norm = 0.407854\n",
      "vit.conv_proj.bias: grad norm = 0.075534\n",
      "vit.encoder.pos_embedding: grad norm = 0.046944\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.021165\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.034864\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.752884\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.040038\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 1.524135\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias: grad norm = 0.058481\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.weight: grad norm = 0.038067\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.bias: grad norm = 0.029651\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.weight: grad norm = 1.365962\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.bias: grad norm = 0.039167\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.weight: grad norm = 1.401141\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.bias: grad norm = 0.034778\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.weight: grad norm = 0.017624\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.bias: grad norm = 0.013102\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight: grad norm = 0.647666\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias: grad norm = 0.016589\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight: grad norm = 1.015336\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias: grad norm = 0.027669\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.weight: grad norm = 0.030990\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.bias: grad norm = 0.020063\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.weight: grad norm = 1.087202\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.bias: grad norm = 0.025765\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.weight: grad norm = 1.050025\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.bias: grad norm = 0.018898\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.weight: grad norm = 0.013219\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.bias: grad norm = 0.009498\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight: grad norm = 0.476790\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias: grad norm = 0.011537\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight: grad norm = 0.667085\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias: grad norm = 0.018506\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.weight: grad norm = 0.018790\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.bias: grad norm = 0.013000\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.weight: grad norm = 0.689004\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.bias: grad norm = 0.017826\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.weight: grad norm = 0.721676\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.bias: grad norm = 0.017787\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.weight: grad norm = 0.011068\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.bias: grad norm = 0.008506\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight: grad norm = 0.393870\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias: grad norm = 0.010647\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight: grad norm = 0.560507\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias: grad norm = 0.017609\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.weight: grad norm = 0.016180\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.bias: grad norm = 0.013243\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.weight: grad norm = 0.576995\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.bias: grad norm = 0.016491\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.weight: grad norm = 0.598013\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.bias: grad norm = 0.017374\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.weight: grad norm = 0.009163\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.bias: grad norm = 0.007534\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight: grad norm = 0.347601\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias: grad norm = 0.010155\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight: grad norm = 0.519864\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias: grad norm = 0.017275\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.weight: grad norm = 0.014916\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.bias: grad norm = 0.012389\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.weight: grad norm = 0.528972\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.bias: grad norm = 0.016095\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.weight: grad norm = 0.548630\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.bias: grad norm = 0.017179\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.weight: grad norm = 0.009190\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.bias: grad norm = 0.008155\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight: grad norm = 0.340706\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias: grad norm = 0.010262\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight: grad norm = 0.471222\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias: grad norm = 0.017157\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.weight: grad norm = 0.013826\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.bias: grad norm = 0.011541\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.weight: grad norm = 0.482365\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.bias: grad norm = 0.014791\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.weight: grad norm = 0.516113\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.bias: grad norm = 0.017049\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.weight: grad norm = 0.008635\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.bias: grad norm = 0.007846\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight: grad norm = 0.328496\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias: grad norm = 0.010196\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight: grad norm = 0.460077\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias: grad norm = 0.017088\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.weight: grad norm = 0.013583\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.bias: grad norm = 0.010964\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.weight: grad norm = 0.473768\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.bias: grad norm = 0.014707\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.weight: grad norm = 0.522524\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.bias: grad norm = 0.017116\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.weight: grad norm = 0.009023\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.bias: grad norm = 0.007800\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight: grad norm = 0.320936\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias: grad norm = 0.010191\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight: grad norm = 0.446406\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias: grad norm = 0.017098\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.weight: grad norm = 0.013044\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.bias: grad norm = 0.011584\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.weight: grad norm = 0.470689\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.bias: grad norm = 0.015087\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.weight: grad norm = 0.488949\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.bias: grad norm = 0.017162\n",
      "vit.encoder.ln.weight: grad norm = 0.225194\n",
      "vit.encoder.ln.bias: grad norm = 0.197713\n",
      "ifs2tfs.weight: grad norm = 8.670148\n",
      "ifs2tfs.bias: grad norm = 0.204658\n",
      "final_layernorm.weight: grad norm = 0.104786\n",
      "final_layernorm.bias: grad norm = 0.119511\n",
      "latent_text_features.weight: grad norm = 6.222881\n",
      "latent_text_features.bias: grad norm = 0.372588\n",
      "text_embeddings.weight: grad norm = 0.116202\n",
      "contrastive_layernorm.weight: grad norm = 0.247491\n",
      "contrastive_layernorm.bias: grad norm = 0.237947\n",
      "text_decoder_layers.0.k.weight: grad norm = 0.098948\n",
      "text_decoder_layers.0.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q.weight: grad norm = 0.092396\n",
      "text_decoder_layers.0.q.bias: grad norm = 0.006795\n",
      "text_decoder_layers.0.v.weight: grad norm = 0.830956\n",
      "text_decoder_layers.0.v.bias: grad norm = 0.221573\n",
      "text_decoder_layers.0.MHA_1.in_proj_weight: grad norm = 0.748655\n",
      "text_decoder_layers.0.MHA_1.in_proj_bias: grad norm = 0.327041\n",
      "text_decoder_layers.0.MHA_1.out_proj.weight: grad norm = 0.964380\n",
      "text_decoder_layers.0.MHA_1.out_proj.bias: grad norm = 0.513408\n",
      "text_decoder_layers.0.layer_norm1.weight: grad norm = 0.268981\n",
      "text_decoder_layers.0.layer_norm1.bias: grad norm = 0.511025\n",
      "text_decoder_layers.0.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.v_cross_attn.weight: grad norm = 5.323436\n",
      "text_decoder_layers.0.v_cross_attn.bias: grad norm = 0.191497\n",
      "text_decoder_layers.0.cross_attn.in_proj_weight: grad norm = 2.310301\n",
      "text_decoder_layers.0.cross_attn.in_proj_bias: grad norm = 0.267994\n",
      "text_decoder_layers.0.cross_attn.out_proj.weight: grad norm = 2.887365\n",
      "text_decoder_layers.0.cross_attn.out_proj.bias: grad norm = 0.453484\n",
      "text_decoder_layers.0.layer_norm_cross_attn.weight: grad norm = 0.150364\n",
      "text_decoder_layers.0.layer_norm_cross_attn.bias: grad norm = 0.463800\n",
      "text_decoder_layers.0.fc1.weight: grad norm = 1.793343\n",
      "text_decoder_layers.0.fc1.bias: grad norm = 0.170763\n",
      "text_decoder_layers.0.fc2.weight: grad norm = 6.750014\n",
      "text_decoder_layers.0.fc2.bias: grad norm = 0.515066\n",
      "text_decoder_layers.0.layer_norm_ff.weight: grad norm = 0.291005\n",
      "text_decoder_layers.0.layer_norm_ff.bias: grad norm = 0.528300\n",
      "text_decoder_layers.1.k.weight: grad norm = 0.093500\n",
      "text_decoder_layers.1.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.1.q.weight: grad norm = 0.093433\n",
      "text_decoder_layers.1.q.bias: grad norm = 0.005814\n",
      "text_decoder_layers.1.v.weight: grad norm = 1.231984\n",
      "text_decoder_layers.1.v.bias: grad norm = 0.219507\n",
      "text_decoder_layers.1.MHA_1.in_proj_weight: grad norm = 0.978219\n",
      "text_decoder_layers.1.MHA_1.in_proj_bias: grad norm = 0.308601\n",
      "text_decoder_layers.1.MHA_1.out_proj.weight: grad norm = 1.098848\n",
      "text_decoder_layers.1.MHA_1.out_proj.bias: grad norm = 0.508349\n",
      "text_decoder_layers.1.layer_norm1.weight: grad norm = 0.282636\n",
      "text_decoder_layers.1.layer_norm1.bias: grad norm = 0.509470\n",
      "text_decoder_layers.1.fc1.weight: grad norm = 1.810117\n",
      "text_decoder_layers.1.fc1.bias: grad norm = 0.162665\n",
      "text_decoder_layers.1.fc2.weight: grad norm = 6.554308\n",
      "text_decoder_layers.1.fc2.bias: grad norm = 0.496077\n",
      "text_decoder_layers.1.layer_norm_ff.weight: grad norm = 0.296155\n",
      "text_decoder_layers.1.layer_norm_ff.bias: grad norm = 0.508654\n",
      "text_decoder_layers.2.k.weight: grad norm = 0.090648\n",
      "text_decoder_layers.2.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q.weight: grad norm = 0.118145\n",
      "text_decoder_layers.2.q.bias: grad norm = 0.008769\n",
      "text_decoder_layers.2.v.weight: grad norm = 1.193742\n",
      "text_decoder_layers.2.v.bias: grad norm = 0.198061\n",
      "text_decoder_layers.2.MHA_1.in_proj_weight: grad norm = 1.017959\n",
      "text_decoder_layers.2.MHA_1.in_proj_bias: grad norm = 0.278545\n",
      "text_decoder_layers.2.MHA_1.out_proj.weight: grad norm = 1.322094\n",
      "text_decoder_layers.2.MHA_1.out_proj.bias: grad norm = 0.494674\n",
      "text_decoder_layers.2.layer_norm1.weight: grad norm = 0.301048\n",
      "text_decoder_layers.2.layer_norm1.bias: grad norm = 0.498545\n",
      "text_decoder_layers.2.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.v_cross_attn.weight: grad norm = 5.585979\n",
      "text_decoder_layers.2.v_cross_attn.bias: grad norm = 0.200940\n",
      "text_decoder_layers.2.cross_attn.in_proj_weight: grad norm = 2.680474\n",
      "text_decoder_layers.2.cross_attn.in_proj_bias: grad norm = 0.275699\n",
      "text_decoder_layers.2.cross_attn.out_proj.weight: grad norm = 3.255826\n",
      "text_decoder_layers.2.cross_attn.out_proj.bias: grad norm = 0.456075\n",
      "text_decoder_layers.2.layer_norm_cross_attn.weight: grad norm = 0.215494\n",
      "text_decoder_layers.2.layer_norm_cross_attn.bias: grad norm = 0.476098\n",
      "text_decoder_layers.2.fc1.weight: grad norm = 1.939091\n",
      "text_decoder_layers.2.fc1.bias: grad norm = 0.170458\n",
      "text_decoder_layers.2.fc2.weight: grad norm = 6.923932\n",
      "text_decoder_layers.2.fc2.bias: grad norm = 0.508323\n",
      "text_decoder_layers.2.layer_norm_ff.weight: grad norm = 0.318772\n",
      "text_decoder_layers.2.layer_norm_ff.bias: grad norm = 0.522807\n",
      "text_decoder_layers.3.k.weight: grad norm = 0.086338\n",
      "text_decoder_layers.3.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.3.q.weight: grad norm = 0.094611\n",
      "text_decoder_layers.3.q.bias: grad norm = 0.007169\n",
      "text_decoder_layers.3.v.weight: grad norm = 1.747550\n",
      "text_decoder_layers.3.v.bias: grad norm = 0.234496\n",
      "text_decoder_layers.3.MHA_1.in_proj_weight: grad norm = 1.283282\n",
      "text_decoder_layers.3.MHA_1.in_proj_bias: grad norm = 0.314369\n",
      "text_decoder_layers.3.MHA_1.out_proj.weight: grad norm = 1.454098\n",
      "text_decoder_layers.3.MHA_1.out_proj.bias: grad norm = 0.514468\n",
      "text_decoder_layers.3.layer_norm1.weight: grad norm = 0.325014\n",
      "text_decoder_layers.3.layer_norm1.bias: grad norm = 0.516272\n",
      "text_decoder_layers.3.fc1.weight: grad norm = 1.929006\n",
      "text_decoder_layers.3.fc1.bias: grad norm = 0.173924\n",
      "text_decoder_layers.3.fc2.weight: grad norm = 7.156120\n",
      "text_decoder_layers.3.fc2.bias: grad norm = 0.514537\n",
      "text_decoder_layers.3.layer_norm_ff.weight: grad norm = 0.335900\n",
      "text_decoder_layers.3.layer_norm_ff.bias: grad norm = 0.531563\n",
      "decoder_output_features_to_text_tokens_layer.weight: grad norm = 2.902133\n",
      "decoder_output_features_to_text_tokens_layer.bias: grad norm = 0.219164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 2   Total Loss: 18.57837677001953   Gen Loss: 10.146315574645996   Contr Loss: 4.216030120849609\n",
      "contrastive_norms\n",
      "pos_embedding: grad norm = 0.022373\n",
      "text_cls_token: grad norm = 0.081758\n",
      "vit.class_token: grad norm = 0.017312\n",
      "vit.conv_proj.weight: grad norm = 0.157158\n",
      "vit.conv_proj.bias: grad norm = 0.026102\n",
      "vit.encoder.pos_embedding: grad norm = 0.017479\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.009651\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.013896\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.329171\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.014815\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.601913\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias: grad norm = 0.018694\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.weight: grad norm = 0.011209\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.bias: grad norm = 0.009291\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.weight: grad norm = 0.413271\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.bias: grad norm = 0.012103\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.weight: grad norm = 0.426043\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.bias: grad norm = 0.010849\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.weight: grad norm = 0.006120\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.bias: grad norm = 0.004479\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight: grad norm = 0.217045\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias: grad norm = 0.005376\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight: grad norm = 0.327996\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias: grad norm = 0.008730\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.weight: grad norm = 0.008798\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.bias: grad norm = 0.006240\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.weight: grad norm = 0.304320\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.bias: grad norm = 0.007686\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.weight: grad norm = 0.298294\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.bias: grad norm = 0.006633\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.weight: grad norm = 0.004327\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.bias: grad norm = 0.003199\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight: grad norm = 0.157858\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias: grad norm = 0.003960\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight: grad norm = 0.234078\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias: grad norm = 0.006505\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.weight: grad norm = 0.005911\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.bias: grad norm = 0.004517\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.weight: grad norm = 0.216714\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.bias: grad norm = 0.005961\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.weight: grad norm = 0.229831\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.bias: grad norm = 0.006331\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.weight: grad norm = 0.003935\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.bias: grad norm = 0.003078\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight: grad norm = 0.139584\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias: grad norm = 0.003957\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight: grad norm = 0.198339\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias: grad norm = 0.006270\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.weight: grad norm = 0.005214\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.bias: grad norm = 0.004453\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.weight: grad norm = 0.189547\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.bias: grad norm = 0.005656\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.weight: grad norm = 0.198192\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.bias: grad norm = 0.006208\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.weight: grad norm = 0.003325\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.bias: grad norm = 0.002837\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight: grad norm = 0.123831\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias: grad norm = 0.003685\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight: grad norm = 0.184254\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias: grad norm = 0.006170\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.weight: grad norm = 0.004683\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.bias: grad norm = 0.004026\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.weight: grad norm = 0.172046\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.bias: grad norm = 0.005338\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.weight: grad norm = 0.185215\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.bias: grad norm = 0.006160\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.weight: grad norm = 0.003305\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.bias: grad norm = 0.002932\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight: grad norm = 0.121076\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias: grad norm = 0.003666\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight: grad norm = 0.168414\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias: grad norm = 0.006141\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.weight: grad norm = 0.004844\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.bias: grad norm = 0.004159\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.weight: grad norm = 0.165949\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.bias: grad norm = 0.005265\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.weight: grad norm = 0.177416\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.bias: grad norm = 0.006146\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.weight: grad norm = 0.003205\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.bias: grad norm = 0.002824\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight: grad norm = 0.115688\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias: grad norm = 0.003589\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight: grad norm = 0.164727\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias: grad norm = 0.006165\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.weight: grad norm = 0.004494\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.bias: grad norm = 0.003828\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.weight: grad norm = 0.160725\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.bias: grad norm = 0.005128\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.weight: grad norm = 0.180053\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.bias: grad norm = 0.006156\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.weight: grad norm = 0.003119\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.bias: grad norm = 0.002798\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight: grad norm = 0.114060\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias: grad norm = 0.003626\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight: grad norm = 0.159388\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias: grad norm = 0.006159\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.weight: grad norm = 0.004352\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.bias: grad norm = 0.003908\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.weight: grad norm = 0.157651\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.bias: grad norm = 0.005139\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.weight: grad norm = 0.168452\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.bias: grad norm = 0.006175\n",
      "vit.encoder.ln.weight: grad norm = 0.077522\n",
      "vit.encoder.ln.bias: grad norm = 0.069785\n",
      "ifs2tfs.weight: grad norm = 3.509693\n",
      "ifs2tfs.bias: grad norm = 0.101764\n",
      "final_layernorm.weight: grad norm = 0.024533\n",
      "final_layernorm.bias: grad norm = 0.027980\n",
      "latent_text_features.weight: grad norm = 2.214723\n",
      "latent_text_features.bias: grad norm = 0.123787\n",
      "text_embeddings.weight: grad norm = 0.032811\n",
      "contrastive_layernorm.weight: grad norm = 0.085619\n",
      "contrastive_layernorm.bias: grad norm = 0.079870\n",
      "text_decoder_layers.0.k.weight: grad norm = 0.046191\n",
      "text_decoder_layers.0.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q.weight: grad norm = 0.041714\n",
      "text_decoder_layers.0.q.bias: grad norm = 0.002833\n",
      "text_decoder_layers.0.v.weight: grad norm = 0.338687\n",
      "text_decoder_layers.0.v.bias: grad norm = 0.059784\n",
      "text_decoder_layers.0.MHA_1.in_proj_weight: grad norm = 0.306113\n",
      "text_decoder_layers.0.MHA_1.in_proj_bias: grad norm = 0.087607\n",
      "text_decoder_layers.0.MHA_1.out_proj.weight: grad norm = 0.387201\n",
      "text_decoder_layers.0.MHA_1.out_proj.bias: grad norm = 0.138275\n",
      "text_decoder_layers.0.layer_norm1.weight: grad norm = 0.088546\n",
      "text_decoder_layers.0.layer_norm1.bias: grad norm = 0.136753\n",
      "text_decoder_layers.0.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.v_cross_attn.weight: grad norm = 1.246325\n",
      "text_decoder_layers.0.v_cross_attn.bias: grad norm = 0.044833\n",
      "text_decoder_layers.0.cross_attn.in_proj_weight: grad norm = 0.540889\n",
      "text_decoder_layers.0.cross_attn.in_proj_bias: grad norm = 0.062743\n",
      "text_decoder_layers.0.cross_attn.out_proj.weight: grad norm = 0.675991\n",
      "text_decoder_layers.0.cross_attn.out_proj.bias: grad norm = 0.106170\n",
      "text_decoder_layers.0.layer_norm_cross_attn.weight: grad norm = 0.035203\n",
      "text_decoder_layers.0.layer_norm_cross_attn.bias: grad norm = 0.108585\n",
      "text_decoder_layers.0.fc1.weight: grad norm = 0.593155\n",
      "text_decoder_layers.0.fc1.bias: grad norm = 0.048250\n",
      "text_decoder_layers.0.fc2.weight: grad norm = 1.925224\n",
      "text_decoder_layers.0.fc2.bias: grad norm = 0.136763\n",
      "text_decoder_layers.0.layer_norm_ff.weight: grad norm = 0.090563\n",
      "text_decoder_layers.0.layer_norm_ff.bias: grad norm = 0.140351\n",
      "text_decoder_layers.1.k.weight: grad norm = 0.046814\n",
      "text_decoder_layers.1.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.1.q.weight: grad norm = 0.049562\n",
      "text_decoder_layers.1.q.bias: grad norm = 0.003126\n",
      "text_decoder_layers.1.v.weight: grad norm = 0.412002\n",
      "text_decoder_layers.1.v.bias: grad norm = 0.056966\n",
      "text_decoder_layers.1.MHA_1.in_proj_weight: grad norm = 0.328003\n",
      "text_decoder_layers.1.MHA_1.in_proj_bias: grad norm = 0.079986\n",
      "text_decoder_layers.1.MHA_1.out_proj.weight: grad norm = 0.384013\n",
      "text_decoder_layers.1.MHA_1.out_proj.bias: grad norm = 0.134396\n",
      "text_decoder_layers.1.layer_norm1.weight: grad norm = 0.088968\n",
      "text_decoder_layers.1.layer_norm1.bias: grad norm = 0.134643\n",
      "text_decoder_layers.1.fc1.weight: grad norm = 0.607737\n",
      "text_decoder_layers.1.fc1.bias: grad norm = 0.046484\n",
      "text_decoder_layers.1.fc2.weight: grad norm = 1.867709\n",
      "text_decoder_layers.1.fc2.bias: grad norm = 0.130198\n",
      "text_decoder_layers.1.layer_norm_ff.weight: grad norm = 0.092406\n",
      "text_decoder_layers.1.layer_norm_ff.bias: grad norm = 0.133652\n",
      "text_decoder_layers.2.k.weight: grad norm = 0.038013\n",
      "text_decoder_layers.2.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q.weight: grad norm = 0.049290\n",
      "text_decoder_layers.2.q.bias: grad norm = 0.003349\n",
      "text_decoder_layers.2.v.weight: grad norm = 0.416554\n",
      "text_decoder_layers.2.v.bias: grad norm = 0.051345\n",
      "text_decoder_layers.2.MHA_1.in_proj_weight: grad norm = 0.343462\n",
      "text_decoder_layers.2.MHA_1.in_proj_bias: grad norm = 0.072817\n",
      "text_decoder_layers.2.MHA_1.out_proj.weight: grad norm = 0.420618\n",
      "text_decoder_layers.2.MHA_1.out_proj.bias: grad norm = 0.129375\n",
      "text_decoder_layers.2.layer_norm1.weight: grad norm = 0.093314\n",
      "text_decoder_layers.2.layer_norm1.bias: grad norm = 0.130372\n",
      "text_decoder_layers.2.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.v_cross_attn.weight: grad norm = 1.307792\n",
      "text_decoder_layers.2.v_cross_attn.bias: grad norm = 0.047044\n",
      "text_decoder_layers.2.cross_attn.in_proj_weight: grad norm = 0.627554\n",
      "text_decoder_layers.2.cross_attn.in_proj_bias: grad norm = 0.064547\n",
      "text_decoder_layers.2.cross_attn.out_proj.weight: grad norm = 0.762255\n",
      "text_decoder_layers.2.cross_attn.out_proj.bias: grad norm = 0.106777\n",
      "text_decoder_layers.2.layer_norm_cross_attn.weight: grad norm = 0.050451\n",
      "text_decoder_layers.2.layer_norm_cross_attn.bias: grad norm = 0.111464\n",
      "text_decoder_layers.2.fc1.weight: grad norm = 0.616564\n",
      "text_decoder_layers.2.fc1.bias: grad norm = 0.046711\n",
      "text_decoder_layers.2.fc2.weight: grad norm = 1.915274\n",
      "text_decoder_layers.2.fc2.bias: grad norm = 0.131222\n",
      "text_decoder_layers.2.layer_norm_ff.weight: grad norm = 0.093082\n",
      "text_decoder_layers.2.layer_norm_ff.bias: grad norm = 0.134614\n",
      "text_decoder_layers.3.k.weight: grad norm = 0.040100\n",
      "text_decoder_layers.3.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.3.q.weight: grad norm = 0.040881\n",
      "text_decoder_layers.3.q.bias: grad norm = 0.002811\n",
      "text_decoder_layers.3.v.weight: grad norm = 0.501023\n",
      "text_decoder_layers.3.v.bias: grad norm = 0.059342\n",
      "text_decoder_layers.3.MHA_1.in_proj_weight: grad norm = 0.385851\n",
      "text_decoder_layers.3.MHA_1.in_proj_bias: grad norm = 0.080871\n",
      "text_decoder_layers.3.MHA_1.out_proj.weight: grad norm = 0.458191\n",
      "text_decoder_layers.3.MHA_1.out_proj.bias: grad norm = 0.132818\n",
      "text_decoder_layers.3.layer_norm1.weight: grad norm = 0.096441\n",
      "text_decoder_layers.3.layer_norm1.bias: grad norm = 0.133342\n",
      "text_decoder_layers.3.fc1.weight: grad norm = 0.611193\n",
      "text_decoder_layers.3.fc1.bias: grad norm = 0.047376\n",
      "text_decoder_layers.3.fc2.weight: grad norm = 2.001888\n",
      "text_decoder_layers.3.fc2.bias: grad norm = 0.132400\n",
      "text_decoder_layers.3.layer_norm_ff.weight: grad norm = 0.099969\n",
      "text_decoder_layers.3.layer_norm_ff.bias: grad norm = 0.136694\n",
      "decoder_output_features_to_text_tokens_layer.weight: grad norm = 0.679449\n",
      "decoder_output_features_to_text_tokens_layer.bias: grad norm = 0.051311\n",
      "generative_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embedding: grad norm = 0.022564\n",
      "text_cls_token: grad norm = 0.081758\n",
      "vit.class_token: grad norm = 0.025342\n",
      "vit.conv_proj.weight: grad norm = 0.210282\n",
      "vit.conv_proj.bias: grad norm = 0.027699\n",
      "vit.encoder.pos_embedding: grad norm = 0.025466\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.011705\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.015874\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.419620\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.018709\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.796260\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias: grad norm = 0.026827\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.weight: grad norm = 0.015409\n",
      "vit.encoder.layers.encoder_layer_0.ln_2.bias: grad norm = 0.013698\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.weight: grad norm = 0.555653\n",
      "vit.encoder.layers.encoder_layer_0.mlp.0.bias: grad norm = 0.018320\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.weight: grad norm = 0.622091\n",
      "vit.encoder.layers.encoder_layer_0.mlp.3.bias: grad norm = 0.021267\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.weight: grad norm = 0.010133\n",
      "vit.encoder.layers.encoder_layer_1.ln_1.bias: grad norm = 0.009255\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight: grad norm = 0.356814\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias: grad norm = 0.012091\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight: grad norm = 0.584573\n",
      "vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias: grad norm = 0.019915\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.weight: grad norm = 0.014540\n",
      "vit.encoder.layers.encoder_layer_1.ln_2.bias: grad norm = 0.012741\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.weight: grad norm = 0.501542\n",
      "vit.encoder.layers.encoder_layer_1.mlp.0.bias: grad norm = 0.016486\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.weight: grad norm = 0.552483\n",
      "vit.encoder.layers.encoder_layer_1.mlp.3.bias: grad norm = 0.018667\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.weight: grad norm = 0.009320\n",
      "vit.encoder.layers.encoder_layer_2.ln_1.bias: grad norm = 0.008431\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight: grad norm = 0.329175\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias: grad norm = 0.011269\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight: grad norm = 0.513173\n",
      "vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias: grad norm = 0.018646\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.weight: grad norm = 0.010803\n",
      "vit.encoder.layers.encoder_layer_2.ln_2.bias: grad norm = 0.010473\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.weight: grad norm = 0.432232\n",
      "vit.encoder.layers.encoder_layer_2.mlp.0.bias: grad norm = 0.014774\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.weight: grad norm = 0.519202\n",
      "vit.encoder.layers.encoder_layer_2.mlp.3.bias: grad norm = 0.018592\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.weight: grad norm = 0.008304\n",
      "vit.encoder.layers.encoder_layer_3.ln_1.bias: grad norm = 0.007804\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight: grad norm = 0.305369\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias: grad norm = 0.010678\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight: grad norm = 0.475108\n",
      "vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias: grad norm = 0.018522\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.weight: grad norm = 0.012000\n",
      "vit.encoder.layers.encoder_layer_3.ln_2.bias: grad norm = 0.011853\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.weight: grad norm = 0.427682\n",
      "vit.encoder.layers.encoder_layer_3.mlp.0.bias: grad norm = 0.014943\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.weight: grad norm = 0.476117\n",
      "vit.encoder.layers.encoder_layer_3.mlp.3.bias: grad norm = 0.018412\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.weight: grad norm = 0.007465\n",
      "vit.encoder.layers.encoder_layer_4.ln_1.bias: grad norm = 0.007476\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight: grad norm = 0.301459\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias: grad norm = 0.010651\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight: grad norm = 0.470682\n",
      "vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias: grad norm = 0.018346\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.weight: grad norm = 0.011814\n",
      "vit.encoder.layers.encoder_layer_4.ln_2.bias: grad norm = 0.011206\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.weight: grad norm = 0.418859\n",
      "vit.encoder.layers.encoder_layer_4.mlp.0.bias: grad norm = 0.014797\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.weight: grad norm = 0.466732\n",
      "vit.encoder.layers.encoder_layer_4.mlp.3.bias: grad norm = 0.018253\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.weight: grad norm = 0.007807\n",
      "vit.encoder.layers.encoder_layer_5.ln_1.bias: grad norm = 0.008238\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight: grad norm = 0.304882\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias: grad norm = 0.010788\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight: grad norm = 0.438389\n",
      "vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias: grad norm = 0.018198\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.weight: grad norm = 0.010909\n",
      "vit.encoder.layers.encoder_layer_5.ln_2.bias: grad norm = 0.010930\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.weight: grad norm = 0.411304\n",
      "vit.encoder.layers.encoder_layer_5.mlp.0.bias: grad norm = 0.014559\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.weight: grad norm = 0.458707\n",
      "vit.encoder.layers.encoder_layer_5.mlp.3.bias: grad norm = 0.018165\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.weight: grad norm = 0.007813\n",
      "vit.encoder.layers.encoder_layer_6.ln_1.bias: grad norm = 0.007945\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight: grad norm = 0.296257\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias: grad norm = 0.010514\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight: grad norm = 0.435144\n",
      "vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias: grad norm = 0.018202\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.weight: grad norm = 0.010913\n",
      "vit.encoder.layers.encoder_layer_6.ln_2.bias: grad norm = 0.010444\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.weight: grad norm = 0.396872\n",
      "vit.encoder.layers.encoder_layer_6.mlp.0.bias: grad norm = 0.014056\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.weight: grad norm = 0.471019\n",
      "vit.encoder.layers.encoder_layer_6.mlp.3.bias: grad norm = 0.018212\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.weight: grad norm = 0.007906\n",
      "vit.encoder.layers.encoder_layer_7.ln_1.bias: grad norm = 0.007861\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight: grad norm = 0.299369\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias: grad norm = 0.010641\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight: grad norm = 0.424935\n",
      "vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias: grad norm = 0.018190\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.weight: grad norm = 0.011238\n",
      "vit.encoder.layers.encoder_layer_7.ln_2.bias: grad norm = 0.011257\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.weight: grad norm = 0.411376\n",
      "vit.encoder.layers.encoder_layer_7.mlp.0.bias: grad norm = 0.014651\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.weight: grad norm = 0.448544\n",
      "vit.encoder.layers.encoder_layer_7.mlp.3.bias: grad norm = 0.018262\n",
      "vit.encoder.ln.weight: grad norm = 0.203636\n",
      "vit.encoder.ln.bias: grad norm = 0.212206\n",
      "ifs2tfs.weight: grad norm = 3.509693\n",
      "ifs2tfs.bias: grad norm = 0.101764\n",
      "final_layernorm.weight: grad norm = 0.182885\n",
      "final_layernorm.bias: grad norm = 0.153224\n",
      "latent_text_features.weight: grad norm = 2.214723\n",
      "latent_text_features.bias: grad norm = 0.123787\n",
      "text_embeddings.weight: grad norm = 0.130443\n",
      "contrastive_layernorm.weight: grad norm = 0.085619\n",
      "contrastive_layernorm.bias: grad norm = 0.079870\n",
      "text_decoder_layers.0.k.weight: grad norm = 0.055030\n",
      "text_decoder_layers.0.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q.weight: grad norm = 0.052644\n",
      "text_decoder_layers.0.q.bias: grad norm = 0.006102\n",
      "text_decoder_layers.0.v.weight: grad norm = 0.695174\n",
      "text_decoder_layers.0.v.bias: grad norm = 0.258076\n",
      "text_decoder_layers.0.MHA_1.in_proj_weight: grad norm = 0.679029\n",
      "text_decoder_layers.0.MHA_1.in_proj_bias: grad norm = 0.376708\n",
      "text_decoder_layers.0.MHA_1.out_proj.weight: grad norm = 0.849490\n",
      "text_decoder_layers.0.MHA_1.out_proj.bias: grad norm = 0.601251\n",
      "text_decoder_layers.0.layer_norm1.weight: grad norm = 0.151616\n",
      "text_decoder_layers.0.layer_norm1.bias: grad norm = 0.598805\n",
      "text_decoder_layers.0.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.0.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.0.v_cross_attn.weight: grad norm = 6.999847\n",
      "text_decoder_layers.0.v_cross_attn.bias: grad norm = 0.251962\n",
      "text_decoder_layers.0.cross_attn.in_proj_weight: grad norm = 3.007926\n",
      "text_decoder_layers.0.cross_attn.in_proj_bias: grad norm = 0.348566\n",
      "text_decoder_layers.0.cross_attn.out_proj.weight: grad norm = 3.789563\n",
      "text_decoder_layers.0.cross_attn.out_proj.bias: grad norm = 0.593747\n",
      "text_decoder_layers.0.layer_norm_cross_attn.weight: grad norm = 0.183825\n",
      "text_decoder_layers.0.layer_norm_cross_attn.bias: grad norm = 0.608777\n",
      "text_decoder_layers.0.fc1.weight: grad norm = 1.188004\n",
      "text_decoder_layers.0.fc1.bias: grad norm = 0.184791\n",
      "text_decoder_layers.0.fc2.weight: grad norm = 7.049505\n",
      "text_decoder_layers.0.fc2.bias: grad norm = 0.610715\n",
      "text_decoder_layers.0.layer_norm_ff.weight: grad norm = 0.216336\n",
      "text_decoder_layers.0.layer_norm_ff.bias: grad norm = 0.626679\n",
      "text_decoder_layers.1.k.weight: grad norm = 0.058540\n",
      "text_decoder_layers.1.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.1.q.weight: grad norm = 0.062287\n",
      "text_decoder_layers.1.q.bias: grad norm = 0.006051\n",
      "text_decoder_layers.1.v.weight: grad norm = 1.314087\n",
      "text_decoder_layers.1.v.bias: grad norm = 0.267341\n",
      "text_decoder_layers.1.MHA_1.in_proj_weight: grad norm = 1.028763\n",
      "text_decoder_layers.1.MHA_1.in_proj_bias: grad norm = 0.364637\n",
      "text_decoder_layers.1.MHA_1.out_proj.weight: grad norm = 1.140306\n",
      "text_decoder_layers.1.MHA_1.out_proj.bias: grad norm = 0.605024\n",
      "text_decoder_layers.1.layer_norm1.weight: grad norm = 0.212370\n",
      "text_decoder_layers.1.layer_norm1.bias: grad norm = 0.607193\n",
      "text_decoder_layers.1.fc1.weight: grad norm = 1.231400\n",
      "text_decoder_layers.1.fc1.bias: grad norm = 0.181144\n",
      "text_decoder_layers.1.fc2.weight: grad norm = 6.853773\n",
      "text_decoder_layers.1.fc2.bias: grad norm = 0.595969\n",
      "text_decoder_layers.1.layer_norm_ff.weight: grad norm = 0.217542\n",
      "text_decoder_layers.1.layer_norm_ff.bias: grad norm = 0.610599\n",
      "text_decoder_layers.2.k.weight: grad norm = 0.051611\n",
      "text_decoder_layers.2.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q.weight: grad norm = 0.062126\n",
      "text_decoder_layers.2.q.bias: grad norm = 0.006880\n",
      "text_decoder_layers.2.v.weight: grad norm = 1.236855\n",
      "text_decoder_layers.2.v.bias: grad norm = 0.228969\n",
      "text_decoder_layers.2.MHA_1.in_proj_weight: grad norm = 1.072159\n",
      "text_decoder_layers.2.MHA_1.in_proj_bias: grad norm = 0.328080\n",
      "text_decoder_layers.2.MHA_1.out_proj.weight: grad norm = 1.442609\n",
      "text_decoder_layers.2.MHA_1.out_proj.bias: grad norm = 0.596737\n",
      "text_decoder_layers.2.layer_norm1.weight: grad norm = 0.226877\n",
      "text_decoder_layers.2.layer_norm1.bias: grad norm = 0.601165\n",
      "text_decoder_layers.2.k_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.k_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.weight: grad norm = 0.000000\n",
      "text_decoder_layers.2.q_cross_attn.bias: grad norm = 0.000000\n",
      "text_decoder_layers.2.v_cross_attn.weight: grad norm = 7.215776\n",
      "text_decoder_layers.2.v_cross_attn.bias: grad norm = 0.259736\n",
      "text_decoder_layers.2.cross_attn.in_proj_weight: grad norm = 3.485076\n",
      "text_decoder_layers.2.cross_attn.in_proj_bias: grad norm = 0.358380\n",
      "text_decoder_layers.2.cross_attn.out_proj.weight: grad norm = 4.336848\n",
      "text_decoder_layers.2.cross_attn.out_proj.bias: grad norm = 0.602327\n",
      "text_decoder_layers.2.layer_norm_cross_attn.weight: grad norm = 0.274603\n",
      "text_decoder_layers.2.layer_norm_cross_attn.bias: grad norm = 0.629894\n",
      "text_decoder_layers.2.fc1.weight: grad norm = 1.599001\n",
      "text_decoder_layers.2.fc1.bias: grad norm = 0.194218\n",
      "text_decoder_layers.2.fc2.weight: grad norm = 7.577659\n",
      "text_decoder_layers.2.fc2.bias: grad norm = 0.618254\n",
      "text_decoder_layers.2.layer_norm_ff.weight: grad norm = 0.296743\n",
      "text_decoder_layers.2.layer_norm_ff.bias: grad norm = 0.636415\n",
      "text_decoder_layers.3.k.weight: grad norm = 0.059502\n",
      "text_decoder_layers.3.k.bias: grad norm = 0.000000\n",
      "text_decoder_layers.3.q.weight: grad norm = 0.058606\n",
      "text_decoder_layers.3.q.bias: grad norm = 0.006309\n",
      "text_decoder_layers.3.v.weight: grad norm = 2.141871\n",
      "text_decoder_layers.3.v.bias: grad norm = 0.291208\n",
      "text_decoder_layers.3.MHA_1.in_proj_weight: grad norm = 1.560309\n",
      "text_decoder_layers.3.MHA_1.in_proj_bias: grad norm = 0.384649\n",
      "text_decoder_layers.3.MHA_1.out_proj.weight: grad norm = 1.824291\n",
      "text_decoder_layers.3.MHA_1.out_proj.bias: grad norm = 0.622789\n",
      "text_decoder_layers.3.layer_norm1.weight: grad norm = 0.293941\n",
      "text_decoder_layers.3.layer_norm1.bias: grad norm = 0.625337\n",
      "text_decoder_layers.3.fc1.weight: grad norm = 1.644622\n",
      "text_decoder_layers.3.fc1.bias: grad norm = 0.195604\n",
      "text_decoder_layers.3.fc2.weight: grad norm = 7.647062\n",
      "text_decoder_layers.3.fc2.bias: grad norm = 0.613819\n",
      "text_decoder_layers.3.layer_norm_ff.weight: grad norm = 0.307492\n",
      "text_decoder_layers.3.layer_norm_ff.bias: grad norm = 0.633762\n",
      "decoder_output_features_to_text_tokens_layer.weight: grad norm = 3.329459\n",
      "decoder_output_features_to_text_tokens_layer.bias: grad norm = 0.279309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, val_data, opt, lr, weight_decay, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m         t_contrastive_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m         t_generative_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m     40\u001b[0m             \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#             print(batch[0], len(batch[0]))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# input images, and texts\u001b[39;00m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m, in \u001b[0;36mPreTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         text = self.encode_text(text)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m             response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m             image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model=model, opt=None, data=train_loader, val_data=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448aaf16",
   "metadata": {},
   "source": [
    "##### def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.float32).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "        # Since task is to predict next token, the labels will start form position 1\n",
    "        text_labels = text[:, 1:] \n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss.detach()\n",
    "        val_contrastive_loss += contrastive_loss.detach()\n",
    "        val_generative_loss += generative_loss.detach()\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
