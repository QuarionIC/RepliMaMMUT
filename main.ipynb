{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb886d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b019739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a884230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c36fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dc2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41faf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01120acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6628e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = pre_train_data.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b2f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "#         text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None\n",
    "#     def encode_text(self, example):\n",
    "#         text = self.tokenizer(example, padding='max_length', max_length=max_seq_len, add_special_tokens=True) # hard-coded max_length for now\n",
    "#         bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "#          # add a bos token as well\n",
    "#         text = {\n",
    "#             \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "#             \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "#         }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00981577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812a3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)\n",
    "    return images, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b9ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=8, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4461936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0ca6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.001, weight_decay=0.00001, num_epochs=20, checkpoint_path='../checkpoints/'):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "\n",
    "    batch_size = 2\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "       \n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32).to(device)\n",
    "            text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "#             print(text)\n",
    "\n",
    "            if len(imgs) < batch_size:\n",
    "                # Last batch will have less images, text pairs since it will be the\n",
    "                # remainder of Total images / batch_size.\n",
    "\n",
    "                # Adjust the learning rate of the last batch by \n",
    "                # (size(last_batch) / batch_size) to account \n",
    "                # for the smaller size.\n",
    "                adj_lr = lr * (len(inp) / batch_size)\n",
    "                optimizer = optim.AdamW(model.parameters(),\n",
    "                    lr=adj_lr,\n",
    "                    weight_decay=weight_decay)\n",
    "            # Since task is to predict next token, the labels will start form position 1\n",
    "            text_labels = text[:, 1:] \n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            print(f\"Total Loss: {total_loss.item()}   Gen Loss: {generative_loss.item()}   Contr Loss: {contrastive_loss.item()}\")\n",
    "            total_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss\n",
    "            t_contrastive_loss += contrastive_loss\n",
    "            t_generative_loss += generative_loss\n",
    "            del imgs\n",
    "            del text\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss / len(loader))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(loader))\n",
    "        train_generative_losses.append(t_generative_loss / len(loader))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "        if epoch % 5 == 0: # save model every 5th epoch\n",
    "            torch.save(model.state_dict(), checkpoint_path.format(epoch))\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(loader), t_contrastive_loss / len(loader), t_generative_loss / len(loader)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(loader), val_contrastive_loss / len(loader), val_generative_loss / len(loader)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9b3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15c208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaMMUT\n",
    "model = MaMMUT(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Total Loss: 13.47065544128418   Gen Loss: 10.764650344848633   Contr Loss: 2.706005573272705\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.484277725219727   Gen Loss: 10.535784721374512   Contr Loss: 5.948493003845215\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.831682205200195   Gen Loss: 10.509672164916992   Contr Loss: 4.322009563446045\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.234180450439453   Gen Loss: 10.284846305847168   Contr Loss: 5.949333667755127\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 17.68054962158203   Gen Loss: 10.13766860961914   Contr Loss: 7.542880535125732\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.98520278930664   Gen Loss: 9.884836196899414   Contr Loss: 6.100366592407227\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.952230453491211   Gen Loss: 9.288335800170898   Contr Loss: 5.6638946533203125\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 20.545177459716797   Gen Loss: 11.501850128173828   Contr Loss: 9.043327331542969\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 18.771324157714844   Gen Loss: 11.740560531616211   Contr Loss: 7.030762672424316\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.128864288330078   Gen Loss: 11.023348808288574   Contr Loss: 5.1055145263671875\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 17.173141479492188   Gen Loss: 10.92947769165039   Contr Loss: 6.243663787841797\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.68170166015625   Gen Loss: 10.930546760559082   Contr Loss: 4.751154899597168\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.478654861450195   Gen Loss: 10.463787078857422   Contr Loss: 6.014868259429932\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.984275817871094   Gen Loss: 9.935160636901855   Contr Loss: 4.04911470413208\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.094024658203125   Gen Loss: 10.140713691711426   Contr Loss: 5.953310966491699\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 16.064910888671875   Gen Loss: 10.07097339630127   Contr Loss: 5.993936538696289\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.127470016479492   Gen Loss: 9.768675804138184   Contr Loss: 5.358794212341309\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.474077224731445   Gen Loss: 9.527504920959473   Contr Loss: 5.9465718269348145\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.938435554504395   Gen Loss: 9.386185646057129   Contr Loss: 5.552249908447266\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.094037055969238   Gen Loss: 9.184333801269531   Contr Loss: 2.909703254699707\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 11.859155654907227   Gen Loss: 8.918482780456543   Contr Loss: 2.940673351287842\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.693742752075195   Gen Loss: 9.222179412841797   Contr Loss: 3.4715628623962402\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.519041061401367   Gen Loss: 7.5716094970703125   Contr Loss: 5.947431564331055\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.075912475585938   Gen Loss: 9.12783432006836   Contr Loss: 5.94807767868042\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.047357559204102   Gen Loss: 9.092342376708984   Contr Loss: 5.955015182495117\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.61618709564209   Gen Loss: 9.205595016479492   Contr Loss: 5.410592079162598\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.17485237121582   Gen Loss: 7.999034404754639   Contr Loss: 6.175817489624023\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.223978042602539   Gen Loss: 8.276226997375488   Contr Loss: 5.947750568389893\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.597753524780273   Gen Loss: 8.238486289978027   Contr Loss: 2.3592677116394043\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.528290748596191   Gen Loss: 7.616913795471191   Contr Loss: 2.911376953125\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.20858097076416   Gen Loss: 8.261322021484375   Contr Loss: 5.947258949279785\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.765945434570312   Gen Loss: 8.735925674438477   Contr Loss: 6.030019760131836\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.956416130065918   Gen Loss: 8.007814407348633   Contr Loss: 5.948601722717285\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.143372535705566   Gen Loss: 8.193618774414062   Contr Loss: 5.949753761291504\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.733753204345703   Gen Loss: 8.692634582519531   Contr Loss: 6.041118144989014\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.660165786743164   Gen Loss: 7.716954231262207   Contr Loss: 5.943211078643799\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.903185844421387   Gen Loss: 8.096184730529785   Contr Loss: 4.807001113891602\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.948995590209961   Gen Loss: 7.995079517364502   Contr Loss: 5.953916072845459\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 15.029760360717773   Gen Loss: 9.083922386169434   Contr Loss: 5.945838451385498\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.872507095336914   Gen Loss: 8.925244331359863   Contr Loss: 5.947262287139893\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.805877685546875   Gen Loss: 8.17359733581543   Contr Loss: 5.632279872894287\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.556560516357422   Gen Loss: 7.077216625213623   Contr Loss: 3.4793434143066406\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.46461296081543   Gen Loss: 8.517900466918945   Contr Loss: 5.946712017059326\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.942442893981934   Gen Loss: 8.034537315368652   Contr Loss: 2.9079058170318604\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.921566009521484   Gen Loss: 8.84183406829834   Contr Loss: 6.0797319412231445\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.595510482788086   Gen Loss: 8.649198532104492   Contr Loss: 5.946311950683594\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.074787139892578   Gen Loss: 8.028977394104004   Contr Loss: 4.045810222625732\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.214628219604492   Gen Loss: 8.260354995727539   Contr Loss: 5.954272747039795\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.036337852478027   Gen Loss: 8.088655471801758   Contr Loss: 5.9476823806762695\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.821853637695312   Gen Loss: 7.877030372619629   Contr Loss: 5.944823741912842\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.002739906311035   Gen Loss: 8.053287506103516   Contr Loss: 5.9494524002075195\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.098441123962402   Gen Loss: 8.145903587341309   Contr Loss: 5.952537536621094\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.091424942016602   Gen Loss: 8.144693374633789   Contr Loss: 5.946731090545654\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.898575782775879   Gen Loss: 9.15453052520752   Contr Loss: 5.744045257568359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Total Loss: 14.017463684082031   Gen Loss: 8.070934295654297   Contr Loss: 5.946529865264893\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.380477905273438   Gen Loss: 8.43381118774414   Contr Loss: 5.946666240692139\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.537328720092773   Gen Loss: 7.8917365074157715   Contr Loss: 4.64559268951416\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.961116790771484   Gen Loss: 8.008559226989746   Contr Loss: 5.95255708694458\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.284788131713867   Gen Loss: 8.343875885009766   Contr Loss: 5.94091272354126\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.834314346313477   Gen Loss: 7.8820576667785645   Contr Loss: 5.952256679534912\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.393145561218262   Gen Loss: 8.446158409118652   Contr Loss: 5.946987152099609\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.496585845947266   Gen Loss: 8.549375534057617   Contr Loss: 5.947210788726807\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.809660911560059   Gen Loss: 8.159523010253906   Contr Loss: 4.650137901306152\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.748176574707031   Gen Loss: 6.801698207855225   Contr Loss: 5.946478366851807\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.521100997924805   Gen Loss: 7.573853492736816   Contr Loss: 5.947247505187988\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 12.098557472229004   Gen Loss: 8.04406452178955   Contr Loss: 4.054492950439453\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.04005241394043   Gen Loss: 8.09140682220459   Contr Loss: 5.948646068572998\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.346800804138184   Gen Loss: 8.394308090209961   Contr Loss: 5.952492713928223\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.544928550720215   Gen Loss: 8.597034454345703   Contr Loss: 5.947894096374512\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.510254859924316   Gen Loss: 7.560152530670166   Contr Loss: 5.95010232925415\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.797063827514648   Gen Loss: 7.848604202270508   Contr Loss: 5.948459148406982\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.726505279541016   Gen Loss: 7.780482292175293   Contr Loss: 5.946023464202881\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.586511611938477   Gen Loss: 7.639031410217285   Contr Loss: 5.947480201721191\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.915727615356445   Gen Loss: 7.444993495941162   Contr Loss: 3.470733642578125\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 14.133404731750488   Gen Loss: 8.178400039672852   Contr Loss: 5.955004692077637\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.400510787963867   Gen Loss: 7.443272113800049   Contr Loss: 5.957239151000977\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.69041633605957   Gen Loss: 7.784356117248535   Contr Loss: 2.906060218811035\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.819191932678223   Gen Loss: 7.873924732208252   Contr Loss: 5.945267200469971\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 13.925302505493164   Gen Loss: 7.978176593780518   Contr Loss: 5.9471259117126465\n",
      "-----------------------------------------------------------\n",
      "Total Loss: 10.93165397644043   Gen Loss: 8.026571273803711   Contr Loss: 2.9050822257995605\n"
     ]
    }
   ],
   "source": [
    "train(model=model, data=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.long).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "\n",
    "        text_labels = text[:, 1:] # labels are the same text just with the <s> token removed\n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss\n",
    "        val_contrastive_loss += contrastive_loss\n",
    "        val_generative_loss += generative_loss\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26c56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextDecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                num_heads_mha, \n",
    "                num_heads_cross_attn, \n",
    "                d_feedforward, \n",
    "                d_k,\n",
    "                d_v, \n",
    "                vit_dim):\n",
    "        super(TextDecoderLayer, self).__init__()\n",
    "\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.MHA_1 = nn.MultiheadAttention(d_model, num_heads_mha, batch_first =True)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.k_cross_attn = nn.Linear(vit_dim, d_model)\n",
    "        self.q_cross_attn = nn.Linear(d_model, d_model)\n",
    "        self.v_cross_attn = nn.Linear(vit_dim, d_model)\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, num_heads_cross_attn, batch_first =True)\n",
    "        self.layer_norm_cross_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_feedforward)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_feedforward, d_model)\n",
    "        self.layer_norm_ff = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, vision_features=None, enable_cross_attn=True, causal_mask=False, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"Forward method for decoder layer with added option to disable cross attention and causal masking\"\"\"\n",
    "        k1 = self.k(x)\n",
    "        q1 = self.q(x)\n",
    "        v1 = self.v(x)\n",
    "\n",
    "        out = self.MHA_1(q1, k1, v1, is_causal=causal_mask, key_padding_mask=padding_mask, attn_mask=attn_mask)\n",
    "#         print(out.shape)\n",
    "        out = self.layer_norm1(out[0] + x)\n",
    "        out_layer_norm1 = torch.clone(out)\n",
    "\n",
    "        if enable_cross_attn:\n",
    "            k2 = self.k_cross_attn(vision_features)\n",
    "            q2 = self.q_cross_attn(x)\n",
    "            v2 = self.v_cross_attn(vision_features)\n",
    "            out = self.cross_attn(q2, k2, v2)\n",
    "            out = self.layer_norm_cross_attn(out[0] + out_layer_norm1)\n",
    "            out_layer_norm_cross_attn = torch.clone(out)\n",
    "        \n",
    "        out = self.fc2(self.relu(self.fc1(out)))\n",
    "        if enable_cross_attn:\n",
    "            out = self.layer_norm_ff(out + out_layer_norm_cross_attn)\n",
    "        else:\n",
    "            out = self.layer_norm_ff(out + out_layer_norm1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa5b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as PIL_Image\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "from torchvision.transforms import v2\n",
    "# from text_decoder import TextDecoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaMMUT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 14,\n",
    "                 vit_num_layers: int = 32,\n",
    "                 vit_num_heads: int = 16,\n",
    "                 vit_hidden_dim: int = 1280,\n",
    "                 vit_mlp_dim: int = 5120,\n",
    "                 vit_dropout: float = 0.0, # Potential ablation / extension to add to the replication\n",
    "                 vit_attention_dropout: float = 0.0, # Potential ablation / extension to add to the replication\n",
    "                 contrastive_loss_weight: float = 1.0,\n",
    "                 generative_loss_weight: float = 1.0,\n",
    "                 text_decoder_depth: int = 6,\n",
    "                 text_decoder_embed_dim: int = 512,\n",
    "                 text_decoder_sub_layer_heads: int = 8,\n",
    "                 text_decoder_feedforward_dim: int = 2048,\n",
    "                 text_decoder_dk: int = 128,\n",
    "                 vocab_size: int = 1000,\n",
    "                 latent_dim: int = 512,\n",
    "                 contrastive_loss_temp: float = 0.5,\n",
    "                 contrastive_loss_gamma: float = 1.0):\n",
    "                 \n",
    "        super(MaMMUT, self).__init__()   \n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.vit = VisionTransformer(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_layers=vit_num_layers,\n",
    "            num_heads=vit_num_heads,\n",
    "            hidden_dim=vit_hidden_dim,\n",
    "            mlp_dim=vit_mlp_dim,\n",
    "            dropout=vit_dropout,\n",
    "            attention_dropout=vit_attention_dropout,\n",
    "            num_classes=1000\n",
    "        )\n",
    "\n",
    "        self.token_size = vocab_size\n",
    "        self.text_decoder_embed_dim = text_decoder_embed_dim\n",
    "        self.text_decoder_sub_layer_heads = text_decoder_sub_layer_heads\n",
    "        \n",
    "        self.contrastive_loss_weight = contrastive_loss_weight\n",
    "        self.generative_loss_weight = generative_loss_weight\n",
    "        \n",
    "        self.ifs2tfs = nn.Linear(vit_hidden_dim, text_decoder_embed_dim)\n",
    "        \n",
    "        self.text_decoder_depth = text_decoder_depth\n",
    "        self.text_decoder_layers = []\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (image_size // patch_size)**2 + 1, vit_hidden_dim))\n",
    "        \n",
    "        self.final_layernorm = nn.LayerNorm(self.token_size)\n",
    "\n",
    "        self.latent_text_features = nn.Linear(text_decoder_embed_dim, text_decoder_embed_dim) # for contrastive loss\n",
    "        self.pad_token_id = 0 # we can set this in the SentencePiece tokenizer\n",
    "\n",
    "        self.text_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=text_decoder_embed_dim, padding_idx=self.pad_token_id)\n",
    "\n",
    "        self.text_cls_token = nn.Parameter(torch.randn(text_decoder_embed_dim))\n",
    "        self.contrastive_layernorm = nn.LayerNorm(text_decoder_embed_dim)\n",
    "\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id).to(self.device)\n",
    "        self.contrastive_loss_temp = contrastive_loss_temp\n",
    "        self.contrastive_loss_gamma = contrastive_loss_gamma\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        \n",
    "        # Changing logic for the decoder layer. This way we can disable cross-attention during the forward pass and keep everything else the same\n",
    "        for i in range(text_decoder_depth):\n",
    "            self.text_decoder_layers.append(TextDecoderLayer(d_model=text_decoder_embed_dim, num_heads_mha=text_decoder_sub_layer_heads, \\\n",
    "                                                            num_heads_cross_attn=text_decoder_sub_layer_heads, d_feedforward=text_decoder_feedforward_dim, \\\n",
    "                                                             d_k=text_decoder_dk, d_v=(text_decoder_embed_dim // text_decoder_sub_layer_heads), vit_dim=vit_hidden_dim).to(self.device)\n",
    "                                                             )\n",
    "        self.decoder_output_features_to_text_tokens_layer = nn.Linear(self.text_decoder_embed_dim, self.token_size) # for captioning loss\n",
    "            \n",
    "        \n",
    "    def cropped_positional_encoding(self, feats):\n",
    "        # feats shape: N x (H_p x W_p) x Hidden\n",
    "        n, h_w, hidden = feats.shape\n",
    "\n",
    "\n",
    "        # take out cls token before upsampling\n",
    "        cls_pos_embed = self.pos_embedding[:, 0, :]\n",
    "        pos_embeddings = self.pos_embedding[:, 1: :]\n",
    "\n",
    "        pos_embeddings = pos_embeddings.reshape(1, self.image_size // self.patch_size, self.image_size // self.patch_size, hidden).permute(0, -1, 1, 2)\n",
    "\n",
    "        # Upsample using bilinear interpolation\n",
    "        upsample_layer = nn.Upsample(mode='bilinear', scale_factor=4)\n",
    "        upsampled_pos_embeddings = upsample_layer(pos_embeddings)\n",
    "        random_crop = v2.RandomCrop(pos_embeddings.shape[2])\n",
    "        cropped_pos_encoding = random_crop(upsampled_pos_embeddings)\n",
    "\n",
    "        # cropped_pos_encoding shape: N x (H_p x W_p) x Hidden. Reshape to align with feats\n",
    "        cropped_pos_encoding = cropped_pos_encoding.reshape(1, h_w-1, hidden)\n",
    "        \n",
    "        cropped_pos_encoding = torch.cat([cropped_pos_encoding, cls_pos_embed.reshape(1, 1, hidden)], dim=1)\n",
    "\n",
    "\n",
    "        return feats + cropped_pos_encoding\n",
    "\n",
    "        \n",
    "    def get_vision_features(self, img: torch.tensor):\n",
    "        # image has shape N x C x H x W where\n",
    "        # N is the batch size\n",
    "        # C is the channel size\n",
    "        # H is the image height\n",
    "        # W is the image width\n",
    "#         preprocessing = v2.Compose([\n",
    "#             v2.ToImage(),\n",
    "#             v2.Resize((272,272)),\n",
    "#             v2.RandomCrop(224)\n",
    "#         ])\n",
    "\n",
    "#         img = PIL_Image.open(\"example_2353642598754.jpeg\")\n",
    "#         img = preprocessing(img)\n",
    "\n",
    "        # Add batch dimension - for testing on one image, remove for training\n",
    "#         img = img.unsqueeze(0)\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w), converts into patches\n",
    "        feats = self.vit._process_input(img)\n",
    "        # Expand the CLS token to the full batch\n",
    "        batch_class_token = self.vit.class_token.expand(img.shape[0], -1, -1)\n",
    "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
    "        \n",
    "        feats = self.cropped_positional_encoding(feats)\n",
    "\n",
    "        feats = self.vit.encoder(feats)\n",
    "\n",
    "        # Fetch pre-prended CLS token at position 0 in dimension 1\n",
    "        feats = feats[:, 0].reshape(feats.shape[0], 1, feats.shape[-1])\n",
    "                \n",
    "        return feats\n",
    "    \n",
    "    def img_feat_size_to_txt_feat_size(self, vision_features: torch.tensor):\n",
    "        return self.ifs2tfs(vision_features)\n",
    "    \n",
    "    def contrastive_text_features(self, text_embeds: torch.Tensor):\n",
    "        # text has shape N x S\n",
    "        # Remember to pass bidirectional mask (as far as I understand, a mask that allows attention to all non-padded areas or maybe just all non-CLS areas and maybe stops cls from attending to padding TODO: Clarify)\n",
    "        # Remember to perform residual additions         \n",
    "        # expand to match dimensions\n",
    "        cls_tokens = self.text_cls_token.expand(text_embeds.shape[0], 1, self.text_decoder_embed_dim).to(self.device)\n",
    "        # Add cls tokens to start of the sequences\n",
    "        text_embeds = torch.cat([cls_tokens, text_embeds], dim=1)\n",
    "        cls_padding_mask = (text_embeds == 0).all(dim=-1) # From nn.Embedding, padding tokens are embedded as vector of 0s. Result should be shape N x S.\n",
    "\n",
    "        output = text_embeds.clone()\n",
    "        for i, layer in enumerate(self.text_decoder_layers):\n",
    "            # Disable cross-attention for contrastive features\n",
    "            output = layer(output, enable_cross_attn=False, padding_mask=cls_padding_mask)\n",
    "\n",
    "        output = output[:, 0]\n",
    "        output = self.contrastive_layernorm(output)\n",
    "        return output\n",
    "    \n",
    "    def generative_text_features(self, text_embeds: torch.tensor, vision_features: torch.tensor):\n",
    "        # Remember to toggle causal in forward pass\n",
    "        # Remember to perform residual additions\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones((text_embeds.shape[1], text_embeds.shape[1])), diagonal=1).bool().to(self.device) # Assuming shape[1] is the sequence dim\n",
    "        output = text_embeds.clone()\n",
    "        padding_mask = (text_embeds == 0).all(dim=-1)\n",
    "        for i, layer in enumerate(self.text_decoder_layers):\n",
    "            # Disable cross-attention for odd numbered layers\n",
    "            if i % 2 != 0:\n",
    "                output = layer(output, vision_features=None, enable_cross_attn=False, causal_mask=True, attn_mask=attn_mask, padding_mask=padding_mask)\n",
    "            else:\n",
    "                # enable cross-attention for even numbered layers\n",
    "                output = layer(output, vision_features=vision_features, enable_cross_attn=True, causal_mask=True, attn_mask=attn_mask, padding_mask=padding_mask)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def contrastive_loss(self, vision_features: torch.tensor, constrastive_text_features: torch.tensor):\n",
    "        \"\"\"Implement Focal-contrastive loss as in the paper\"\"\"\n",
    "        similarity = (vision_features @ constrastive_text_features.T) / self.contrastive_loss_temp\n",
    "        similarity = similarity.squeeze(1)\n",
    "        # In contrastive learning we aim to minimize loss for between the matching image and text pairs, and maximize loss \n",
    "        # for mismatching image text pairs.\n",
    "        # after the matrix multipication, shape will be N x N\n",
    "        # each row represents image i, and each column would represent each caption\n",
    "        # Therefore, the matching pairs will be across the diagonal (0,0), (1, 1) ... and we can treat this as a classification task\n",
    "        # where we compute the loss between the text_logits and its matching image and vice-versa for the image loss\n",
    "    \n",
    "        # We can construct the labels by just creating a diagonal matrix\n",
    "        labels = torch.arange(similarity.shape[0]).to(self.device)\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=similarity.shape[0]).to(self.device)\n",
    "        probs_imgs = F.softmax(similarity, dim=1) # using softmax instead of sigmoid\n",
    "        p_t_imgs = torch.sum(labels_one_hot * probs_imgs, dim=1)\n",
    "        p_t_imgs = torch.clamp(p_t_imgs, min=1e-5)\n",
    "        loss_i2t = -(((1 - p_t_imgs) ** self.contrastive_loss_gamma) * (torch.log(p_t_imgs))).mean()\n",
    "        probs_texts = F.softmax(similarity, dim=0)\n",
    "        p_t_texts = torch.sum(labels_one_hot * probs_texts, dim=1)\n",
    "        p_t_texts = torch.clamp(p_t_texts, min=1e-5)\n",
    "\n",
    "        loss_t2i = -(((1 - p_t_texts) ** self.contrastive_loss_gamma) * (torch.log(p_t_texts))).mean()\n",
    "        total_contrastive_loss = (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "        return total_contrastive_loss.to(self.device)\n",
    "\n",
    "\n",
    "    def generative_loss(self, generative_text_features: torch.tensor, text_labels: torch.tensor):\n",
    "        generative_text_features = generative_text_features.permute(0, -1, 1) # cross-entropy expects N x C as first two dims\n",
    "        loss = self.loss_criterion(generative_text_features, text_labels)\n",
    "        return loss.to(self.device)\n",
    "\n",
    "    def decoder_output_features_to_text_tokens(self, text_features: torch.tensor):\n",
    "        return self.final_layernorm(self.decoder_output_features_to_text_tokens_layer(text_features))\n",
    "        \n",
    "    \n",
    "    def forward(self, image, text, text_labels):\n",
    "        # Pseudocode for now, need to fully implement and test\n",
    "        # TODO: Implement average pooling over spatial dimension and sequence where appropriate\n",
    "        # TODO: Add tokenizer & params ------- Tokenizer would be added in training pipeline\n",
    "        text_embeds = self.text_embeddings(text)\n",
    "#         print(text_embeds)\n",
    "        vision_features = self.get_vision_features(image)\n",
    "        vision_features_contrastive = self.img_feat_size_to_txt_feat_size(vision_features) # projects image feature dim to text feature dim\n",
    "        \n",
    "        constrastive_text_features = self.contrastive_text_features(text_embeds)\n",
    "        constrastive_text_features = self.latent_text_features(constrastive_text_features)\n",
    "\n",
    "        contrastive_loss = self.contrastive_loss(vision_features_contrastive, constrastive_text_features)\n",
    "        \n",
    "        # Since task will be to generate next token, we only go up until the second last token\n",
    "        generative_text_features = self.generative_text_features(text_embeds[:, :-1], vision_features)\n",
    "\n",
    "        text_logits = self.decoder_output_features_to_text_tokens(generative_text_features)\n",
    "\n",
    "        generative_loss = self.generative_loss(text_logits, text_labels)\n",
    "        loss = self.contrastive_loss_weight * contrastive_loss + self.generative_loss_weight * generative_loss\n",
    "        print(\"generative_loss\", generative_loss.item())\n",
    "        print(\"contrastive_loss\", contrastive_loss.item())\n",
    "        return loss, contrastive_loss, generative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43828f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaMMUT(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2989d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/hfaisal8/CS7643/project/RepliMaMMUT\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
