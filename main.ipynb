{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb886d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b019739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a884230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c36fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7dc2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41faf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01120acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6628e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only subset of data for now\n",
    "pre_train_data = pre_train_data.with_format(\"torch\")\n",
    "test_data = val_data = pre_train_data.select(range(120000, 130000))\n",
    "val_data = pre_train_data.select(range(100000, 120000))\n",
    "pre_train_data = pre_train_data.select(range(100000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3acbdc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b2f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "#         text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None\n",
    "#     def encode_text(self, example):\n",
    "#         text = self.tokenizer(example, padding='max_length', max_length=max_seq_len, add_special_tokens=True) # hard-coded max_length for now\n",
    "#         bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "#          # add a bos token as well\n",
    "#         text = {\n",
    "#             \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "#             \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "#         }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00981577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812a3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)\n",
    "    return images, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b9ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "val_dataset_cleaned = PreTrainDataset(val_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=32, shuffle=True, collate_fn=remove_none_fn)\n",
    "val_loader = DataLoader(val_dataset_cleaned, batch_size=10, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4461936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ca6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, val_data, lr=0.001, weight_decay=0.000001, num_epochs=20, checkpoint_path='../checkpoints/'):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "#     n = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    batch_size = 16\n",
    "    n = 0\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "       \n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32).to(device)\n",
    "            text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "#             print(text)\n",
    "\n",
    "            if len(imgs) < batch_size:\n",
    "                # Last batch will have less images, text pairs since it will be the\n",
    "                # remainder of Total images / batch_size.\n",
    "\n",
    "                # Adjust the learning rate of the last batch by \n",
    "                # (size(last_batch) / batch_size) to account \n",
    "                # for the smaller size.\n",
    "                adj_lr = lr * (len(imgs) / batch_size)\n",
    "                optimizer = optim.AdamW(model.parameters(),\n",
    "                    lr=adj_lr,\n",
    "                    weight_decay=weight_decay)\n",
    "            # Since task is to predict next token, the labels will start form position 1\n",
    "            text_labels = text[:, 1:] \n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            \n",
    "            n += 1\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            print(f\"Iter: {n}   Total Loss: {total_loss.item()}   Gen Loss: {generative_loss.item()}   Contr Loss: {contrastive_loss.item()}\")\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            i = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"{name}: grad norm = {param.grad.norm().item():.4f}\")\n",
    "                i += 1\n",
    "                if i > 10:\n",
    "                    break\n",
    "            optimizer.step()\n",
    "            scheduler.step(total_loss)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss.detach()\n",
    "            t_contrastive_loss += contrastive_loss.detach()\n",
    "            t_generative_loss += generative_loss.detach()\n",
    "            del imgs\n",
    "            del text\n",
    "            if n % 500 == 0:\n",
    "                torch.save(model.state_dict(), f\"{checkpoint_path}_epoch_{n}\")\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss / len(data))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(data))\n",
    "        train_generative_losses.append(t_generative_loss / len(data))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "#         if epoch % 5 == 0: # save model every 5th epoch\n",
    "        torch.save(model.state_dict(), f\"{checkpoint_path}_epoch_{epoch}\")\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(data), t_contrastive_loss / len(data), t_generative_loss / len(data)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(val_data), val_contrastive_loss / len(val_data), val_generative_loss / len(val_data)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3732c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df0db509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaMMUT\n",
    "model = MaMMUT(vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 1   Total Loss: 15.05734920501709   Gen Loss: 10.872748374938965   Contr Loss: 4.184600830078125\n",
      "pos_embedding: grad norm = 0.0351\n",
      "text_cls_token: grad norm = 0.0509\n",
      "vit.class_token: grad norm = 0.0645\n",
      "vit.conv_proj.weight: grad norm = 1.1614\n",
      "vit.conv_proj.bias: grad norm = 0.1131\n",
      "vit.encoder.pos_embedding: grad norm = 0.0653\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0258\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0402\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.8845\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0568\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 1.0542\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 2   Total Loss: 21.897106170654297   Gen Loss: 10.969675064086914   Contr Loss: 10.927430152893066\n",
      "pos_embedding: grad norm = 0.1113\n",
      "text_cls_token: grad norm = 0.0498\n",
      "vit.class_token: grad norm = 0.1025\n",
      "vit.conv_proj.weight: grad norm = 0.7144\n",
      "vit.conv_proj.bias: grad norm = 0.1713\n",
      "vit.encoder.pos_embedding: grad norm = 0.1032\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0280\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0794\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.8046\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.1010\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.9507\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 3   Total Loss: 20.96530532836914   Gen Loss: 10.701608657836914   Contr Loss: 10.263696670532227\n",
      "pos_embedding: grad norm = 0.1311\n",
      "text_cls_token: grad norm = 0.0784\n",
      "vit.class_token: grad norm = 0.0846\n",
      "vit.conv_proj.weight: grad norm = 0.7747\n",
      "vit.conv_proj.bias: grad norm = 0.1904\n",
      "vit.encoder.pos_embedding: grad norm = 0.0858\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0327\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0808\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.8864\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0921\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 1.3553\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 4   Total Loss: 16.108469009399414   Gen Loss: 10.39501667022705   Contr Loss: 5.713452339172363\n",
      "pos_embedding: grad norm = 0.1127\n",
      "text_cls_token: grad norm = 0.0923\n",
      "vit.class_token: grad norm = 0.0624\n",
      "vit.conv_proj.weight: grad norm = 0.6252\n",
      "vit.conv_proj.bias: grad norm = 0.1199\n",
      "vit.encoder.pos_embedding: grad norm = 0.0634\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0375\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0539\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.9937\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0519\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 1.0203\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 5   Total Loss: 14.870594024658203   Gen Loss: 10.60901927947998   Contr Loss: 4.261574745178223\n",
      "pos_embedding: grad norm = 0.0183\n",
      "text_cls_token: grad norm = 0.1034\n",
      "vit.class_token: grad norm = 0.0115\n",
      "vit.conv_proj.weight: grad norm = 0.0499\n",
      "vit.conv_proj.bias: grad norm = 0.0085\n",
      "vit.encoder.pos_embedding: grad norm = 0.0116\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0058\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0070\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.1476\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0068\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.1820\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 6   Total Loss: 14.50855541229248   Gen Loss: 10.418899536132812   Contr Loss: 4.089655876159668\n",
      "pos_embedding: grad norm = 0.0030\n",
      "text_cls_token: grad norm = 0.1083\n",
      "vit.class_token: grad norm = 0.0044\n",
      "vit.conv_proj.weight: grad norm = 0.0204\n",
      "vit.conv_proj.bias: grad norm = 0.0017\n",
      "vit.encoder.pos_embedding: grad norm = 0.0044\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0017\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0019\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0512\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0026\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.1236\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 7   Total Loss: 14.382320404052734   Gen Loss: 10.349111557006836   Contr Loss: 4.033208847045898\n",
      "pos_embedding: grad norm = 0.0026\n",
      "text_cls_token: grad norm = 0.1152\n",
      "vit.class_token: grad norm = 0.0037\n",
      "vit.conv_proj.weight: grad norm = 0.0181\n",
      "vit.conv_proj.bias: grad norm = 0.0010\n",
      "vit.encoder.pos_embedding: grad norm = 0.0037\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0015\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0016\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0488\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0021\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.1290\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 8   Total Loss: 15.08697509765625   Gen Loss: 10.165194511413574   Contr Loss: 4.921781063079834\n",
      "pos_embedding: grad norm = 0.0018\n",
      "text_cls_token: grad norm = 0.1220\n",
      "vit.class_token: grad norm = 0.0029\n",
      "vit.conv_proj.weight: grad norm = 0.0101\n",
      "vit.conv_proj.bias: grad norm = 0.0009\n",
      "vit.encoder.pos_embedding: grad norm = 0.0029\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0010\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0011\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0326\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0016\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0956\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 9   Total Loss: 13.97785758972168   Gen Loss: 9.902299880981445   Contr Loss: 4.075557708740234\n",
      "pos_embedding: grad norm = 0.0033\n",
      "text_cls_token: grad norm = 0.1297\n",
      "vit.class_token: grad norm = 0.0031\n",
      "vit.conv_proj.weight: grad norm = 0.0129\n",
      "vit.conv_proj.bias: grad norm = 0.0012\n",
      "vit.encoder.pos_embedding: grad norm = 0.0031\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0020\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0016\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0463\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0018\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.1097\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 10   Total Loss: 16.22984504699707   Gen Loss: 9.989444732666016   Contr Loss: 6.240400791168213\n",
      "pos_embedding: grad norm = 0.0022\n",
      "text_cls_token: grad norm = 0.1276\n",
      "vit.class_token: grad norm = 0.0024\n",
      "vit.conv_proj.weight: grad norm = 0.0092\n",
      "vit.conv_proj.bias: grad norm = 0.0012\n",
      "vit.encoder.pos_embedding: grad norm = 0.0024\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0012\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0012\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0313\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0013\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0845\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 11   Total Loss: 14.255643844604492   Gen Loss: 9.974874496459961   Contr Loss: 4.2807698249816895\n",
      "pos_embedding: grad norm = 0.0136\n",
      "text_cls_token: grad norm = 0.1306\n",
      "vit.class_token: grad norm = 0.0073\n",
      "vit.conv_proj.weight: grad norm = 0.0162\n",
      "vit.conv_proj.bias: grad norm = 0.0028\n",
      "vit.encoder.pos_embedding: grad norm = 0.0073\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0050\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0045\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0946\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0035\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 12   Total Loss: 15.399240493774414   Gen Loss: 9.584150314331055   Contr Loss: 5.815089702606201\n",
      "pos_embedding: grad norm = 0.0012\n",
      "text_cls_token: grad norm = 0.1478\n",
      "vit.class_token: grad norm = 0.0019\n",
      "vit.conv_proj.weight: grad norm = 0.0058\n",
      "vit.conv_proj.bias: grad norm = 0.0006\n",
      "vit.encoder.pos_embedding: grad norm = 0.0019\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0008\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0008\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0229\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0011\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 13   Total Loss: 16.35308074951172   Gen Loss: 9.46204662322998   Contr Loss: 6.891035079956055\n",
      "pos_embedding: grad norm = 0.0015\n",
      "text_cls_token: grad norm = 0.1534\n",
      "vit.class_token: grad norm = 0.0021\n",
      "vit.conv_proj.weight: grad norm = 0.0043\n",
      "vit.conv_proj.bias: grad norm = 0.0005\n",
      "vit.encoder.pos_embedding: grad norm = 0.0021\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0007\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0009\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0215\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0012\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0501\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 14   Total Loss: 14.117812156677246   Gen Loss: 9.32785415649414   Contr Loss: 4.7899580001831055\n",
      "pos_embedding: grad norm = 0.0010\n",
      "text_cls_token: grad norm = 0.1466\n",
      "vit.class_token: grad norm = 0.0016\n",
      "vit.conv_proj.weight: grad norm = 0.0050\n",
      "vit.conv_proj.bias: grad norm = 0.0004\n",
      "vit.encoder.pos_embedding: grad norm = 0.0016\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0007\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0007\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0211\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0010\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0669\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 15   Total Loss: 14.025243759155273   Gen Loss: 9.43490982055664   Contr Loss: 4.590333938598633\n",
      "pos_embedding: grad norm = 0.0025\n",
      "text_cls_token: grad norm = 0.1471\n",
      "vit.class_token: grad norm = 0.0018\n",
      "vit.conv_proj.weight: grad norm = 0.0053\n",
      "vit.conv_proj.bias: grad norm = 0.0007\n",
      "vit.encoder.pos_embedding: grad norm = 0.0018\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0009\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0010\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0238\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0010\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0697\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 16   Total Loss: 14.935355186462402   Gen Loss: 9.387200355529785   Contr Loss: 5.548154830932617\n",
      "pos_embedding: grad norm = 0.0025\n",
      "text_cls_token: grad norm = 0.1818\n",
      "vit.class_token: grad norm = 0.0019\n",
      "vit.conv_proj.weight: grad norm = 0.0036\n",
      "vit.conv_proj.bias: grad norm = 0.0007\n",
      "vit.encoder.pos_embedding: grad norm = 0.0020\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0009\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0009\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0219\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0010\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0487\n",
      "hiu\n",
      "-----------------------------------------------------------\n",
      "Iter: 17   Total Loss: 15.041007995605469   Gen Loss: 9.323884010314941   Contr Loss: 5.717123985290527\n",
      "pos_embedding: grad norm = 0.0009\n",
      "text_cls_token: grad norm = 0.1337\n",
      "vit.class_token: grad norm = 0.0014\n",
      "vit.conv_proj.weight: grad norm = 0.0043\n",
      "vit.conv_proj.bias: grad norm = 0.0002\n",
      "vit.encoder.pos_embedding: grad norm = 0.0014\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.weight: grad norm = 0.0006\n",
      "vit.encoder.layers.encoder_layer_0.ln_1.bias: grad norm = 0.0006\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight: grad norm = 0.0178\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias: grad norm = 0.0008\n",
      "vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight: grad norm = 0.0651\n"
     ]
    }
   ],
   "source": [
    "train(model=model, data=train_loader, val_data=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.float32).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "        # Since task is to predict next token, the labels will start form position 1\n",
    "        text_labels = text[:, 1:] \n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss.detach()\n",
    "        val_contrastive_loss += contrastive_loss.detach()\n",
    "        val_generative_loss += generative_loss.detach()\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
