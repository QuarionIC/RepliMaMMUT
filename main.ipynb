{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb886d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b019739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a884230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c36fe84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/hice1/hfaisal8/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7dc2a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41faf9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only using 1 parquet file. It contains about 5.8m examples\n",
    "url = 'https://huggingface.co/datasets/kakaobrain/coyo-700m/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet'\n",
    "\n",
    "data_files = {\"train\": url}\n",
    "\n",
    "pre_train_data = load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b72784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5836073\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01120acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6628e321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olive oil infused with Tuscany herbs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04191de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using only subset of data for now\n",
    "pre_train_data = pre_train_data.with_format(\"torch\")\n",
    "test_data = pre_train_data.select(range(6000, 7000))\n",
    "val_data = pre_train_data.select(range(5000, 6000))\n",
    "pre_train_data = pre_train_data.select(range(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e9a51cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'text', 'width', 'height', 'image_phash', 'text_length', 'word_count', 'num_tokens_bert', 'num_tokens_gpt', 'num_faces', 'clip_similarity_vitb32', 'clip_similarity_vitl14', 'nsfw_score_opennsfw2', 'nsfw_score_gantman', 'watermark_score', 'aesthetic_score_laion_v2'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b2f6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddeab91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Need to encode the text descriptions, and clean up images\n",
    "# TODO: Need to create a final dataset with text, and images\n",
    "# We can create a custom datalaoder that will load images from urls at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d71fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PreTrainDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']  \n",
    "        text = self.dataset[idx]['text'] # text has already been encoded and padded \n",
    "#         text = self.encode_text(text)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=1)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, text\n",
    "        except Exception:\n",
    "            return None\n",
    "#     def encode_text(self, example):\n",
    "#         text = self.tokenizer(example, padding='max_length', max_length=max_seq_len, add_special_tokens=True) # hard-coded max_length for now\n",
    "#         bos_id = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "#          # add a bos token as well\n",
    "#         text = {\n",
    "#             \"input_ids\": [bos_id] + text[\"input_ids\"],\n",
    "#             \"attention_mask\": [1] + text[\"attention_mask\"]\n",
    "#         }\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00981577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812a3ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_none_fn(batch):\n",
    "    batch_without_nones = [item for item in batch if item is not None]\n",
    "    if not batch_without_nones:\n",
    "        return []\n",
    "    if len(batch_without_nones) < len(batch):\n",
    "        batch_without_nones.extend([batch_without_nones[-1]] * (len(batch)-len(batch_without_nones)))\n",
    "    images, texts = zip(*batch_without_nones)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)\n",
    "    return images, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b9ec14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "pre_train_dataset_cleaned = PreTrainDataset(pre_train_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "val_dataset_cleaned = PreTrainDataset(val_data, tokenizer= tokenizer, transform=custom_transforms)\n",
    "train_loader = DataLoader(pre_train_dataset_cleaned, batch_size=32, shuffle=True, collate_fn=remove_none_fn)\n",
    "val_loader = DataLoader(val_dataset_cleaned, batch_size=10, shuffle=True, collate_fn=remove_none_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4461936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4c36d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_grad_norms(model, norm_type=2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            grad_norm = param.grad.norm(norm_type).item()\n",
    "            print(f\"{name}: grad norm = {grad_norm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150e5013-8069-4109-8c36-4e49b9e55de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0ca6d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, val_data, opt=None, lr=0.0001, weight_decay=0.00000, num_epochs=20, checkpoint_path='../checkpoints/', warmup_steps=100):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "#     n = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_contrastive_losses = []\n",
    "    train_generative_losses = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_contrastive_losses = []\n",
    "    val_generative_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    batch_size = 32\n",
    "    n = 0\n",
    "    accumulation_steps = 4\n",
    "    if opt is not None:\n",
    "        optimizer = opt\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay)\n",
    "    # scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, total_iters=warmup_steps)\n",
    "#     main_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "#     schedule = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_steps])\n",
    "    while epoch < num_epochs:\n",
    "\n",
    "        # Using AdamW for now, can try with other optimizers too\n",
    "\n",
    "\n",
    "        t_loss = 0\n",
    "        t_contrastive_loss = 0\n",
    "        t_generative_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "#             print(batch[0], len(batch[0]))\n",
    "            # input images, and texts\n",
    "            if not batch:\n",
    "                continue\n",
    "            imgs = batch[0].type(torch.float32).to(device)\n",
    "            text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "#             print(text)\n",
    "\n",
    "#             if len(imgs) < batch_size:\n",
    "#                 # Last batch will have less images, text pairs since it will be the\n",
    "#                 # remainder of Total images / batch_size.\n",
    "\n",
    "#                 # Adjust the learning rate of the last batch by \n",
    "#                 # (size(last_batch) / batch_size) to account \n",
    "#                 # for the smaller size.\n",
    "#                 adj_lr = lr * (len(imgs) / batch_size)\n",
    "#                 optimizer = optim.AdamW(model.parameters(),\n",
    "#                     lr=adj_lr,\n",
    "#                     weight_decay=weight_decay)\n",
    "            # Since task is to predict next token, the labels will start form position 1\n",
    "            text_labels = text[:, 1:] \n",
    "            total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "            total_loss = total_loss / accumulation_steps\n",
    "            \n",
    "            n += 1\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "            print(f\"Iter: {n}   Total Loss: {total_loss.item() * accumulation_steps}   Gen Loss: {generative_loss.item()}   Contr Loss: {contrastive_loss.item()}\")\n",
    "            total_loss.backward(retain_graph=True)\n",
    "#             contrastive_loss.backward(retain_graph=True)\n",
    "#             print(\"contrastive_norms\")\n",
    "#             log_grad_norms(model)\n",
    "#             generative_loss.backward(retain_graph=True)\n",
    "#             print(\"generative_norms\")\n",
    "#             log_grad_norms(model)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n",
    "            i = 0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.grad is not None:\n",
    "#                     print(f\"{name}: grad norm = {param.grad.norm().item():.4f}\")\n",
    "#                 i += 1\n",
    "#                 if i > 10:\n",
    "#                     break\n",
    "            if n % accumulation_steps == 0: # accumulate gradients to artifically increase batch size for learning\n",
    "               \n",
    "                optimizer.step()\n",
    "                # scheduler.step(total_loss)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # accumulate epoch loss\n",
    "            t_loss += total_loss.detach()\n",
    "            t_contrastive_loss += contrastive_loss.detach()\n",
    "            t_generative_loss += generative_loss.detach()\n",
    "            del imgs\n",
    "            del text\n",
    "            if n % 100 == 0:\n",
    "#                 torch.save(model.state_dict(), f\"{checkpoint_path}_iter_{n}\")\n",
    "                torch.save({\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        }, f\"{checkpoint_path}_model_checkpoint.pt\")\n",
    "                with open(f\"{checkpoint_path}_train_loss.pkl\", 'wb') as f:\n",
    "                    pickle.dump(train_losses, f)\n",
    "\n",
    "        # end of epoch\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        train_losses.append(t_loss * accumulation_steps / len(data))\n",
    "        train_contrastive_losses.append(t_contrastive_loss / len(data))\n",
    "        train_generative_losses.append(t_generative_loss / len(data))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        val_loss, val_contrastive_loss, val_generative_loss = validation(model, val_data)\n",
    "        val_losses.append(val_loss)\n",
    "        val_contrastive_losses.append(val_contrastive_loss)\n",
    "        val_generative_losses.append(val_generative_loss)\n",
    "        \n",
    "#         if epoch % 5 == 0: # save model every 5th epoch\n",
    "#         torch.save(model.state_dict(), f\"{checkpoint_path}_epoch_{epoch}\")\n",
    "        torch.save({\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }, f\"_model_checkpoint.pt\")\n",
    "            \n",
    "        print(\"Epoch {}:  Train loss: {}   Train Contrastive Loss: {}   Train Generative Loss: {}]\".format(epoch, t_loss / len(data), t_contrastive_loss / len(data), t_generative_loss / len(data)))\n",
    "        print(\"Epoch {}:  Val loss: {}   Val Contrastive Loss: {}   Val Generative Loss: {}]\".format(epoch, val_loss / len(val_data), val_contrastive_loss / len(val_data), val_generative_loss / len(val_data)))\n",
    "\n",
    "    return train_losses, train_contrastive_losses, train_generative_losses, val_losses, val_contrastive_losses, val_generative_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f87bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epoch = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_contrastive_loss = 0\n",
    "    val_generative_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        # input images, and texts\n",
    "        imgs = batch[0].type(torch.float32).to(device)\n",
    "        text = batch[1]['input_ids'].type(torch.long).to(device)\n",
    "        # Since task is to predict next token, the labels will start form position 1\n",
    "        text_labels = text[:, 1:] \n",
    "        total_loss, contrastive_loss, generative_loss = model(imgs, text, text_labels)\n",
    "\n",
    "        val_loss += total_loss.detach()\n",
    "        val_contrastive_loss += contrastive_loss.detach()\n",
    "        val_generative_loss += generative_loss.detach()\n",
    "\n",
    "    return val_loss, val_contrastive_loss, val_generative_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3732c220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0db509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import MaMMUT\n",
    "model = MaMMUT(vocab_size=tokenizer.vocab_size,\n",
    "                image_size= 224,\n",
    "                patch_size = 16,\n",
    "                vit_num_layers= 6,\n",
    "                vit_num_heads= 4,\n",
    "                vit_hidden_dim = 768,\n",
    "                vit_mlp_dim = 2048,\n",
    "                vit_dropout = 0.0, # Potential ablation / extension to add to the replication\n",
    "                vit_attention_dropout = 0.0, # Potential ablation / extension to add to the replication\n",
    "                contrastive_loss_weight = 1.25,\n",
    "                generative_loss_weight = 1.0,\n",
    "                text_decoder_depth = 4,\n",
    "                text_decoder_embed_dim = 256,\n",
    "                text_decoder_sub_layer_heads = 4,\n",
    "                text_decoder_feedforward_dim = 2048,\n",
    "                text_decoder_dk = 128,\n",
    "                latent_dim = 512,\n",
    "                contrastive_loss_gamma = 1.0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a8f045a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = torch.load('../checkpoints/_model_checkpoint.pt', weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10002b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "013cf1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(c['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263105a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = optim.AdamW(model.parameters(),\n",
    "                lr=0.01,\n",
    "                weight_decay=0)\n",
    "opt.load_state_dict(c['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d34f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 1   Total Loss: 10.513639450073242   Gen Loss: 6.381706237792969   Contr Loss: 3.3055460453033447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Iter: 2   Total Loss: 10.763405799865723   Gen Loss: 7.047630310058594   Contr Loss: 2.9726204872131348\n",
      "-----------------------------------------------------------\n",
      "Iter: 3   Total Loss: 10.812945365905762   Gen Loss: 7.086523532867432   Contr Loss: 2.981137275695801\n",
      "-----------------------------------------------------------\n",
      "Iter: 4   Total Loss: 10.941603660583496   Gen Loss: 7.328691005706787   Contr Loss: 2.8903303146362305\n",
      "-----------------------------------------------------------\n",
      "Iter: 5   Total Loss: 11.884536743164062   Gen Loss: 7.01146936416626   Contr Loss: 3.898453712463379\n",
      "-----------------------------------------------------------\n",
      "Iter: 6   Total Loss: 11.98591136932373   Gen Loss: 7.101951599121094   Contr Loss: 3.907167673110962\n",
      "-----------------------------------------------------------\n",
      "Iter: 7   Total Loss: 12.360930442810059   Gen Loss: 7.07823371887207   Contr Loss: 4.226157188415527\n",
      "-----------------------------------------------------------\n",
      "Iter: 8   Total Loss: 12.448683738708496   Gen Loss: 6.954749584197998   Contr Loss: 4.395147323608398\n",
      "-----------------------------------------------------------\n",
      "Iter: 9   Total Loss: 12.52770709991455   Gen Loss: 7.197288513183594   Contr Loss: 4.264334678649902\n",
      "-----------------------------------------------------------\n",
      "Iter: 10   Total Loss: 12.575323104858398   Gen Loss: 6.931310653686523   Contr Loss: 4.515210151672363\n",
      "-----------------------------------------------------------\n",
      "Iter: 11   Total Loss: 12.600114822387695   Gen Loss: 7.237364292144775   Contr Loss: 4.290200710296631\n",
      "-----------------------------------------------------------\n",
      "Iter: 12   Total Loss: 12.974018096923828   Gen Loss: 6.678192615509033   Contr Loss: 5.036660671234131\n",
      "-----------------------------------------------------------\n",
      "Iter: 13   Total Loss: 13.669240951538086   Gen Loss: 6.937936305999756   Contr Loss: 5.385043144226074\n",
      "-----------------------------------------------------------\n",
      "Iter: 14   Total Loss: 11.83061408996582   Gen Loss: 7.322380065917969   Contr Loss: 3.6065871715545654\n",
      "-----------------------------------------------------------\n",
      "Iter: 15   Total Loss: 12.03720474243164   Gen Loss: 7.33690071105957   Contr Loss: 3.7602434158325195\n",
      "-----------------------------------------------------------\n",
      "Iter: 16   Total Loss: 13.411947250366211   Gen Loss: 6.411391735076904   Contr Loss: 5.600444793701172\n",
      "-----------------------------------------------------------\n",
      "Iter: 17   Total Loss: 11.831851959228516   Gen Loss: 7.442592620849609   Contr Loss: 3.5114071369171143\n",
      "-----------------------------------------------------------\n",
      "Iter: 18   Total Loss: 11.777681350708008   Gen Loss: 7.000401496887207   Contr Loss: 3.821824073791504\n",
      "-----------------------------------------------------------\n",
      "Iter: 19   Total Loss: 11.9364652633667   Gen Loss: 6.9982099533081055   Contr Loss: 3.950604200363159\n",
      "-----------------------------------------------------------\n",
      "Iter: 20   Total Loss: 11.899872779846191   Gen Loss: 7.401697158813477   Contr Loss: 3.5985403060913086\n",
      "-----------------------------------------------------------\n",
      "Iter: 21   Total Loss: 12.208538055419922   Gen Loss: 7.825306415557861   Contr Loss: 3.5065855979919434\n",
      "-----------------------------------------------------------\n",
      "Iter: 22   Total Loss: 11.24828815460205   Gen Loss: 6.784778594970703   Contr Loss: 3.570807456970215\n",
      "-----------------------------------------------------------\n",
      "Iter: 23   Total Loss: 11.882497787475586   Gen Loss: 7.566761016845703   Contr Loss: 3.4525890350341797\n",
      "-----------------------------------------------------------\n",
      "Iter: 24   Total Loss: 11.746797561645508   Gen Loss: 7.173861503601074   Contr Loss: 3.658348798751831\n",
      "-----------------------------------------------------------\n",
      "Iter: 25   Total Loss: 11.535699844360352   Gen Loss: 7.208989143371582   Contr Loss: 3.461369037628174\n",
      "-----------------------------------------------------------\n",
      "Iter: 26   Total Loss: 11.77267837524414   Gen Loss: 7.321380138397217   Contr Loss: 3.561038017272949\n",
      "-----------------------------------------------------------\n",
      "Iter: 27   Total Loss: 11.975892066955566   Gen Loss: 7.71681547164917   Contr Loss: 3.407261371612549\n",
      "-----------------------------------------------------------\n",
      "Iter: 28   Total Loss: 12.144158363342285   Gen Loss: 7.613403797149658   Contr Loss: 3.624603748321533\n",
      "-----------------------------------------------------------\n",
      "Iter: 29   Total Loss: 11.866012573242188   Gen Loss: 7.575089454650879   Contr Loss: 3.4327383041381836\n",
      "-----------------------------------------------------------\n",
      "Iter: 30   Total Loss: 11.297555923461914   Gen Loss: 7.06873083114624   Contr Loss: 3.383059501647949\n",
      "-----------------------------------------------------------\n",
      "Iter: 31   Total Loss: 11.779155731201172   Gen Loss: 7.490909099578857   Contr Loss: 3.4305975437164307\n",
      "-----------------------------------------------------------\n",
      "Iter: 32   Total Loss: 11.324560165405273   Gen Loss: 7.067148208618164   Contr Loss: 3.4059300422668457\n",
      "-----------------------------------------------------------\n",
      "Iter: 33   Total Loss: 10.804200172424316   Gen Loss: 6.464824199676514   Contr Loss: 3.4715006351470947\n",
      "-----------------------------------------------------------\n",
      "Iter: 34   Total Loss: 11.741361618041992   Gen Loss: 7.47128438949585   Contr Loss: 3.416062355041504\n",
      "-----------------------------------------------------------\n",
      "Iter: 35   Total Loss: 11.317113876342773   Gen Loss: 7.023332595825195   Contr Loss: 3.435025215148926\n",
      "-----------------------------------------------------------\n",
      "Iter: 36   Total Loss: 11.487768173217773   Gen Loss: 7.22001314163208   Contr Loss: 3.4142038822174072\n",
      "-----------------------------------------------------------\n",
      "Iter: 37   Total Loss: 11.70008373260498   Gen Loss: 7.48248291015625   Contr Loss: 3.3740806579589844\n",
      "-----------------------------------------------------------\n",
      "Iter: 38   Total Loss: 11.458208084106445   Gen Loss: 7.236985206604004   Contr Loss: 3.3769781589508057\n",
      "-----------------------------------------------------------\n",
      "Iter: 39   Total Loss: 11.570110321044922   Gen Loss: 7.1641693115234375   Contr Loss: 3.5247530937194824\n",
      "-----------------------------------------------------------\n",
      "Iter: 40   Total Loss: 11.186483383178711   Gen Loss: 6.867361545562744   Contr Loss: 3.4552969932556152\n",
      "-----------------------------------------------------------\n",
      "Iter: 41   Total Loss: 11.737749099731445   Gen Loss: 7.530908584594727   Contr Loss: 3.3654727935791016\n",
      "-----------------------------------------------------------\n",
      "Iter: 42   Total Loss: 11.128039360046387   Gen Loss: 6.97152042388916   Contr Loss: 3.3252151012420654\n",
      "-----------------------------------------------------------\n",
      "Iter: 43   Total Loss: 10.8284330368042   Gen Loss: 6.646728992462158   Contr Loss: 3.345363140106201\n",
      "-----------------------------------------------------------\n",
      "Iter: 44   Total Loss: 11.7584228515625   Gen Loss: 7.624885559082031   Contr Loss: 3.3068294525146484\n",
      "-----------------------------------------------------------\n",
      "Iter: 45   Total Loss: 11.572150230407715   Gen Loss: 7.304859161376953   Contr Loss: 3.413832664489746\n",
      "-----------------------------------------------------------\n",
      "Iter: 46   Total Loss: 11.97523307800293   Gen Loss: 7.884187698364258   Contr Loss: 3.272836208343506\n",
      "-----------------------------------------------------------\n",
      "Iter: 47   Total Loss: 11.440170288085938   Gen Loss: 7.197887420654297   Contr Loss: 3.3938257694244385\n",
      "-----------------------------------------------------------\n",
      "Iter: 48   Total Loss: 11.933479309082031   Gen Loss: 7.588979244232178   Contr Loss: 3.475600004196167\n",
      "-----------------------------------------------------------\n",
      "Iter: 49   Total Loss: 11.695184707641602   Gen Loss: 7.40806245803833   Contr Loss: 3.429697275161743\n",
      "-----------------------------------------------------------\n",
      "Iter: 50   Total Loss: 11.886359214782715   Gen Loss: 7.556053638458252   Contr Loss: 3.4642446041107178\n",
      "-----------------------------------------------------------\n",
      "Iter: 51   Total Loss: 11.378822326660156   Gen Loss: 6.984076023101807   Contr Loss: 3.5157971382141113\n",
      "-----------------------------------------------------------\n",
      "Iter: 52   Total Loss: 11.571722984313965   Gen Loss: 7.380974769592285   Contr Loss: 3.3525986671447754\n",
      "-----------------------------------------------------------\n",
      "Iter: 53   Total Loss: 11.69914436340332   Gen Loss: 7.443621635437012   Contr Loss: 3.404418468475342\n",
      "-----------------------------------------------------------\n",
      "Iter: 54   Total Loss: 11.463905334472656   Gen Loss: 7.124760150909424   Contr Loss: 3.471315622329712\n",
      "-----------------------------------------------------------\n",
      "Iter: 55   Total Loss: 11.613338470458984   Gen Loss: 7.263256072998047   Contr Loss: 3.4800662994384766\n",
      "-----------------------------------------------------------\n",
      "Iter: 56   Total Loss: 11.252933502197266   Gen Loss: 7.239981651306152   Contr Loss: 3.2103610038757324\n",
      "-----------------------------------------------------------\n",
      "Iter: 57   Total Loss: 11.100008964538574   Gen Loss: 6.893227577209473   Contr Loss: 3.3654251098632812\n",
      "-----------------------------------------------------------\n",
      "Iter: 58   Total Loss: 11.56035041809082   Gen Loss: 7.240543365478516   Contr Loss: 3.455845355987549\n",
      "-----------------------------------------------------------\n",
      "Iter: 59   Total Loss: 11.22020149230957   Gen Loss: 6.983767509460449   Contr Loss: 3.3891472816467285\n",
      "-----------------------------------------------------------\n",
      "Iter: 60   Total Loss: 11.578981399536133   Gen Loss: 7.328226089477539   Contr Loss: 3.400604248046875\n",
      "-----------------------------------------------------------\n",
      "Iter: 61   Total Loss: 11.992936134338379   Gen Loss: 7.797947883605957   Contr Loss: 3.355990409851074\n",
      "-----------------------------------------------------------\n",
      "Iter: 62   Total Loss: 11.531098365783691   Gen Loss: 7.3737077713012695   Contr Loss: 3.3259124755859375\n",
      "-----------------------------------------------------------\n",
      "Iter: 63   Total Loss: 11.166141510009766   Gen Loss: 6.910002708435059   Contr Loss: 3.4049110412597656\n",
      "-----------------------------------------------------------\n",
      "Iter: 64   Total Loss: 11.183862686157227   Gen Loss: 6.995088577270508   Contr Loss: 3.3510191440582275\n",
      "-----------------------------------------------------------\n",
      "Iter: 65   Total Loss: 11.411310195922852   Gen Loss: 7.184267520904541   Contr Loss: 3.381633996963501\n",
      "-----------------------------------------------------------\n",
      "Iter: 66   Total Loss: 11.988965034484863   Gen Loss: 7.707766056060791   Contr Loss: 3.424959182739258\n",
      "-----------------------------------------------------------\n",
      "Iter: 67   Total Loss: 11.86097526550293   Gen Loss: 7.574845314025879   Contr Loss: 3.428903579711914\n",
      "-----------------------------------------------------------\n",
      "Iter: 68   Total Loss: 11.863920211791992   Gen Loss: 7.643831729888916   Contr Loss: 3.376070499420166\n",
      "-----------------------------------------------------------\n",
      "Iter: 69   Total Loss: 12.12498664855957   Gen Loss: 7.938148498535156   Contr Loss: 3.349470615386963\n",
      "-----------------------------------------------------------\n",
      "Iter: 70   Total Loss: 11.427284240722656   Gen Loss: 7.1178975105285645   Contr Loss: 3.447509288787842\n",
      "-----------------------------------------------------------\n",
      "Iter: 71   Total Loss: 10.861148834228516   Gen Loss: 6.628940582275391   Contr Loss: 3.3857665061950684\n",
      "-----------------------------------------------------------\n",
      "Iter: 72   Total Loss: 10.566835403442383   Gen Loss: 5.850493907928467   Contr Loss: 3.7730727195739746\n",
      "-----------------------------------------------------------\n",
      "Iter: 73   Total Loss: 11.651458740234375   Gen Loss: 7.381248474121094   Contr Loss: 3.416168212890625\n",
      "-----------------------------------------------------------\n",
      "Iter: 74   Total Loss: 11.61471939086914   Gen Loss: 7.552250385284424   Contr Loss: 3.2499752044677734\n",
      "-----------------------------------------------------------\n",
      "Iter: 75   Total Loss: 11.893330574035645   Gen Loss: 7.6011457443237305   Contr Loss: 3.4337477684020996\n",
      "-----------------------------------------------------------\n",
      "Iter: 76   Total Loss: 11.235834121704102   Gen Loss: 7.172041416168213   Contr Loss: 3.2510340213775635\n",
      "-----------------------------------------------------------\n",
      "Iter: 77   Total Loss: 11.296317100524902   Gen Loss: 7.11565637588501   Contr Loss: 3.3445284366607666\n",
      "-----------------------------------------------------------\n",
      "Iter: 78   Total Loss: 11.333621978759766   Gen Loss: 7.103949069976807   Contr Loss: 3.3837385177612305\n",
      "-----------------------------------------------------------\n",
      "Iter: 79   Total Loss: 11.026606559753418   Gen Loss: 6.804365158081055   Contr Loss: 3.377793312072754\n",
      "-----------------------------------------------------------\n",
      "Iter: 80   Total Loss: 11.515493392944336   Gen Loss: 7.281336784362793   Contr Loss: 3.387324810028076\n",
      "-----------------------------------------------------------\n",
      "Iter: 81   Total Loss: 12.075982093811035   Gen Loss: 8.032607078552246   Contr Loss: 3.2347002029418945\n",
      "-----------------------------------------------------------\n",
      "Iter: 82   Total Loss: 11.255661010742188   Gen Loss: 6.966400146484375   Contr Loss: 3.431408405303955\n",
      "-----------------------------------------------------------\n",
      "Iter: 83   Total Loss: 11.394718170166016   Gen Loss: 7.174309253692627   Contr Loss: 3.3763267993927\n",
      "-----------------------------------------------------------\n",
      "Iter: 84   Total Loss: 11.538829803466797   Gen Loss: 7.064907550811768   Contr Loss: 3.5791373252868652\n",
      "-----------------------------------------------------------\n",
      "Iter: 85   Total Loss: 11.27366828918457   Gen Loss: 7.157690048217773   Contr Loss: 3.2927823066711426\n",
      "-----------------------------------------------------------\n",
      "Iter: 86   Total Loss: 10.613823890686035   Gen Loss: 6.512991905212402   Contr Loss: 3.280665636062622\n",
      "-----------------------------------------------------------\n",
      "Iter: 87   Total Loss: 11.203413009643555   Gen Loss: 7.205124855041504   Contr Loss: 3.1986303329467773\n",
      "-----------------------------------------------------------\n",
      "Iter: 88   Total Loss: 11.480561256408691   Gen Loss: 7.332729339599609   Contr Loss: 3.318265438079834\n",
      "-----------------------------------------------------------\n",
      "Iter: 89   Total Loss: 11.010663986206055   Gen Loss: 6.900332450866699   Contr Loss: 3.2882657051086426\n",
      "-----------------------------------------------------------\n",
      "Iter: 90   Total Loss: 11.768108367919922   Gen Loss: 7.454518795013428   Contr Loss: 3.450871467590332\n",
      "-----------------------------------------------------------\n",
      "Iter: 91   Total Loss: 11.44141960144043   Gen Loss: 7.1072845458984375   Contr Loss: 3.4673078060150146\n",
      "-----------------------------------------------------------\n",
      "Iter: 92   Total Loss: 11.66099739074707   Gen Loss: 7.657109260559082   Contr Loss: 3.203110456466675\n",
      "-----------------------------------------------------------\n",
      "Iter: 93   Total Loss: 11.013507843017578   Gen Loss: 6.9449591636657715   Contr Loss: 3.2548394203186035\n",
      "-----------------------------------------------------------\n",
      "Iter: 94   Total Loss: 11.85507869720459   Gen Loss: 7.534584045410156   Contr Loss: 3.4563956260681152\n",
      "-----------------------------------------------------------\n",
      "Iter: 95   Total Loss: 11.418846130371094   Gen Loss: 7.3663763999938965   Contr Loss: 3.2419753074645996\n",
      "-----------------------------------------------------------\n",
      "Iter: 96   Total Loss: 11.720695495605469   Gen Loss: 7.500147342681885   Contr Loss: 3.376439094543457\n",
      "-----------------------------------------------------------\n",
      "Iter: 97   Total Loss: 11.1013765335083   Gen Loss: 7.016858100891113   Contr Loss: 3.2676146030426025\n",
      "-----------------------------------------------------------\n",
      "Iter: 98   Total Loss: 11.715572357177734   Gen Loss: 7.71092414855957   Contr Loss: 3.203718423843384\n",
      "-----------------------------------------------------------\n",
      "Iter: 99   Total Loss: 11.74157428741455   Gen Loss: 7.481144905090332   Contr Loss: 3.4083433151245117\n",
      "-----------------------------------------------------------\n",
      "Iter: 100   Total Loss: 11.169652938842773   Gen Loss: 7.042900085449219   Contr Loss: 3.3014018535614014\n",
      "-----------------------------------------------------------\n",
      "Iter: 101   Total Loss: 10.348175048828125   Gen Loss: 6.28037691116333   Contr Loss: 3.254239082336426\n",
      "-----------------------------------------------------------\n",
      "Iter: 102   Total Loss: 11.723633766174316   Gen Loss: 7.622562885284424   Contr Loss: 3.2808566093444824\n",
      "-----------------------------------------------------------\n",
      "Iter: 103   Total Loss: 11.840923309326172   Gen Loss: 7.783833980560303   Contr Loss: 3.2456717491149902\n",
      "-----------------------------------------------------------\n",
      "Iter: 104   Total Loss: 11.18056583404541   Gen Loss: 7.015542030334473   Contr Loss: 3.332019090652466\n",
      "-----------------------------------------------------------\n",
      "Iter: 105   Total Loss: 11.080911636352539   Gen Loss: 7.029893398284912   Contr Loss: 3.240814685821533\n",
      "-----------------------------------------------------------\n",
      "Iter: 106   Total Loss: 10.761613845825195   Gen Loss: 6.473907947540283   Contr Loss: 3.4301652908325195\n",
      "-----------------------------------------------------------\n",
      "Iter: 107   Total Loss: 11.83169937133789   Gen Loss: 7.561207294464111   Contr Loss: 3.416393756866455\n",
      "-----------------------------------------------------------\n",
      "Iter: 108   Total Loss: 10.957378387451172   Gen Loss: 6.800252437591553   Contr Loss: 3.325700283050537\n",
      "-----------------------------------------------------------\n",
      "Iter: 109   Total Loss: 11.405945777893066   Gen Loss: 7.368244171142578   Contr Loss: 3.230161190032959\n",
      "-----------------------------------------------------------\n",
      "Iter: 110   Total Loss: 11.612115859985352   Gen Loss: 6.987574100494385   Contr Loss: 3.6996331214904785\n",
      "-----------------------------------------------------------\n",
      "Iter: 111   Total Loss: 11.291467666625977   Gen Loss: 7.17075252532959   Contr Loss: 3.296572208404541\n",
      "-----------------------------------------------------------\n",
      "Iter: 112   Total Loss: 11.688312530517578   Gen Loss: 7.716273307800293   Contr Loss: 3.17763090133667\n",
      "-----------------------------------------------------------\n",
      "Iter: 113   Total Loss: 11.688241958618164   Gen Loss: 7.497336387634277   Contr Loss: 3.352724075317383\n",
      "-----------------------------------------------------------\n",
      "Iter: 114   Total Loss: 11.091226577758789   Gen Loss: 7.040744304656982   Contr Loss: 3.2403852939605713\n",
      "-----------------------------------------------------------\n",
      "Iter: 115   Total Loss: 11.046215057373047   Gen Loss: 6.883703708648682   Contr Loss: 3.3300085067749023\n",
      "-----------------------------------------------------------\n",
      "Iter: 116   Total Loss: 11.33283519744873   Gen Loss: 7.310874938964844   Contr Loss: 3.2175681591033936\n",
      "-----------------------------------------------------------\n",
      "Iter: 117   Total Loss: 11.083990097045898   Gen Loss: 6.8790693283081055   Contr Loss: 3.3639371395111084\n",
      "-----------------------------------------------------------\n",
      "Iter: 118   Total Loss: 11.086877822875977   Gen Loss: 7.034698009490967   Contr Loss: 3.24174427986145\n",
      "-----------------------------------------------------------\n",
      "Iter: 119   Total Loss: 11.167367935180664   Gen Loss: 6.800950050354004   Contr Loss: 3.493134021759033\n",
      "-----------------------------------------------------------\n",
      "Iter: 120   Total Loss: 11.842437744140625   Gen Loss: 7.0059661865234375   Contr Loss: 3.8691768646240234\n",
      "-----------------------------------------------------------\n",
      "Iter: 121   Total Loss: 11.228809356689453   Gen Loss: 7.15605354309082   Contr Loss: 3.2582051753997803\n",
      "-----------------------------------------------------------\n",
      "Iter: 122   Total Loss: 11.790454864501953   Gen Loss: 7.627975940704346   Contr Loss: 3.3299827575683594\n",
      "-----------------------------------------------------------\n",
      "Iter: 123   Total Loss: 11.863224029541016   Gen Loss: 7.303344249725342   Contr Loss: 3.6479034423828125\n",
      "-----------------------------------------------------------\n",
      "Iter: 124   Total Loss: 10.438279151916504   Gen Loss: 6.337396144866943   Contr Loss: 3.2807064056396484\n",
      "-----------------------------------------------------------\n",
      "Iter: 125   Total Loss: 11.731477737426758   Gen Loss: 7.52573823928833   Contr Loss: 3.3645918369293213\n",
      "-----------------------------------------------------------\n",
      "Iter: 126   Total Loss: 11.298389434814453   Gen Loss: 7.230721950531006   Contr Loss: 3.254133701324463\n",
      "-----------------------------------------------------------\n",
      "Iter: 127   Total Loss: 11.231419563293457   Gen Loss: 7.0551300048828125   Contr Loss: 3.341031551361084\n",
      "-----------------------------------------------------------\n",
      "Iter: 128   Total Loss: 11.382547378540039   Gen Loss: 7.3493475914001465   Contr Loss: 3.226560354232788\n",
      "-----------------------------------------------------------\n",
      "Iter: 129   Total Loss: 11.045867919921875   Gen Loss: 6.86661958694458   Contr Loss: 3.3433985710144043\n",
      "-----------------------------------------------------------\n",
      "Iter: 130   Total Loss: 11.20318603515625   Gen Loss: 7.0164594650268555   Contr Loss: 3.3493809700012207\n",
      "-----------------------------------------------------------\n",
      "Iter: 131   Total Loss: 11.268524169921875   Gen Loss: 6.9010491371154785   Contr Loss: 3.4939799308776855\n",
      "-----------------------------------------------------------\n",
      "Iter: 132   Total Loss: 11.730586051940918   Gen Loss: 7.542915344238281   Contr Loss: 3.3501367568969727\n",
      "-----------------------------------------------------------\n",
      "Iter: 133   Total Loss: 11.448678016662598   Gen Loss: 7.3587775230407715   Contr Loss: 3.2719204425811768\n",
      "-----------------------------------------------------------\n",
      "Iter: 134   Total Loss: 11.232826232910156   Gen Loss: 7.153680801391602   Contr Loss: 3.2633159160614014\n",
      "-----------------------------------------------------------\n",
      "Iter: 135   Total Loss: 11.28979778289795   Gen Loss: 7.275263786315918   Contr Loss: 3.2116270065307617\n",
      "-----------------------------------------------------------\n",
      "Iter: 136   Total Loss: 11.778142929077148   Gen Loss: 7.858972072601318   Contr Loss: 3.135336399078369\n",
      "-----------------------------------------------------------\n",
      "Iter: 137   Total Loss: 11.390841484069824   Gen Loss: 7.15782356262207   Contr Loss: 3.3864145278930664\n",
      "-----------------------------------------------------------\n",
      "Iter: 138   Total Loss: 11.502481460571289   Gen Loss: 7.467321395874023   Contr Loss: 3.2281277179718018\n",
      "-----------------------------------------------------------\n",
      "Iter: 139   Total Loss: 12.126218795776367   Gen Loss: 7.535978317260742   Contr Loss: 3.672191858291626\n",
      "-----------------------------------------------------------\n",
      "Iter: 140   Total Loss: 11.27157211303711   Gen Loss: 7.118686676025391   Contr Loss: 3.32230806350708\n",
      "-----------------------------------------------------------\n",
      "Iter: 141   Total Loss: 11.512641906738281   Gen Loss: 7.490421295166016   Contr Loss: 3.217776298522949\n",
      "-----------------------------------------------------------\n",
      "Iter: 142   Total Loss: 11.370532989501953   Gen Loss: 7.3319172859191895   Contr Loss: 3.2308928966522217\n",
      "-----------------------------------------------------------\n",
      "Iter: 143   Total Loss: 11.880317687988281   Gen Loss: 7.724816799163818   Contr Loss: 3.3244009017944336\n",
      "-----------------------------------------------------------\n",
      "Iter: 144   Total Loss: 11.273133277893066   Gen Loss: 7.210389614105225   Contr Loss: 3.250195026397705\n",
      "-----------------------------------------------------------\n",
      "Iter: 145   Total Loss: 9.772939682006836   Gen Loss: 5.714200496673584   Contr Loss: 3.2469918727874756\n",
      "-----------------------------------------------------------\n",
      "Iter: 146   Total Loss: 11.831920623779297   Gen Loss: 7.462802886962891   Contr Loss: 3.4952938556671143\n",
      "-----------------------------------------------------------\n",
      "Iter: 147   Total Loss: 11.642821311950684   Gen Loss: 7.419033050537109   Contr Loss: 3.379030704498291\n",
      "-----------------------------------------------------------\n",
      "Iter: 148   Total Loss: 11.085387229919434   Gen Loss: 7.1754374504089355   Contr Loss: 3.127959728240967\n",
      "-----------------------------------------------------------\n",
      "Iter: 149   Total Loss: 11.79305648803711   Gen Loss: 7.47927188873291   Contr Loss: 3.4510278701782227\n",
      "-----------------------------------------------------------\n",
      "Iter: 150   Total Loss: 11.754404067993164   Gen Loss: 6.773470401763916   Contr Loss: 3.9847469329833984\n",
      "-----------------------------------------------------------\n",
      "Iter: 151   Total Loss: 13.225927352905273   Gen Loss: 7.624665260314941   Contr Loss: 4.4810099601745605\n",
      "-----------------------------------------------------------\n",
      "Iter: 152   Total Loss: 11.432411193847656   Gen Loss: 7.384641170501709   Contr Loss: 3.238215446472168\n",
      "-----------------------------------------------------------\n",
      "Iter: 153   Total Loss: 11.166613578796387   Gen Loss: 7.076149940490723   Contr Loss: 3.2723708152770996\n",
      "-----------------------------------------------------------\n",
      "Iter: 154   Total Loss: 11.472765922546387   Gen Loss: 7.371457099914551   Contr Loss: 3.2810468673706055\n",
      "-----------------------------------------------------------\n",
      "Iter: 155   Total Loss: 11.106874465942383   Gen Loss: 6.9361891746521   Contr Loss: 3.336548328399658\n",
      "-----------------------------------------------------------\n",
      "Iter: 156   Total Loss: 11.56922435760498   Gen Loss: 7.416111946105957   Contr Loss: 3.3224899768829346\n",
      "-----------------------------------------------------------\n",
      "Iter: 157   Total Loss: 9.543373107910156   Gen Loss: 7.073355674743652   Contr Loss: 1.9760140180587769\n",
      "Epoch 1:  Train loss: 2.880535840988159   Train Contrastive Loss: 3.452144145965576   Train Generative Loss: 7.206961154937744]\n",
      "Epoch 1:  Val loss: 10.326748847961426   Val Contrastive Loss: 2.293469190597534   Val Generative Loss: 7.45991325378418]\n",
      "-----------------------------------------------------------\n",
      "Iter: 158   Total Loss: 11.951688766479492   Gen Loss: 7.91336727142334   Contr Loss: 3.2306575775146484\n",
      "-----------------------------------------------------------\n",
      "Iter: 159   Total Loss: 11.545370101928711   Gen Loss: 7.58278751373291   Contr Loss: 3.1700661182403564\n",
      "-----------------------------------------------------------\n",
      "Iter: 160   Total Loss: 11.570760726928711   Gen Loss: 7.168988227844238   Contr Loss: 3.5214180946350098\n",
      "-----------------------------------------------------------\n",
      "Iter: 161   Total Loss: 11.060272216796875   Gen Loss: 6.9177350997924805   Contr Loss: 3.314030170440674\n",
      "-----------------------------------------------------------\n",
      "Iter: 162   Total Loss: 10.834224700927734   Gen Loss: 7.109375   Contr Loss: 2.979879379272461\n",
      "-----------------------------------------------------------\n",
      "Iter: 163   Total Loss: 11.654218673706055   Gen Loss: 6.729981899261475   Contr Loss: 3.939389944076538\n",
      "-----------------------------------------------------------\n",
      "Iter: 164   Total Loss: 11.426351547241211   Gen Loss: 7.3774824142456055   Contr Loss: 3.2390949726104736\n",
      "-----------------------------------------------------------\n",
      "Iter: 165   Total Loss: 11.410476684570312   Gen Loss: 7.124244689941406   Contr Loss: 3.428986072540283\n",
      "-----------------------------------------------------------\n",
      "Iter: 166   Total Loss: 11.359901428222656   Gen Loss: 7.100956916809082   Contr Loss: 3.4071555137634277\n",
      "-----------------------------------------------------------\n",
      "Iter: 167   Total Loss: 11.117420196533203   Gen Loss: 7.315385341644287   Contr Loss: 3.041628360748291\n",
      "-----------------------------------------------------------\n",
      "Iter: 168   Total Loss: 11.625387191772461   Gen Loss: 7.336871147155762   Contr Loss: 3.4308128356933594\n",
      "-----------------------------------------------------------\n",
      "Iter: 169   Total Loss: 11.504413604736328   Gen Loss: 7.345790386199951   Contr Loss: 3.3268990516662598\n",
      "-----------------------------------------------------------\n",
      "Iter: 170   Total Loss: 11.2867431640625   Gen Loss: 6.902939319610596   Contr Loss: 3.5070433616638184\n",
      "-----------------------------------------------------------\n",
      "Iter: 171   Total Loss: 11.651592254638672   Gen Loss: 7.015235424041748   Contr Loss: 3.7090859413146973\n",
      "-----------------------------------------------------------\n",
      "Iter: 172   Total Loss: 10.71306037902832   Gen Loss: 6.706672668457031   Contr Loss: 3.2051100730895996\n",
      "-----------------------------------------------------------\n",
      "Iter: 173   Total Loss: 10.199919700622559   Gen Loss: 6.326252460479736   Contr Loss: 3.098933696746826\n",
      "-----------------------------------------------------------\n",
      "Iter: 174   Total Loss: 11.10162353515625   Gen Loss: 7.2149977684021   Contr Loss: 3.1093008518218994\n",
      "-----------------------------------------------------------\n",
      "Iter: 175   Total Loss: 11.201902389526367   Gen Loss: 7.120259761810303   Contr Loss: 3.2653145790100098\n",
      "-----------------------------------------------------------\n",
      "Iter: 176   Total Loss: 11.038972854614258   Gen Loss: 7.0486063957214355   Contr Loss: 3.1922926902770996\n",
      "-----------------------------------------------------------\n",
      "Iter: 177   Total Loss: 11.17487907409668   Gen Loss: 7.251468658447266   Contr Loss: 3.138728141784668\n",
      "-----------------------------------------------------------\n",
      "Iter: 178   Total Loss: 11.728775024414062   Gen Loss: 7.640840530395508   Contr Loss: 3.2703471183776855\n",
      "-----------------------------------------------------------\n",
      "Iter: 179   Total Loss: 11.515848159790039   Gen Loss: 7.319734573364258   Contr Loss: 3.35689115524292\n",
      "-----------------------------------------------------------\n",
      "Iter: 180   Total Loss: 10.792176246643066   Gen Loss: 6.699982643127441   Contr Loss: 3.2737550735473633\n",
      "-----------------------------------------------------------\n",
      "Iter: 181   Total Loss: 11.711054801940918   Gen Loss: 7.5728278160095215   Contr Loss: 3.3105814456939697\n",
      "-----------------------------------------------------------\n",
      "Iter: 182   Total Loss: 11.130605697631836   Gen Loss: 7.109158992767334   Contr Loss: 3.2171578407287598\n",
      "-----------------------------------------------------------\n",
      "Iter: 183   Total Loss: 10.939231872558594   Gen Loss: 6.848311901092529   Contr Loss: 3.272735834121704\n",
      "-----------------------------------------------------------\n",
      "Iter: 184   Total Loss: 11.469992637634277   Gen Loss: 7.324806213378906   Contr Loss: 3.3161489963531494\n",
      "-----------------------------------------------------------\n",
      "Iter: 185   Total Loss: 11.59537124633789   Gen Loss: 7.413601875305176   Contr Loss: 3.3454155921936035\n",
      "-----------------------------------------------------------\n",
      "Iter: 186   Total Loss: 10.94180679321289   Gen Loss: 6.889523983001709   Contr Loss: 3.2418267726898193\n",
      "-----------------------------------------------------------\n",
      "Iter: 187   Total Loss: 11.159757614135742   Gen Loss: 7.137829780578613   Contr Loss: 3.2175421714782715\n",
      "-----------------------------------------------------------\n",
      "Iter: 188   Total Loss: 11.205785751342773   Gen Loss: 7.126036167144775   Contr Loss: 3.2637996673583984\n",
      "-----------------------------------------------------------\n",
      "Iter: 189   Total Loss: 11.38205623626709   Gen Loss: 7.390939235687256   Contr Loss: 3.1928935050964355\n",
      "-----------------------------------------------------------\n",
      "Iter: 190   Total Loss: 11.291339874267578   Gen Loss: 7.120797634124756   Contr Loss: 3.3364341259002686\n",
      "-----------------------------------------------------------\n",
      "Iter: 191   Total Loss: 11.179529190063477   Gen Loss: 7.143404006958008   Contr Loss: 3.2288999557495117\n",
      "-----------------------------------------------------------\n",
      "Iter: 192   Total Loss: 11.28216552734375   Gen Loss: 7.262082576751709   Contr Loss: 3.216066837310791\n",
      "-----------------------------------------------------------\n",
      "Iter: 193   Total Loss: 11.280366897583008   Gen Loss: 7.454132080078125   Contr Loss: 3.060988187789917\n",
      "-----------------------------------------------------------\n",
      "Iter: 194   Total Loss: 11.046385765075684   Gen Loss: 6.766048908233643   Contr Loss: 3.424269437789917\n",
      "-----------------------------------------------------------\n",
      "Iter: 195   Total Loss: 12.265386581420898   Gen Loss: 7.69264030456543   Contr Loss: 3.6581971645355225\n",
      "-----------------------------------------------------------\n",
      "Iter: 196   Total Loss: 11.179706573486328   Gen Loss: 7.1772027015686035   Contr Loss: 3.20200252532959\n",
      "-----------------------------------------------------------\n",
      "Iter: 197   Total Loss: 11.311495780944824   Gen Loss: 7.026086807250977   Contr Loss: 3.4283270835876465\n",
      "-----------------------------------------------------------\n",
      "Iter: 198   Total Loss: 11.086629867553711   Gen Loss: 7.144243240356445   Contr Loss: 3.153909683227539\n",
      "-----------------------------------------------------------\n",
      "Iter: 199   Total Loss: 11.073629379272461   Gen Loss: 7.125837802886963   Contr Loss: 3.158233642578125\n",
      "-----------------------------------------------------------\n",
      "Iter: 200   Total Loss: 10.941620826721191   Gen Loss: 6.909731864929199   Contr Loss: 3.225511312484741\n",
      "-----------------------------------------------------------\n",
      "Iter: 201   Total Loss: 10.95599365234375   Gen Loss: 6.845585346221924   Contr Loss: 3.2883267402648926\n",
      "-----------------------------------------------------------\n",
      "Iter: 202   Total Loss: 10.908088684082031   Gen Loss: 6.727605819702148   Contr Loss: 3.344386577606201\n",
      "-----------------------------------------------------------\n",
      "Iter: 203   Total Loss: 11.145883560180664   Gen Loss: 7.023693561553955   Contr Loss: 3.2977519035339355\n",
      "-----------------------------------------------------------\n",
      "Iter: 204   Total Loss: 11.07270622253418   Gen Loss: 7.248215198516846   Contr Loss: 3.0595932006835938\n",
      "-----------------------------------------------------------\n",
      "Iter: 205   Total Loss: 11.52534294128418   Gen Loss: 7.242709636688232   Contr Loss: 3.4261064529418945\n",
      "-----------------------------------------------------------\n",
      "Iter: 206   Total Loss: 11.345056533813477   Gen Loss: 7.208731174468994   Contr Loss: 3.30906081199646\n",
      "-----------------------------------------------------------\n",
      "Iter: 207   Total Loss: 10.940206527709961   Gen Loss: 7.07170295715332   Contr Loss: 3.0948028564453125\n",
      "-----------------------------------------------------------\n",
      "Iter: 208   Total Loss: 10.81269645690918   Gen Loss: 6.870077133178711   Contr Loss: 3.1540956497192383\n",
      "-----------------------------------------------------------\n",
      "Iter: 209   Total Loss: 11.441118240356445   Gen Loss: 7.390288352966309   Contr Loss: 3.240663528442383\n",
      "-----------------------------------------------------------\n",
      "Iter: 210   Total Loss: 11.407838821411133   Gen Loss: 7.198379039764404   Contr Loss: 3.367567539215088\n",
      "-----------------------------------------------------------\n",
      "Iter: 211   Total Loss: 10.887767791748047   Gen Loss: 6.835148811340332   Contr Loss: 3.242095470428467\n",
      "-----------------------------------------------------------\n",
      "Iter: 212   Total Loss: 11.418667793273926   Gen Loss: 7.453085899353027   Contr Loss: 3.1724653244018555\n",
      "-----------------------------------------------------------\n",
      "Iter: 213   Total Loss: 11.32715892791748   Gen Loss: 7.548648357391357   Contr Loss: 3.022808313369751\n",
      "-----------------------------------------------------------\n",
      "Iter: 214   Total Loss: 11.439934730529785   Gen Loss: 7.354160785675049   Contr Loss: 3.2686192989349365\n",
      "-----------------------------------------------------------\n",
      "Iter: 215   Total Loss: 11.789288520812988   Gen Loss: 7.092482566833496   Contr Loss: 3.7574448585510254\n",
      "-----------------------------------------------------------\n",
      "Iter: 216   Total Loss: 11.230507850646973   Gen Loss: 6.700009346008301   Contr Loss: 3.624398708343506\n",
      "-----------------------------------------------------------\n",
      "Iter: 217   Total Loss: 11.257814407348633   Gen Loss: 7.163811206817627   Contr Loss: 3.275202512741089\n",
      "-----------------------------------------------------------\n",
      "Iter: 218   Total Loss: 11.671634674072266   Gen Loss: 7.239486217498779   Contr Loss: 3.5457186698913574\n",
      "-----------------------------------------------------------\n",
      "Iter: 219   Total Loss: 11.505966186523438   Gen Loss: 7.342891216278076   Contr Loss: 3.3304603099823\n",
      "-----------------------------------------------------------\n",
      "Iter: 220   Total Loss: 11.316839218139648   Gen Loss: 7.217973709106445   Contr Loss: 3.279092788696289\n",
      "-----------------------------------------------------------\n",
      "Iter: 221   Total Loss: 11.750177383422852   Gen Loss: 7.569520950317383   Contr Loss: 3.34452486038208\n",
      "-----------------------------------------------------------\n",
      "Iter: 222   Total Loss: 11.137224197387695   Gen Loss: 6.944700241088867   Contr Loss: 3.3540194034576416\n",
      "-----------------------------------------------------------\n",
      "Iter: 223   Total Loss: 11.07440185546875   Gen Loss: 6.986510276794434   Contr Loss: 3.2703137397766113\n",
      "-----------------------------------------------------------\n",
      "Iter: 224   Total Loss: 11.283964157104492   Gen Loss: 7.29316520690918   Contr Loss: 3.1926393508911133\n",
      "-----------------------------------------------------------\n",
      "Iter: 225   Total Loss: 11.77591323852539   Gen Loss: 7.322571754455566   Contr Loss: 3.5626730918884277\n",
      "-----------------------------------------------------------\n",
      "Iter: 226   Total Loss: 11.227365493774414   Gen Loss: 7.110193729400635   Contr Loss: 3.2937378883361816\n",
      "-----------------------------------------------------------\n",
      "Iter: 227   Total Loss: 11.54672622680664   Gen Loss: 7.371703624725342   Contr Loss: 3.3400182723999023\n",
      "-----------------------------------------------------------\n",
      "Iter: 228   Total Loss: 11.500020980834961   Gen Loss: 7.272328853607178   Contr Loss: 3.382153272628784\n",
      "-----------------------------------------------------------\n",
      "Iter: 229   Total Loss: 10.76593017578125   Gen Loss: 6.664788722991943   Contr Loss: 3.2809131145477295\n",
      "-----------------------------------------------------------\n",
      "Iter: 230   Total Loss: 11.499551773071289   Gen Loss: 7.66441535949707   Contr Loss: 3.0681090354919434\n",
      "-----------------------------------------------------------\n",
      "Iter: 231   Total Loss: 11.517343521118164   Gen Loss: 6.932061672210693   Contr Loss: 3.6682257652282715\n",
      "-----------------------------------------------------------\n",
      "Iter: 232   Total Loss: 11.650151252746582   Gen Loss: 7.382839679718018   Contr Loss: 3.413849115371704\n",
      "-----------------------------------------------------------\n",
      "Iter: 233   Total Loss: 11.396554946899414   Gen Loss: 7.124782085418701   Contr Loss: 3.4174180030822754\n",
      "-----------------------------------------------------------\n",
      "Iter: 234   Total Loss: 11.047174453735352   Gen Loss: 7.092372894287109   Contr Loss: 3.1638412475585938\n",
      "-----------------------------------------------------------\n",
      "Iter: 235   Total Loss: 10.72951889038086   Gen Loss: 6.73637580871582   Contr Loss: 3.194514751434326\n",
      "-----------------------------------------------------------\n",
      "Iter: 236   Total Loss: 11.36938762664795   Gen Loss: 7.245090484619141   Contr Loss: 3.2994377613067627\n",
      "-----------------------------------------------------------\n",
      "Iter: 237   Total Loss: 10.642324447631836   Gen Loss: 6.484591484069824   Contr Loss: 3.326185941696167\n",
      "-----------------------------------------------------------\n",
      "Iter: 238   Total Loss: 11.058794021606445   Gen Loss: 6.973932266235352   Contr Loss: 3.2678890228271484\n",
      "-----------------------------------------------------------\n",
      "Iter: 239   Total Loss: 11.10028076171875   Gen Loss: 7.083027362823486   Contr Loss: 3.2138028144836426\n",
      "-----------------------------------------------------------\n",
      "Iter: 240   Total Loss: 11.060853004455566   Gen Loss: 7.409976482391357   Contr Loss: 2.920701026916504\n",
      "-----------------------------------------------------------\n",
      "Iter: 241   Total Loss: 11.387396812438965   Gen Loss: 7.217782974243164   Contr Loss: 3.335691213607788\n",
      "-----------------------------------------------------------\n",
      "Iter: 242   Total Loss: 11.162956237792969   Gen Loss: 6.982789039611816   Contr Loss: 3.3441340923309326\n",
      "-----------------------------------------------------------\n",
      "Iter: 243   Total Loss: 11.019220352172852   Gen Loss: 6.784063816070557   Contr Loss: 3.3881256580352783\n",
      "-----------------------------------------------------------\n",
      "Iter: 244   Total Loss: 10.966581344604492   Gen Loss: 6.904608726501465   Contr Loss: 3.2495779991149902\n",
      "-----------------------------------------------------------\n",
      "Iter: 245   Total Loss: 11.233667373657227   Gen Loss: 7.156710147857666   Contr Loss: 3.261565685272217\n",
      "-----------------------------------------------------------\n",
      "Iter: 246   Total Loss: 11.24969482421875   Gen Loss: 7.3446125984191895   Contr Loss: 3.124065637588501\n",
      "-----------------------------------------------------------\n",
      "Iter: 247   Total Loss: 11.81589126586914   Gen Loss: 7.535674571990967   Contr Loss: 3.4241738319396973\n",
      "-----------------------------------------------------------\n",
      "Iter: 248   Total Loss: 10.834535598754883   Gen Loss: 6.810990333557129   Contr Loss: 3.2188358306884766\n",
      "-----------------------------------------------------------\n",
      "Iter: 249   Total Loss: 11.430482864379883   Gen Loss: 7.266287803649902   Contr Loss: 3.3313562870025635\n",
      "-----------------------------------------------------------\n",
      "Iter: 250   Total Loss: 10.602578163146973   Gen Loss: 6.690883636474609   Contr Loss: 3.1293554306030273\n",
      "-----------------------------------------------------------\n",
      "Iter: 251   Total Loss: 11.291433334350586   Gen Loss: 7.20785665512085   Contr Loss: 3.266861915588379\n",
      "-----------------------------------------------------------\n",
      "Iter: 252   Total Loss: 11.087057113647461   Gen Loss: 7.120939254760742   Contr Loss: 3.17289400100708\n",
      "-----------------------------------------------------------\n",
      "Iter: 253   Total Loss: 11.000720977783203   Gen Loss: 6.8984808921813965   Contr Loss: 3.281792640686035\n",
      "-----------------------------------------------------------\n",
      "Iter: 254   Total Loss: 11.149752616882324   Gen Loss: 6.946338653564453   Contr Loss: 3.3627309799194336\n",
      "-----------------------------------------------------------\n",
      "Iter: 255   Total Loss: 10.985296249389648   Gen Loss: 6.947664260864258   Contr Loss: 3.2301058769226074\n",
      "-----------------------------------------------------------\n",
      "Iter: 256   Total Loss: 11.510404586791992   Gen Loss: 7.351346969604492   Contr Loss: 3.3272461891174316\n",
      "-----------------------------------------------------------\n",
      "Iter: 257   Total Loss: 10.671950340270996   Gen Loss: 6.599353313446045   Contr Loss: 3.258077621459961\n",
      "-----------------------------------------------------------\n",
      "Iter: 258   Total Loss: 10.366357803344727   Gen Loss: 5.6762800216674805   Contr Loss: 3.7520623207092285\n",
      "-----------------------------------------------------------\n",
      "Iter: 259   Total Loss: 11.747696876525879   Gen Loss: 7.558887958526611   Contr Loss: 3.3510470390319824\n",
      "-----------------------------------------------------------\n",
      "Iter: 260   Total Loss: 11.008852005004883   Gen Loss: 6.887556076049805   Contr Loss: 3.297037124633789\n",
      "-----------------------------------------------------------\n",
      "Iter: 261   Total Loss: 11.008805274963379   Gen Loss: 7.35724401473999   Contr Loss: 2.9212491512298584\n",
      "-----------------------------------------------------------\n",
      "Iter: 262   Total Loss: 11.375883102416992   Gen Loss: 7.236082553863525   Contr Loss: 3.311840534210205\n",
      "-----------------------------------------------------------\n",
      "Iter: 263   Total Loss: 11.106212615966797   Gen Loss: 7.208180904388428   Contr Loss: 3.1184258460998535\n",
      "-----------------------------------------------------------\n",
      "Iter: 264   Total Loss: 11.180681228637695   Gen Loss: 6.838560581207275   Contr Loss: 3.473696708679199\n",
      "-----------------------------------------------------------\n",
      "Iter: 265   Total Loss: 11.174880027770996   Gen Loss: 7.34688138961792   Contr Loss: 3.062398910522461\n",
      "-----------------------------------------------------------\n",
      "Iter: 266   Total Loss: 10.833932876586914   Gen Loss: 6.823703765869141   Contr Loss: 3.2081832885742188\n",
      "-----------------------------------------------------------\n",
      "Iter: 267   Total Loss: 11.469162940979004   Gen Loss: 6.739409923553467   Contr Loss: 3.7838025093078613\n",
      "-----------------------------------------------------------\n",
      "Iter: 268   Total Loss: 11.25051498413086   Gen Loss: 7.208186626434326   Contr Loss: 3.233863115310669\n",
      "-----------------------------------------------------------\n",
      "Iter: 269   Total Loss: 11.38039493560791   Gen Loss: 7.3204665184021   Contr Loss: 3.2479426860809326\n",
      "-----------------------------------------------------------\n",
      "Iter: 270   Total Loss: 11.721256256103516   Gen Loss: 7.497419834136963   Contr Loss: 3.3790688514709473\n",
      "-----------------------------------------------------------\n",
      "Iter: 271   Total Loss: 11.063260078430176   Gen Loss: 7.139407157897949   Contr Loss: 3.139082431793213\n",
      "-----------------------------------------------------------\n",
      "Iter: 272   Total Loss: 11.431137084960938   Gen Loss: 7.210992813110352   Contr Loss: 3.376115560531616\n",
      "-----------------------------------------------------------\n",
      "Iter: 273   Total Loss: 11.998941421508789   Gen Loss: 7.937504291534424   Contr Loss: 3.2491495609283447\n",
      "-----------------------------------------------------------\n",
      "Iter: 274   Total Loss: 10.90580940246582   Gen Loss: 6.881249904632568   Contr Loss: 3.219647169113159\n",
      "-----------------------------------------------------------\n",
      "Iter: 275   Total Loss: 11.33389663696289   Gen Loss: 7.3238749504089355   Contr Loss: 3.208016872406006\n",
      "-----------------------------------------------------------\n",
      "Iter: 276   Total Loss: 11.026138305664062   Gen Loss: 6.956202983856201   Contr Loss: 3.255948066711426\n",
      "-----------------------------------------------------------\n",
      "Iter: 277   Total Loss: 11.035735130310059   Gen Loss: 7.241569519042969   Contr Loss: 3.035332679748535\n",
      "-----------------------------------------------------------\n",
      "Iter: 278   Total Loss: 11.313321113586426   Gen Loss: 6.946755886077881   Contr Loss: 3.4932522773742676\n",
      "-----------------------------------------------------------\n",
      "Iter: 279   Total Loss: 11.230319023132324   Gen Loss: 7.143224239349365   Contr Loss: 3.2696757316589355\n",
      "-----------------------------------------------------------\n",
      "Iter: 280   Total Loss: 11.159228324890137   Gen Loss: 7.192506313323975   Contr Loss: 3.173377752304077\n",
      "-----------------------------------------------------------\n",
      "Iter: 281   Total Loss: 11.737035751342773   Gen Loss: 7.540726184844971   Contr Loss: 3.3570475578308105\n",
      "-----------------------------------------------------------\n",
      "Iter: 282   Total Loss: 11.174108505249023   Gen Loss: 7.0680718421936035   Contr Loss: 3.2848293781280518\n",
      "-----------------------------------------------------------\n",
      "Iter: 283   Total Loss: 11.76872730255127   Gen Loss: 7.623981952667236   Contr Loss: 3.315796375274658\n",
      "-----------------------------------------------------------\n",
      "Iter: 284   Total Loss: 11.335540771484375   Gen Loss: 7.031186103820801   Contr Loss: 3.443483829498291\n",
      "-----------------------------------------------------------\n",
      "Iter: 285   Total Loss: 11.960480690002441   Gen Loss: 7.873642444610596   Contr Loss: 3.269470453262329\n",
      "-----------------------------------------------------------\n",
      "Iter: 286   Total Loss: 11.224817276000977   Gen Loss: 7.187765598297119   Contr Loss: 3.2296409606933594\n",
      "-----------------------------------------------------------\n",
      "Iter: 287   Total Loss: 11.568193435668945   Gen Loss: 7.057236194610596   Contr Loss: 3.6087656021118164\n",
      "-----------------------------------------------------------\n",
      "Iter: 288   Total Loss: 11.301077842712402   Gen Loss: 7.467831134796143   Contr Loss: 3.0665974617004395\n",
      "-----------------------------------------------------------\n",
      "Iter: 289   Total Loss: 11.311689376831055   Gen Loss: 6.973470211029053   Contr Loss: 3.4705748558044434\n",
      "-----------------------------------------------------------\n",
      "Iter: 290   Total Loss: 10.88125991821289   Gen Loss: 7.0129828453063965   Contr Loss: 3.0946221351623535\n",
      "-----------------------------------------------------------\n",
      "Iter: 291   Total Loss: 11.466691970825195   Gen Loss: 7.495312213897705   Contr Loss: 3.1771035194396973\n",
      "-----------------------------------------------------------\n",
      "Iter: 292   Total Loss: 11.688946723937988   Gen Loss: 7.724877834320068   Contr Loss: 3.171254873275757\n",
      "-----------------------------------------------------------\n",
      "Iter: 293   Total Loss: 12.87923812866211   Gen Loss: 7.49927282333374   Contr Loss: 4.303972244262695\n",
      "-----------------------------------------------------------\n",
      "Iter: 294   Total Loss: 11.872222900390625   Gen Loss: 8.017190933227539   Contr Loss: 3.0840253829956055\n",
      "-----------------------------------------------------------\n",
      "Iter: 295   Total Loss: 11.099188804626465   Gen Loss: 6.960678577423096   Contr Loss: 3.3108081817626953\n",
      "-----------------------------------------------------------\n",
      "Iter: 296   Total Loss: 11.046002388000488   Gen Loss: 7.086780548095703   Contr Loss: 3.1673777103424072\n",
      "-----------------------------------------------------------\n",
      "Iter: 297   Total Loss: 11.651596069335938   Gen Loss: 7.4162068367004395   Contr Loss: 3.3883118629455566\n",
      "-----------------------------------------------------------\n",
      "Iter: 298   Total Loss: 11.2488374710083   Gen Loss: 6.919325828552246   Contr Loss: 3.463609457015991\n",
      "-----------------------------------------------------------\n",
      "Iter: 299   Total Loss: 11.246026039123535   Gen Loss: 7.039935111999512   Contr Loss: 3.364872694015503\n",
      "-----------------------------------------------------------\n",
      "Iter: 300   Total Loss: 11.809113502502441   Gen Loss: 7.701910495758057   Contr Loss: 3.285762310028076\n",
      "-----------------------------------------------------------\n",
      "Iter: 301   Total Loss: 11.592941284179688   Gen Loss: 7.441338539123535   Contr Loss: 3.321281909942627\n",
      "-----------------------------------------------------------\n",
      "Iter: 302   Total Loss: 11.089558601379395   Gen Loss: 7.043496131896973   Contr Loss: 3.236849784851074\n",
      "-----------------------------------------------------------\n",
      "Iter: 303   Total Loss: 11.109952926635742   Gen Loss: 6.805849075317383   Contr Loss: 3.4432826042175293\n",
      "-----------------------------------------------------------\n",
      "Iter: 304   Total Loss: 11.925003051757812   Gen Loss: 7.422519683837891   Contr Loss: 3.6019864082336426\n",
      "-----------------------------------------------------------\n",
      "Iter: 305   Total Loss: 11.302595138549805   Gen Loss: 7.3026580810546875   Contr Loss: 3.1999497413635254\n",
      "-----------------------------------------------------------\n",
      "Iter: 306   Total Loss: 10.952343940734863   Gen Loss: 6.769038200378418   Contr Loss: 3.346644401550293\n",
      "-----------------------------------------------------------\n",
      "Iter: 307   Total Loss: 10.763256072998047   Gen Loss: 6.863722801208496   Contr Loss: 3.119626998901367\n",
      "-----------------------------------------------------------\n",
      "Iter: 308   Total Loss: 11.542739868164062   Gen Loss: 7.1151041984558105   Contr Loss: 3.5421090126037598\n",
      "-----------------------------------------------------------\n",
      "Iter: 309   Total Loss: 12.117979049682617   Gen Loss: 7.55966854095459   Contr Loss: 3.646648406982422\n",
      "-----------------------------------------------------------\n",
      "Iter: 310   Total Loss: 11.891134262084961   Gen Loss: 7.580365180969238   Contr Loss: 3.448615550994873\n",
      "-----------------------------------------------------------\n",
      "Iter: 311   Total Loss: 11.212461471557617   Gen Loss: 7.111358642578125   Contr Loss: 3.2808823585510254\n",
      "-----------------------------------------------------------\n",
      "Iter: 312   Total Loss: 11.532862663269043   Gen Loss: 7.195997714996338   Contr Loss: 3.469491958618164\n",
      "-----------------------------------------------------------\n",
      "Iter: 313   Total Loss: 11.17060661315918   Gen Loss: 6.997481822967529   Contr Loss: 3.3384993076324463\n",
      "-----------------------------------------------------------\n",
      "Iter: 314   Total Loss: 9.714055061340332   Gen Loss: 7.359163761138916   Contr Loss: 1.8839130401611328\n",
      "Epoch 2:  Train loss: 2.820357084274292   Train Contrastive Loss: 3.2975032329559326   Train Generative Loss: 7.159549713134766]\n",
      "Epoch 2:  Val loss: 10.419831275939941   Val Contrastive Loss: 2.359382152557373   Val Generative Loss: 7.470601558685303]\n",
      "-----------------------------------------------------------\n",
      "Iter: 315   Total Loss: 10.860836029052734   Gen Loss: 7.202773571014404   Contr Loss: 2.9264495372772217\n",
      "-----------------------------------------------------------\n",
      "Iter: 316   Total Loss: 10.889089584350586   Gen Loss: 6.953854560852051   Contr Loss: 3.1481881141662598\n",
      "-----------------------------------------------------------\n",
      "Iter: 317   Total Loss: 11.051525115966797   Gen Loss: 7.236678600311279   Contr Loss: 3.051877498626709\n",
      "-----------------------------------------------------------\n",
      "Iter: 318   Total Loss: 11.347439765930176   Gen Loss: 7.443872451782227   Contr Loss: 3.1228537559509277\n",
      "-----------------------------------------------------------\n",
      "Iter: 319   Total Loss: 11.234748840332031   Gen Loss: 7.254395484924316   Contr Loss: 3.1842823028564453\n",
      "-----------------------------------------------------------\n",
      "Iter: 320   Total Loss: 10.495525360107422   Gen Loss: 6.560136318206787   Contr Loss: 3.148311138153076\n",
      "-----------------------------------------------------------\n",
      "Iter: 321   Total Loss: 11.361225128173828   Gen Loss: 7.280521869659424   Contr Loss: 3.2645630836486816\n",
      "-----------------------------------------------------------\n",
      "Iter: 322   Total Loss: 11.202190399169922   Gen Loss: 6.90286922454834   Contr Loss: 3.4394571781158447\n",
      "-----------------------------------------------------------\n",
      "Iter: 323   Total Loss: 11.34852123260498   Gen Loss: 7.1617817878723145   Contr Loss: 3.349391460418701\n",
      "-----------------------------------------------------------\n",
      "Iter: 324   Total Loss: 10.783684730529785   Gen Loss: 7.27354097366333   Contr Loss: 2.808115005493164\n",
      "-----------------------------------------------------------\n",
      "Iter: 325   Total Loss: 10.873903274536133   Gen Loss: 7.017340660095215   Contr Loss: 3.085249900817871\n",
      "-----------------------------------------------------------\n",
      "Iter: 326   Total Loss: 11.98225212097168   Gen Loss: 7.42540168762207   Contr Loss: 3.645480155944824\n",
      "-----------------------------------------------------------\n",
      "Iter: 327   Total Loss: 10.99753189086914   Gen Loss: 6.984691143035889   Contr Loss: 3.2102720737457275\n",
      "-----------------------------------------------------------\n",
      "Iter: 328   Total Loss: 10.960737228393555   Gen Loss: 7.042078495025635   Contr Loss: 3.134927272796631\n",
      "-----------------------------------------------------------\n",
      "Iter: 329   Total Loss: 11.304204940795898   Gen Loss: 7.297153949737549   Contr Loss: 3.205641269683838\n",
      "-----------------------------------------------------------\n",
      "Iter: 330   Total Loss: 11.072166442871094   Gen Loss: 7.119543552398682   Contr Loss: 3.1620984077453613\n",
      "-----------------------------------------------------------\n",
      "Iter: 331   Total Loss: 10.755064010620117   Gen Loss: 6.574807643890381   Contr Loss: 3.344205141067505\n",
      "-----------------------------------------------------------\n",
      "Iter: 332   Total Loss: 11.076998710632324   Gen Loss: 6.916786193847656   Contr Loss: 3.328169822692871\n",
      "-----------------------------------------------------------\n",
      "Iter: 333   Total Loss: 10.89754867553711   Gen Loss: 7.046445846557617   Contr Loss: 3.0808820724487305\n",
      "-----------------------------------------------------------\n",
      "Iter: 334   Total Loss: 10.439497947692871   Gen Loss: 6.400365352630615   Contr Loss: 3.2313060760498047\n",
      "-----------------------------------------------------------\n",
      "Iter: 335   Total Loss: 11.293031692504883   Gen Loss: 7.323610782623291   Contr Loss: 3.175537109375\n",
      "-----------------------------------------------------------\n",
      "Iter: 336   Total Loss: 11.284780502319336   Gen Loss: 7.296204090118408   Contr Loss: 3.190861225128174\n",
      "-----------------------------------------------------------\n",
      "Iter: 337   Total Loss: 11.14547348022461   Gen Loss: 7.366953372955322   Contr Loss: 3.022815704345703\n",
      "-----------------------------------------------------------\n",
      "Iter: 338   Total Loss: 10.978025436401367   Gen Loss: 6.992454528808594   Contr Loss: 3.1884565353393555\n",
      "-----------------------------------------------------------\n",
      "Iter: 339   Total Loss: 11.381963729858398   Gen Loss: 7.231520175933838   Contr Loss: 3.320354461669922\n",
      "-----------------------------------------------------------\n",
      "Iter: 340   Total Loss: 11.64159107208252   Gen Loss: 7.4717512130737305   Contr Loss: 3.335871696472168\n",
      "-----------------------------------------------------------\n",
      "Iter: 341   Total Loss: 11.10097885131836   Gen Loss: 6.963032245635986   Contr Loss: 3.310357093811035\n",
      "-----------------------------------------------------------\n",
      "Iter: 342   Total Loss: 11.093921661376953   Gen Loss: 7.035320281982422   Contr Loss: 3.2468814849853516\n",
      "-----------------------------------------------------------\n",
      "Iter: 343   Total Loss: 11.358835220336914   Gen Loss: 7.132303237915039   Contr Loss: 3.381225824356079\n",
      "-----------------------------------------------------------\n",
      "Iter: 344   Total Loss: 11.262429237365723   Gen Loss: 7.381463527679443   Contr Loss: 3.1047725677490234\n",
      "-----------------------------------------------------------\n",
      "Iter: 345   Total Loss: 10.756335258483887   Gen Loss: 6.894979000091553   Contr Loss: 3.0890848636627197\n",
      "-----------------------------------------------------------\n",
      "Iter: 346   Total Loss: 10.881834030151367   Gen Loss: 7.121713161468506   Contr Loss: 3.008096694946289\n",
      "-----------------------------------------------------------\n",
      "Iter: 347   Total Loss: 11.108760833740234   Gen Loss: 7.110785007476807   Contr Loss: 3.198380470275879\n",
      "-----------------------------------------------------------\n",
      "Iter: 348   Total Loss: 10.68911361694336   Gen Loss: 6.9154052734375   Contr Loss: 3.0189661979675293\n",
      "-----------------------------------------------------------\n",
      "Iter: 349   Total Loss: 11.133764266967773   Gen Loss: 7.083976745605469   Contr Loss: 3.239830255508423\n",
      "-----------------------------------------------------------\n",
      "Iter: 350   Total Loss: 11.25   Gen Loss: 7.177541255950928   Contr Loss: 3.257967472076416\n",
      "-----------------------------------------------------------\n",
      "Iter: 351   Total Loss: 10.730157852172852   Gen Loss: 6.9596405029296875   Contr Loss: 3.016414165496826\n",
      "-----------------------------------------------------------\n",
      "Iter: 352   Total Loss: 11.517127990722656   Gen Loss: 7.3239030838012695   Contr Loss: 3.354579448699951\n",
      "-----------------------------------------------------------\n",
      "Iter: 353   Total Loss: 11.032305717468262   Gen Loss: 7.147758483886719   Contr Loss: 3.107637882232666\n",
      "-----------------------------------------------------------\n",
      "Iter: 354   Total Loss: 10.90536117553711   Gen Loss: 7.153504371643066   Contr Loss: 3.0014851093292236\n",
      "-----------------------------------------------------------\n",
      "Iter: 355   Total Loss: 11.494863510131836   Gen Loss: 7.08620548248291   Contr Loss: 3.526926040649414\n",
      "-----------------------------------------------------------\n",
      "Iter: 356   Total Loss: 12.083423614501953   Gen Loss: 7.91400671005249   Contr Loss: 3.3355331420898438\n",
      "-----------------------------------------------------------\n",
      "Iter: 357   Total Loss: 11.044379234313965   Gen Loss: 7.034217357635498   Contr Loss: 3.208129405975342\n",
      "-----------------------------------------------------------\n",
      "Iter: 358   Total Loss: 11.696244239807129   Gen Loss: 7.489951133728027   Contr Loss: 3.365034341812134\n",
      "-----------------------------------------------------------\n",
      "Iter: 359   Total Loss: 10.544578552246094   Gen Loss: 6.414033889770508   Contr Loss: 3.3044352531433105\n",
      "-----------------------------------------------------------\n",
      "Iter: 360   Total Loss: 11.280855178833008   Gen Loss: 7.124480247497559   Contr Loss: 3.3250999450683594\n",
      "-----------------------------------------------------------\n",
      "Iter: 361   Total Loss: 11.183825492858887   Gen Loss: 7.0668745040893555   Contr Loss: 3.293560743331909\n",
      "-----------------------------------------------------------\n",
      "Iter: 362   Total Loss: 11.588467597961426   Gen Loss: 7.13408899307251   Contr Loss: 3.5635030269622803\n",
      "-----------------------------------------------------------\n",
      "Iter: 363   Total Loss: 9.629619598388672   Gen Loss: 5.9184794425964355   Contr Loss: 2.96891188621521\n",
      "-----------------------------------------------------------\n",
      "Iter: 364   Total Loss: 11.868532180786133   Gen Loss: 7.415651321411133   Contr Loss: 3.5623044967651367\n",
      "-----------------------------------------------------------\n",
      "Iter: 365   Total Loss: 11.152020454406738   Gen Loss: 7.243974208831787   Contr Loss: 3.126436948776245\n",
      "-----------------------------------------------------------\n",
      "Iter: 366   Total Loss: 11.210376739501953   Gen Loss: 7.175827980041504   Contr Loss: 3.2276394367218018\n",
      "-----------------------------------------------------------\n",
      "Iter: 367   Total Loss: 11.422325134277344   Gen Loss: 7.2778801918029785   Contr Loss: 3.315556287765503\n",
      "-----------------------------------------------------------\n",
      "Iter: 368   Total Loss: 12.479934692382812   Gen Loss: 6.717361927032471   Contr Loss: 4.610058307647705\n",
      "-----------------------------------------------------------\n",
      "Iter: 369   Total Loss: 10.620708465576172   Gen Loss: 6.1391921043396   Contr Loss: 3.5852136611938477\n",
      "-----------------------------------------------------------\n",
      "Iter: 370   Total Loss: 10.67497444152832   Gen Loss: 6.67206335067749   Contr Loss: 3.202329397201538\n",
      "-----------------------------------------------------------\n",
      "Iter: 371   Total Loss: 10.782291412353516   Gen Loss: 6.795560836791992   Contr Loss: 3.1893844604492188\n",
      "-----------------------------------------------------------\n",
      "Iter: 372   Total Loss: 11.106061935424805   Gen Loss: 7.050502300262451   Contr Loss: 3.244447708129883\n",
      "-----------------------------------------------------------\n",
      "Iter: 373   Total Loss: 11.486438751220703   Gen Loss: 7.2616868019104   Contr Loss: 3.3798012733459473\n",
      "-----------------------------------------------------------\n",
      "Iter: 374   Total Loss: 9.699954986572266   Gen Loss: 5.430157661437988   Contr Loss: 3.4158377647399902\n",
      "-----------------------------------------------------------\n",
      "Iter: 375   Total Loss: 11.710655212402344   Gen Loss: 7.204336166381836   Contr Loss: 3.6050548553466797\n",
      "-----------------------------------------------------------\n",
      "Iter: 376   Total Loss: 10.913432121276855   Gen Loss: 6.857208251953125   Contr Loss: 3.244978904724121\n",
      "-----------------------------------------------------------\n",
      "Iter: 377   Total Loss: 11.427305221557617   Gen Loss: 7.160034656524658   Contr Loss: 3.4138169288635254\n",
      "-----------------------------------------------------------\n",
      "Iter: 378   Total Loss: 10.808252334594727   Gen Loss: 6.726406574249268   Contr Loss: 3.265476703643799\n",
      "-----------------------------------------------------------\n",
      "Iter: 379   Total Loss: 10.61363410949707   Gen Loss: 6.494972229003906   Contr Loss: 3.294929027557373\n",
      "-----------------------------------------------------------\n",
      "Iter: 380   Total Loss: 11.29643726348877   Gen Loss: 7.440186500549316   Contr Loss: 3.085000514984131\n",
      "-----------------------------------------------------------\n",
      "Iter: 381   Total Loss: 11.510199546813965   Gen Loss: 7.491430759429932   Contr Loss: 3.215014934539795\n",
      "-----------------------------------------------------------\n",
      "Iter: 382   Total Loss: 11.3562650680542   Gen Loss: 7.260124206542969   Contr Loss: 3.2769126892089844\n",
      "-----------------------------------------------------------\n",
      "Iter: 383   Total Loss: 11.159656524658203   Gen Loss: 7.06454610824585   Contr Loss: 3.276087999343872\n",
      "-----------------------------------------------------------\n",
      "Iter: 384   Total Loss: 11.286767959594727   Gen Loss: 7.251240253448486   Contr Loss: 3.2284226417541504\n",
      "-----------------------------------------------------------\n",
      "Iter: 385   Total Loss: 11.401718139648438   Gen Loss: 7.399466037750244   Contr Loss: 3.201801300048828\n",
      "-----------------------------------------------------------\n",
      "Iter: 386   Total Loss: 11.044233322143555   Gen Loss: 7.083302021026611   Contr Loss: 3.1687445640563965\n",
      "-----------------------------------------------------------\n",
      "Iter: 387   Total Loss: 11.266322135925293   Gen Loss: 7.202143669128418   Contr Loss: 3.2513427734375\n",
      "-----------------------------------------------------------\n",
      "Iter: 388   Total Loss: 11.268202781677246   Gen Loss: 7.140926361083984   Contr Loss: 3.301820993423462\n",
      "-----------------------------------------------------------\n",
      "Iter: 389   Total Loss: 10.875324249267578   Gen Loss: 7.093268871307373   Contr Loss: 3.0256447792053223\n",
      "-----------------------------------------------------------\n",
      "Iter: 390   Total Loss: 11.089868545532227   Gen Loss: 6.932908058166504   Contr Loss: 3.325568199157715\n",
      "-----------------------------------------------------------\n",
      "Iter: 391   Total Loss: 11.355608940124512   Gen Loss: 7.1938371658325195   Contr Loss: 3.3294174671173096\n",
      "-----------------------------------------------------------\n",
      "Iter: 392   Total Loss: 11.090084075927734   Gen Loss: 7.067099571228027   Contr Loss: 3.218388080596924\n",
      "-----------------------------------------------------------\n",
      "Iter: 393   Total Loss: 10.906595230102539   Gen Loss: 6.952805042266846   Contr Loss: 3.163032054901123\n",
      "-----------------------------------------------------------\n",
      "Iter: 394   Total Loss: 12.224761009216309   Gen Loss: 7.9741363525390625   Contr Loss: 3.4004998207092285\n",
      "-----------------------------------------------------------\n",
      "Iter: 395   Total Loss: 11.199329376220703   Gen Loss: 6.90419864654541   Contr Loss: 3.4361045360565186\n",
      "-----------------------------------------------------------\n",
      "Iter: 396   Total Loss: 11.532419204711914   Gen Loss: 7.344242572784424   Contr Loss: 3.350541591644287\n",
      "-----------------------------------------------------------\n",
      "Iter: 397   Total Loss: 11.093964576721191   Gen Loss: 7.098836898803711   Contr Loss: 3.1961023807525635\n",
      "-----------------------------------------------------------\n",
      "Iter: 398   Total Loss: 11.515731811523438   Gen Loss: 7.322306156158447   Contr Loss: 3.3547401428222656\n",
      "-----------------------------------------------------------\n",
      "Iter: 399   Total Loss: 10.07856559753418   Gen Loss: 5.958357810974121   Contr Loss: 3.296166181564331\n",
      "-----------------------------------------------------------\n",
      "Iter: 400   Total Loss: 10.194379806518555   Gen Loss: 5.735182762145996   Contr Loss: 3.5673575401306152\n",
      "-----------------------------------------------------------\n",
      "Iter: 401   Total Loss: 11.582511901855469   Gen Loss: 7.535434246063232   Contr Loss: 3.2376623153686523\n",
      "-----------------------------------------------------------\n",
      "Iter: 402   Total Loss: 11.452007293701172   Gen Loss: 7.168922424316406   Contr Loss: 3.4264674186706543\n",
      "-----------------------------------------------------------\n",
      "Iter: 403   Total Loss: 11.427371978759766   Gen Loss: 7.298690319061279   Contr Loss: 3.302945852279663\n",
      "-----------------------------------------------------------\n",
      "Iter: 404   Total Loss: 11.471275329589844   Gen Loss: 7.109799385070801   Contr Loss: 3.489180326461792\n",
      "-----------------------------------------------------------\n",
      "Iter: 405   Total Loss: 11.491661071777344   Gen Loss: 7.314213752746582   Contr Loss: 3.3419575691223145\n",
      "-----------------------------------------------------------\n",
      "Iter: 406   Total Loss: 11.162009239196777   Gen Loss: 7.632456302642822   Contr Loss: 2.8236424922943115\n",
      "-----------------------------------------------------------\n",
      "Iter: 407   Total Loss: 10.370610237121582   Gen Loss: 6.332488059997559   Contr Loss: 3.2304978370666504\n",
      "-----------------------------------------------------------\n",
      "Iter: 408   Total Loss: 11.229920387268066   Gen Loss: 7.020349502563477   Contr Loss: 3.367656707763672\n",
      "-----------------------------------------------------------\n",
      "Iter: 409   Total Loss: 11.356698989868164   Gen Loss: 7.3105082511901855   Contr Loss: 3.236952781677246\n",
      "-----------------------------------------------------------\n",
      "Iter: 410   Total Loss: 11.178180694580078   Gen Loss: 7.03656005859375   Contr Loss: 3.313296318054199\n",
      "-----------------------------------------------------------\n",
      "Iter: 411   Total Loss: 11.295373916625977   Gen Loss: 7.2247748374938965   Contr Loss: 3.256479024887085\n",
      "-----------------------------------------------------------\n",
      "Iter: 412   Total Loss: 11.23742961883545   Gen Loss: 7.1846137046813965   Contr Loss: 3.2422525882720947\n",
      "-----------------------------------------------------------\n",
      "Iter: 413   Total Loss: 11.256141662597656   Gen Loss: 7.151849269866943   Contr Loss: 3.283433437347412\n",
      "-----------------------------------------------------------\n",
      "Iter: 414   Total Loss: 11.22452163696289   Gen Loss: 7.2032880783081055   Contr Loss: 3.216986656188965\n",
      "-----------------------------------------------------------\n",
      "Iter: 415   Total Loss: 11.574498176574707   Gen Loss: 7.498579978942871   Contr Loss: 3.2607345581054688\n",
      "-----------------------------------------------------------\n",
      "Iter: 416   Total Loss: 10.910805702209473   Gen Loss: 6.9051408767700195   Contr Loss: 3.204531669616699\n",
      "-----------------------------------------------------------\n",
      "Iter: 417   Total Loss: 11.539783477783203   Gen Loss: 7.17838716506958   Contr Loss: 3.48911714553833\n",
      "-----------------------------------------------------------\n",
      "Iter: 418   Total Loss: 11.100125312805176   Gen Loss: 7.047375679016113   Contr Loss: 3.242199659347534\n",
      "-----------------------------------------------------------\n",
      "Iter: 419   Total Loss: 11.259100914001465   Gen Loss: 7.18343448638916   Contr Loss: 3.260533094406128\n",
      "-----------------------------------------------------------\n",
      "Iter: 420   Total Loss: 11.07086181640625   Gen Loss: 6.999674320220947   Contr Loss: 3.2569503784179688\n",
      "-----------------------------------------------------------\n",
      "Iter: 421   Total Loss: 11.353874206542969   Gen Loss: 7.319675922393799   Contr Loss: 3.227358102798462\n",
      "-----------------------------------------------------------\n",
      "Iter: 422   Total Loss: 11.623135566711426   Gen Loss: 7.399747848510742   Contr Loss: 3.3787102699279785\n",
      "-----------------------------------------------------------\n",
      "Iter: 423   Total Loss: 11.139270782470703   Gen Loss: 6.85932731628418   Contr Loss: 3.423954725265503\n",
      "-----------------------------------------------------------\n",
      "Iter: 424   Total Loss: 11.1845064163208   Gen Loss: 6.985073089599609   Contr Loss: 3.359546661376953\n",
      "-----------------------------------------------------------\n",
      "Iter: 425   Total Loss: 11.36296272277832   Gen Loss: 7.146590709686279   Contr Loss: 3.3730976581573486\n",
      "-----------------------------------------------------------\n",
      "Iter: 426   Total Loss: 11.58798599243164   Gen Loss: 7.440442085266113   Contr Loss: 3.3180348873138428\n",
      "-----------------------------------------------------------\n",
      "Iter: 427   Total Loss: 10.17070484161377   Gen Loss: 6.078245639801025   Contr Loss: 3.2739675045013428\n",
      "-----------------------------------------------------------\n",
      "Iter: 428   Total Loss: 11.372322082519531   Gen Loss: 7.215754985809326   Contr Loss: 3.325254201889038\n",
      "-----------------------------------------------------------\n",
      "Iter: 429   Total Loss: 11.76028060913086   Gen Loss: 7.617110729217529   Contr Loss: 3.314535617828369\n",
      "-----------------------------------------------------------\n",
      "Iter: 430   Total Loss: 10.978476524353027   Gen Loss: 6.877735137939453   Contr Loss: 3.280592918395996\n",
      "-----------------------------------------------------------\n",
      "Iter: 431   Total Loss: 11.32447624206543   Gen Loss: 7.157362461090088   Contr Loss: 3.333691120147705\n",
      "-----------------------------------------------------------\n",
      "Iter: 432   Total Loss: 11.106853485107422   Gen Loss: 7.0544304847717285   Contr Loss: 3.2419381141662598\n",
      "-----------------------------------------------------------\n",
      "Iter: 433   Total Loss: 11.36336898803711   Gen Loss: 7.172976970672607   Contr Loss: 3.352313280105591\n",
      "-----------------------------------------------------------\n",
      "Iter: 434   Total Loss: 11.042946815490723   Gen Loss: 7.043268203735352   Contr Loss: 3.1997430324554443\n",
      "-----------------------------------------------------------\n",
      "Iter: 435   Total Loss: 10.68403434753418   Gen Loss: 6.699218273162842   Contr Loss: 3.1878528594970703\n",
      "-----------------------------------------------------------\n",
      "Iter: 436   Total Loss: 11.041692733764648   Gen Loss: 7.085181713104248   Contr Loss: 3.1652092933654785\n",
      "-----------------------------------------------------------\n",
      "Iter: 437   Total Loss: 11.22038745880127   Gen Loss: 7.1031389236450195   Contr Loss: 3.2937989234924316\n",
      "-----------------------------------------------------------\n",
      "Iter: 438   Total Loss: 11.69320297241211   Gen Loss: 7.5423994064331055   Contr Loss: 3.3206424713134766\n",
      "-----------------------------------------------------------\n",
      "Iter: 439   Total Loss: 11.472512245178223   Gen Loss: 7.449668884277344   Contr Loss: 3.2182745933532715\n",
      "-----------------------------------------------------------\n",
      "Iter: 440   Total Loss: 11.6298828125   Gen Loss: 7.458992958068848   Contr Loss: 3.3367114067077637\n",
      "-----------------------------------------------------------\n",
      "Iter: 441   Total Loss: 11.35019588470459   Gen Loss: 7.172938346862793   Contr Loss: 3.341806173324585\n",
      "-----------------------------------------------------------\n",
      "Iter: 442   Total Loss: 11.079875946044922   Gen Loss: 7.018996715545654   Contr Loss: 3.248703956604004\n",
      "-----------------------------------------------------------\n",
      "Iter: 443   Total Loss: 10.926761627197266   Gen Loss: 6.866700172424316   Contr Loss: 3.248048782348633\n",
      "-----------------------------------------------------------\n",
      "Iter: 444   Total Loss: 11.435493469238281   Gen Loss: 7.181186199188232   Contr Loss: 3.4034461975097656\n",
      "-----------------------------------------------------------\n",
      "Iter: 445   Total Loss: 11.26969051361084   Gen Loss: 7.139991760253906   Contr Loss: 3.3037590980529785\n",
      "-----------------------------------------------------------\n",
      "Iter: 446   Total Loss: 11.529805183410645   Gen Loss: 7.660771369934082   Contr Loss: 3.0952272415161133\n",
      "-----------------------------------------------------------\n",
      "Iter: 447   Total Loss: 11.115151405334473   Gen Loss: 6.8234148025512695   Contr Loss: 3.433389186859131\n",
      "-----------------------------------------------------------\n",
      "Iter: 448   Total Loss: 10.482176780700684   Gen Loss: 6.635052680969238   Contr Loss: 3.0776991844177246\n",
      "-----------------------------------------------------------\n",
      "Iter: 449   Total Loss: 10.643869400024414   Gen Loss: 6.693787097930908   Contr Loss: 3.1600661277770996\n",
      "-----------------------------------------------------------\n",
      "Iter: 450   Total Loss: 10.604789733886719   Gen Loss: 6.716342926025391   Contr Loss: 3.110757827758789\n",
      "-----------------------------------------------------------\n",
      "Iter: 451   Total Loss: 10.771080017089844   Gen Loss: 6.495996952056885   Contr Loss: 3.4200663566589355\n",
      "-----------------------------------------------------------\n",
      "Iter: 452   Total Loss: 11.01581859588623   Gen Loss: 7.0493316650390625   Contr Loss: 3.173189640045166\n",
      "-----------------------------------------------------------\n",
      "Iter: 453   Total Loss: 10.970918655395508   Gen Loss: 6.802987575531006   Contr Loss: 3.3343448638916016\n",
      "-----------------------------------------------------------\n",
      "Iter: 454   Total Loss: 9.612415313720703   Gen Loss: 5.645530700683594   Contr Loss: 3.1735081672668457\n",
      "-----------------------------------------------------------\n",
      "Iter: 455   Total Loss: 11.343696594238281   Gen Loss: 7.249754905700684   Contr Loss: 3.275153636932373\n",
      "-----------------------------------------------------------\n",
      "Iter: 456   Total Loss: 10.99598503112793   Gen Loss: 6.981067657470703   Contr Loss: 3.2119333744049072\n",
      "-----------------------------------------------------------\n",
      "Iter: 457   Total Loss: 10.681572914123535   Gen Loss: 6.816218852996826   Contr Loss: 3.092283010482788\n",
      "-----------------------------------------------------------\n",
      "Iter: 458   Total Loss: 11.080268859863281   Gen Loss: 7.16308069229126   Contr Loss: 3.1337509155273438\n",
      "-----------------------------------------------------------\n",
      "Iter: 459   Total Loss: 10.333332061767578   Gen Loss: 6.329987525939941   Contr Loss: 3.2026758193969727\n",
      "-----------------------------------------------------------\n",
      "Iter: 460   Total Loss: 11.073290824890137   Gen Loss: 7.197450160980225   Contr Loss: 3.100672483444214\n",
      "-----------------------------------------------------------\n",
      "Iter: 461   Total Loss: 10.853260040283203   Gen Loss: 6.954941272735596   Contr Loss: 3.118654727935791\n",
      "-----------------------------------------------------------\n",
      "Iter: 462   Total Loss: 11.386251449584961   Gen Loss: 7.422401428222656   Contr Loss: 3.171079635620117\n",
      "-----------------------------------------------------------\n",
      "Iter: 463   Total Loss: 12.327836036682129   Gen Loss: 7.933752536773682   Contr Loss: 3.5152666568756104\n",
      "-----------------------------------------------------------\n",
      "Iter: 464   Total Loss: 11.378257751464844   Gen Loss: 7.438072204589844   Contr Loss: 3.1521482467651367\n",
      "-----------------------------------------------------------\n",
      "Iter: 465   Total Loss: 11.065261840820312   Gen Loss: 7.004885673522949   Contr Loss: 3.2483012676239014\n",
      "-----------------------------------------------------------\n",
      "Iter: 466   Total Loss: 11.548713684082031   Gen Loss: 7.273319721221924   Contr Loss: 3.420315742492676\n",
      "-----------------------------------------------------------\n",
      "Iter: 467   Total Loss: 11.557134628295898   Gen Loss: 7.226043701171875   Contr Loss: 3.4648728370666504\n",
      "-----------------------------------------------------------\n",
      "Iter: 468   Total Loss: 11.24872875213623   Gen Loss: 7.222685813903809   Contr Loss: 3.220834493637085\n",
      "-----------------------------------------------------------\n",
      "Iter: 469   Total Loss: 11.072105407714844   Gen Loss: 7.005563259124756   Contr Loss: 3.2532334327697754\n",
      "-----------------------------------------------------------\n",
      "Iter: 470   Total Loss: 11.7904052734375   Gen Loss: 7.484127521514893   Contr Loss: 3.4450225830078125\n",
      "-----------------------------------------------------------\n",
      "Iter: 471   Total Loss: 8.480344772338867   Gen Loss: 6.390585899353027   Contr Loss: 1.6718071699142456\n",
      "Epoch 3:  Train loss: 2.783564805984497   Train Contrastive Loss: 3.256784200668335   Train Generative Loss: 7.0632781982421875]\n",
      "Epoch 3:  Val loss: 10.480241775512695   Val Contrastive Loss: 2.3895299434661865   Val Generative Loss: 7.493326663970947]\n",
      "-----------------------------------------------------------\n",
      "Iter: 472   Total Loss: 10.9386625289917   Gen Loss: 6.820058822631836   Contr Loss: 3.2948827743530273\n",
      "-----------------------------------------------------------\n",
      "Iter: 473   Total Loss: 11.116917610168457   Gen Loss: 6.99946928024292   Contr Loss: 3.2939586639404297\n",
      "-----------------------------------------------------------\n",
      "Iter: 474   Total Loss: 10.423585891723633   Gen Loss: 6.226014614105225   Contr Loss: 3.3580572605133057\n",
      "-----------------------------------------------------------\n",
      "Iter: 475   Total Loss: 12.556571960449219   Gen Loss: 8.199325561523438   Contr Loss: 3.48579740524292\n",
      "-----------------------------------------------------------\n",
      "Iter: 476   Total Loss: 10.574457168579102   Gen Loss: 6.991756916046143   Contr Loss: 2.8661606311798096\n",
      "-----------------------------------------------------------\n",
      "Iter: 477   Total Loss: 11.214540481567383   Gen Loss: 7.083578109741211   Contr Loss: 3.304769992828369\n",
      "-----------------------------------------------------------\n",
      "Iter: 478   Total Loss: 11.105765342712402   Gen Loss: 7.364139080047607   Contr Loss: 2.9933009147644043\n",
      "-----------------------------------------------------------\n",
      "Iter: 479   Total Loss: 11.621630668640137   Gen Loss: 7.3943376541137695   Contr Loss: 3.3818345069885254\n",
      "-----------------------------------------------------------\n",
      "Iter: 480   Total Loss: 10.77374267578125   Gen Loss: 7.213558673858643   Contr Loss: 2.848147392272949\n",
      "-----------------------------------------------------------\n",
      "Iter: 481   Total Loss: 12.120490074157715   Gen Loss: 7.413280963897705   Contr Loss: 3.7657673358917236\n",
      "-----------------------------------------------------------\n",
      "Iter: 482   Total Loss: 12.197066307067871   Gen Loss: 7.900060653686523   Contr Loss: 3.4376044273376465\n",
      "-----------------------------------------------------------\n",
      "Iter: 483   Total Loss: 11.251331329345703   Gen Loss: 6.966806888580322   Contr Loss: 3.427618980407715\n",
      "-----------------------------------------------------------\n",
      "Iter: 484   Total Loss: 10.958992004394531   Gen Loss: 7.015435218811035   Contr Loss: 3.1548452377319336\n",
      "-----------------------------------------------------------\n",
      "Iter: 485   Total Loss: 10.78645133972168   Gen Loss: 7.090145587921143   Contr Loss: 2.957045078277588\n",
      "-----------------------------------------------------------\n",
      "Iter: 486   Total Loss: 11.042633056640625   Gen Loss: 7.111356258392334   Contr Loss: 3.1450209617614746\n",
      "-----------------------------------------------------------\n",
      "Iter: 487   Total Loss: 11.111913681030273   Gen Loss: 7.229550838470459   Contr Loss: 3.1058902740478516\n",
      "-----------------------------------------------------------\n",
      "Iter: 488   Total Loss: 10.803084373474121   Gen Loss: 6.980961322784424   Contr Loss: 3.0576982498168945\n",
      "-----------------------------------------------------------\n",
      "Iter: 489   Total Loss: 10.291854858398438   Gen Loss: 6.303471088409424   Contr Loss: 3.190707206726074\n",
      "-----------------------------------------------------------\n",
      "Iter: 490   Total Loss: 11.17049789428711   Gen Loss: 7.189246654510498   Contr Loss: 3.1850013732910156\n",
      "-----------------------------------------------------------\n",
      "Iter: 491   Total Loss: 11.171194076538086   Gen Loss: 7.251803398132324   Contr Loss: 3.135512113571167\n",
      "-----------------------------------------------------------\n",
      "Iter: 492   Total Loss: 11.207592010498047   Gen Loss: 7.279607772827148   Contr Loss: 3.142387866973877\n",
      "-----------------------------------------------------------\n",
      "Iter: 493   Total Loss: 11.220983505249023   Gen Loss: 7.235737323760986   Contr Loss: 3.1881966590881348\n",
      "-----------------------------------------------------------\n",
      "Iter: 494   Total Loss: 11.341322898864746   Gen Loss: 7.444530010223389   Contr Loss: 3.117434501647949\n",
      "-----------------------------------------------------------\n",
      "Iter: 495   Total Loss: 10.816115379333496   Gen Loss: 6.979025840759277   Contr Loss: 3.069671630859375\n",
      "-----------------------------------------------------------\n",
      "Iter: 496   Total Loss: 11.171895027160645   Gen Loss: 7.117551326751709   Contr Loss: 3.2434749603271484\n",
      "-----------------------------------------------------------\n",
      "Iter: 497   Total Loss: 11.392659187316895   Gen Loss: 6.930124282836914   Contr Loss: 3.570028066635132\n",
      "-----------------------------------------------------------\n",
      "Iter: 498   Total Loss: 11.460092544555664   Gen Loss: 7.304816722869873   Contr Loss: 3.324221134185791\n",
      "-----------------------------------------------------------\n",
      "Iter: 499   Total Loss: 11.020065307617188   Gen Loss: 7.045109748840332   Contr Loss: 3.179964780807495\n",
      "-----------------------------------------------------------\n",
      "Iter: 500   Total Loss: 11.520284652709961   Gen Loss: 7.364077091217041   Contr Loss: 3.3249664306640625\n",
      "-----------------------------------------------------------\n",
      "Iter: 501   Total Loss: 12.059925079345703   Gen Loss: 7.959604740142822   Contr Loss: 3.280256748199463\n",
      "-----------------------------------------------------------\n",
      "Iter: 502   Total Loss: 11.214195251464844   Gen Loss: 7.11094331741333   Contr Loss: 3.282601833343506\n",
      "-----------------------------------------------------------\n",
      "Iter: 503   Total Loss: 11.941217422485352   Gen Loss: 7.634181022644043   Contr Loss: 3.445629596710205\n",
      "-----------------------------------------------------------\n",
      "Iter: 504   Total Loss: 11.354172706604004   Gen Loss: 7.319304943084717   Contr Loss: 3.2278943061828613\n",
      "-----------------------------------------------------------\n",
      "Iter: 505   Total Loss: 11.389345169067383   Gen Loss: 7.661359786987305   Contr Loss: 2.9823880195617676\n",
      "-----------------------------------------------------------\n",
      "Iter: 506   Total Loss: 11.291182518005371   Gen Loss: 7.47052526473999   Contr Loss: 3.056525945663452\n",
      "-----------------------------------------------------------\n",
      "Iter: 507   Total Loss: 10.550673484802246   Gen Loss: 6.625923156738281   Contr Loss: 3.1398003101348877\n",
      "-----------------------------------------------------------\n",
      "Iter: 508   Total Loss: 10.669544219970703   Gen Loss: 6.523233890533447   Contr Loss: 3.3170483112335205\n",
      "-----------------------------------------------------------\n",
      "Iter: 509   Total Loss: 11.128366470336914   Gen Loss: 7.33016300201416   Contr Loss: 3.038562774658203\n",
      "-----------------------------------------------------------\n",
      "Iter: 510   Total Loss: 10.474031448364258   Gen Loss: 6.647551536560059   Contr Loss: 3.0611836910247803\n",
      "-----------------------------------------------------------\n",
      "Iter: 511   Total Loss: 10.805551528930664   Gen Loss: 6.957437515258789   Contr Loss: 3.078491687774658\n",
      "-----------------------------------------------------------\n",
      "Iter: 512   Total Loss: 11.04146957397461   Gen Loss: 6.910613536834717   Contr Loss: 3.304685115814209\n",
      "-----------------------------------------------------------\n",
      "Iter: 513   Total Loss: 11.610736846923828   Gen Loss: 7.446275234222412   Contr Loss: 3.331569194793701\n",
      "-----------------------------------------------------------\n",
      "Iter: 514   Total Loss: 11.441507339477539   Gen Loss: 7.3836798667907715   Contr Loss: 3.2462620735168457\n",
      "-----------------------------------------------------------\n",
      "Iter: 515   Total Loss: 10.511205673217773   Gen Loss: 6.497633457183838   Contr Loss: 3.21085786819458\n",
      "-----------------------------------------------------------\n",
      "Iter: 516   Total Loss: 10.77960205078125   Gen Loss: 7.192724704742432   Contr Loss: 2.869502067565918\n",
      "-----------------------------------------------------------\n",
      "Iter: 517   Total Loss: 11.089930534362793   Gen Loss: 6.983623504638672   Contr Loss: 3.285045623779297\n",
      "-----------------------------------------------------------\n",
      "Iter: 518   Total Loss: 11.125732421875   Gen Loss: 7.077307224273682   Contr Loss: 3.2387397289276123\n",
      "-----------------------------------------------------------\n",
      "Iter: 519   Total Loss: 11.748741149902344   Gen Loss: 7.324184417724609   Contr Loss: 3.5396459102630615\n",
      "-----------------------------------------------------------\n",
      "Iter: 520   Total Loss: 10.984898567199707   Gen Loss: 6.878634452819824   Contr Loss: 3.2850112915039062\n",
      "-----------------------------------------------------------\n",
      "Iter: 521   Total Loss: 10.802399635314941   Gen Loss: 6.808650016784668   Contr Loss: 3.1949994564056396\n",
      "-----------------------------------------------------------\n",
      "Iter: 522   Total Loss: 11.014228820800781   Gen Loss: 6.950372219085693   Contr Loss: 3.2510855197906494\n",
      "-----------------------------------------------------------\n",
      "Iter: 523   Total Loss: 11.45235824584961   Gen Loss: 7.5739288330078125   Contr Loss: 3.102743625640869\n",
      "-----------------------------------------------------------\n",
      "Iter: 524   Total Loss: 10.439088821411133   Gen Loss: 6.667267322540283   Contr Loss: 3.0174574851989746\n",
      "-----------------------------------------------------------\n",
      "Iter: 525   Total Loss: 11.468988418579102   Gen Loss: 7.211153984069824   Contr Loss: 3.4062674045562744\n",
      "-----------------------------------------------------------\n",
      "Iter: 526   Total Loss: 11.33657455444336   Gen Loss: 7.256885528564453   Contr Loss: 3.26375150680542\n",
      "-----------------------------------------------------------\n",
      "Iter: 527   Total Loss: 11.15521240234375   Gen Loss: 7.120998859405518   Contr Loss: 3.2273712158203125\n",
      "-----------------------------------------------------------\n",
      "Iter: 528   Total Loss: 10.77952766418457   Gen Loss: 6.979573726654053   Contr Loss: 3.0399632453918457\n",
      "-----------------------------------------------------------\n",
      "Iter: 529   Total Loss: 11.221467971801758   Gen Loss: 7.162405490875244   Contr Loss: 3.2472496032714844\n",
      "-----------------------------------------------------------\n",
      "Iter: 530   Total Loss: 11.396276473999023   Gen Loss: 7.272711753845215   Contr Loss: 3.29885196685791\n",
      "-----------------------------------------------------------\n",
      "Iter: 531   Total Loss: 12.276596069335938   Gen Loss: 6.989287853240967   Contr Loss: 4.229846477508545\n",
      "-----------------------------------------------------------\n",
      "Iter: 532   Total Loss: 11.18895149230957   Gen Loss: 6.92063570022583   Contr Loss: 3.414652109146118\n",
      "-----------------------------------------------------------\n",
      "Iter: 533   Total Loss: 10.850775718688965   Gen Loss: 7.028543949127197   Contr Loss: 3.0577852725982666\n",
      "-----------------------------------------------------------\n",
      "Iter: 534   Total Loss: 10.632654190063477   Gen Loss: 6.5726189613342285   Contr Loss: 3.248027801513672\n",
      "-----------------------------------------------------------\n",
      "Iter: 535   Total Loss: 11.560108184814453   Gen Loss: 7.426876544952393   Contr Loss: 3.3065848350524902\n",
      "-----------------------------------------------------------\n",
      "Iter: 536   Total Loss: 11.440276145935059   Gen Loss: 7.125773906707764   Contr Loss: 3.45160174369812\n",
      "-----------------------------------------------------------\n",
      "Iter: 537   Total Loss: 11.41224479675293   Gen Loss: 7.327029228210449   Contr Loss: 3.2681727409362793\n",
      "-----------------------------------------------------------\n",
      "Iter: 538   Total Loss: 11.354305267333984   Gen Loss: 7.376293659210205   Contr Loss: 3.1824095249176025\n",
      "-----------------------------------------------------------\n",
      "Iter: 539   Total Loss: 10.829418182373047   Gen Loss: 6.651991844177246   Contr Loss: 3.3419413566589355\n",
      "-----------------------------------------------------------\n",
      "Iter: 540   Total Loss: 10.929518699645996   Gen Loss: 7.034438133239746   Contr Loss: 3.1160643100738525\n",
      "-----------------------------------------------------------\n",
      "Iter: 541   Total Loss: 11.67013168334961   Gen Loss: 7.43013858795166   Contr Loss: 3.391993999481201\n",
      "-----------------------------------------------------------\n",
      "Iter: 542   Total Loss: 10.983250617980957   Gen Loss: 7.045509338378906   Contr Loss: 3.150193214416504\n",
      "-----------------------------------------------------------\n",
      "Iter: 543   Total Loss: 11.015637397766113   Gen Loss: 6.908045768737793   Contr Loss: 3.2860734462738037\n",
      "-----------------------------------------------------------\n",
      "Iter: 544   Total Loss: 10.874396324157715   Gen Loss: 6.9278950691223145   Contr Loss: 3.157201051712036\n",
      "-----------------------------------------------------------\n",
      "Iter: 545   Total Loss: 10.769683837890625   Gen Loss: 6.61516809463501   Contr Loss: 3.3236122131347656\n",
      "-----------------------------------------------------------\n",
      "Iter: 546   Total Loss: 11.266740798950195   Gen Loss: 7.158023357391357   Contr Loss: 3.286973714828491\n",
      "-----------------------------------------------------------\n",
      "Iter: 547   Total Loss: 11.496650695800781   Gen Loss: 7.423373699188232   Contr Loss: 3.2586212158203125\n",
      "-----------------------------------------------------------\n",
      "Iter: 548   Total Loss: 11.053529739379883   Gen Loss: 7.116650104522705   Contr Loss: 3.149503707885742\n",
      "-----------------------------------------------------------\n",
      "Iter: 549   Total Loss: 11.216390609741211   Gen Loss: 7.171966075897217   Contr Loss: 3.2355399131774902\n",
      "-----------------------------------------------------------\n",
      "Iter: 550   Total Loss: 11.083702087402344   Gen Loss: 6.841158866882324   Contr Loss: 3.3940348625183105\n",
      "-----------------------------------------------------------\n",
      "Iter: 551   Total Loss: 11.18239688873291   Gen Loss: 6.991240978240967   Contr Loss: 3.3529248237609863\n",
      "-----------------------------------------------------------\n",
      "Iter: 552   Total Loss: 10.998924255371094   Gen Loss: 6.735117435455322   Contr Loss: 3.411045551300049\n",
      "-----------------------------------------------------------\n",
      "Iter: 553   Total Loss: 10.882698059082031   Gen Loss: 6.756319999694824   Contr Loss: 3.301102876663208\n",
      "-----------------------------------------------------------\n",
      "Iter: 554   Total Loss: 11.019989013671875   Gen Loss: 6.7891340255737305   Contr Loss: 3.384683609008789\n",
      "-----------------------------------------------------------\n",
      "Iter: 555   Total Loss: 11.004911422729492   Gen Loss: 7.095362186431885   Contr Loss: 3.1276395320892334\n",
      "-----------------------------------------------------------\n",
      "Iter: 556   Total Loss: 11.848225593566895   Gen Loss: 7.81043004989624   Contr Loss: 3.230236530303955\n",
      "-----------------------------------------------------------\n",
      "Iter: 557   Total Loss: 10.644604682922363   Gen Loss: 6.775076389312744   Contr Loss: 3.0956225395202637\n",
      "-----------------------------------------------------------\n",
      "Iter: 558   Total Loss: 11.191689491271973   Gen Loss: 7.250066757202148   Contr Loss: 3.1532981395721436\n",
      "-----------------------------------------------------------\n",
      "Iter: 559   Total Loss: 10.766068458557129   Gen Loss: 6.8437910079956055   Contr Loss: 3.137821912765503\n",
      "-----------------------------------------------------------\n",
      "Iter: 560   Total Loss: 10.63896369934082   Gen Loss: 6.669065475463867   Contr Loss: 3.1759181022644043\n",
      "-----------------------------------------------------------\n",
      "Iter: 561   Total Loss: 11.212976455688477   Gen Loss: 7.4843430519104   Contr Loss: 2.9829063415527344\n",
      "-----------------------------------------------------------\n",
      "Iter: 562   Total Loss: 11.635196685791016   Gen Loss: 7.4085822105407715   Contr Loss: 3.3812921047210693\n",
      "-----------------------------------------------------------\n",
      "Iter: 563   Total Loss: 11.125940322875977   Gen Loss: 7.251916885375977   Contr Loss: 3.0992188453674316\n",
      "-----------------------------------------------------------\n",
      "Iter: 564   Total Loss: 10.887998580932617   Gen Loss: 6.722009181976318   Contr Loss: 3.332791328430176\n",
      "-----------------------------------------------------------\n",
      "Iter: 565   Total Loss: 11.978302001953125   Gen Loss: 7.528922080993652   Contr Loss: 3.5595040321350098\n",
      "-----------------------------------------------------------\n",
      "Iter: 566   Total Loss: 10.532977104187012   Gen Loss: 6.783903121948242   Contr Loss: 2.9992589950561523\n",
      "-----------------------------------------------------------\n",
      "Iter: 567   Total Loss: 11.011219024658203   Gen Loss: 7.171276092529297   Contr Loss: 3.0719542503356934\n",
      "-----------------------------------------------------------\n",
      "Iter: 568   Total Loss: 11.270427703857422   Gen Loss: 7.294516086578369   Contr Loss: 3.1807291507720947\n",
      "-----------------------------------------------------------\n",
      "Iter: 569   Total Loss: 11.337044715881348   Gen Loss: 7.202355861663818   Contr Loss: 3.307751178741455\n",
      "-----------------------------------------------------------\n",
      "Iter: 570   Total Loss: 10.772991180419922   Gen Loss: 6.982992172241211   Contr Loss: 3.031999111175537\n",
      "-----------------------------------------------------------\n",
      "Iter: 571   Total Loss: 11.363717079162598   Gen Loss: 7.110140800476074   Contr Loss: 3.4028611183166504\n",
      "-----------------------------------------------------------\n",
      "Iter: 572   Total Loss: 11.178481101989746   Gen Loss: 7.211987018585205   Contr Loss: 3.1731951236724854\n",
      "-----------------------------------------------------------\n",
      "Iter: 573   Total Loss: 11.090683937072754   Gen Loss: 6.9513397216796875   Contr Loss: 3.3114752769470215\n",
      "-----------------------------------------------------------\n",
      "Iter: 574   Total Loss: 11.43460464477539   Gen Loss: 7.182217121124268   Contr Loss: 3.4019103050231934\n",
      "-----------------------------------------------------------\n",
      "Iter: 575   Total Loss: 11.576106071472168   Gen Loss: 7.5376129150390625   Contr Loss: 3.2307944297790527\n",
      "-----------------------------------------------------------\n",
      "Iter: 576   Total Loss: 10.449909210205078   Gen Loss: 6.711404323577881   Contr Loss: 2.9908034801483154\n",
      "-----------------------------------------------------------\n",
      "Iter: 577   Total Loss: 11.128880500793457   Gen Loss: 7.069842338562012   Contr Loss: 3.2472305297851562\n",
      "-----------------------------------------------------------\n",
      "Iter: 578   Total Loss: 11.132301330566406   Gen Loss: 7.295965671539307   Contr Loss: 3.069068670272827\n",
      "-----------------------------------------------------------\n",
      "Iter: 579   Total Loss: 11.428173065185547   Gen Loss: 7.256386756896973   Contr Loss: 3.337428569793701\n",
      "-----------------------------------------------------------\n",
      "Iter: 580   Total Loss: 11.54173469543457   Gen Loss: 7.29580545425415   Contr Loss: 3.3967435359954834\n",
      "-----------------------------------------------------------\n",
      "Iter: 581   Total Loss: 11.410927772521973   Gen Loss: 7.223308563232422   Contr Loss: 3.350095272064209\n",
      "-----------------------------------------------------------\n",
      "Iter: 582   Total Loss: 10.656366348266602   Gen Loss: 6.660406589508057   Contr Loss: 3.196767568588257\n",
      "-----------------------------------------------------------\n",
      "Iter: 583   Total Loss: 11.482263565063477   Gen Loss: 7.461867332458496   Contr Loss: 3.2163166999816895\n",
      "-----------------------------------------------------------\n",
      "Iter: 584   Total Loss: 10.878352165222168   Gen Loss: 7.109609127044678   Contr Loss: 3.0149946212768555\n",
      "-----------------------------------------------------------\n",
      "Iter: 585   Total Loss: 11.119707107543945   Gen Loss: 7.051523685455322   Contr Loss: 3.2545461654663086\n",
      "-----------------------------------------------------------\n",
      "Iter: 586   Total Loss: 10.735376358032227   Gen Loss: 7.035998821258545   Contr Loss: 2.9595017433166504\n",
      "-----------------------------------------------------------\n",
      "Iter: 587   Total Loss: 11.410356521606445   Gen Loss: 7.496788024902344   Contr Loss: 3.130854845046997\n",
      "-----------------------------------------------------------\n",
      "Iter: 588   Total Loss: 10.830455780029297   Gen Loss: 7.055412292480469   Contr Loss: 3.0200350284576416\n",
      "-----------------------------------------------------------\n",
      "Iter: 589   Total Loss: 10.866081237792969   Gen Loss: 6.974486827850342   Contr Loss: 3.1132755279541016\n",
      "-----------------------------------------------------------\n",
      "Iter: 590   Total Loss: 10.673543930053711   Gen Loss: 7.066401958465576   Contr Loss: 2.8857133388519287\n",
      "-----------------------------------------------------------\n",
      "Iter: 591   Total Loss: 10.283909797668457   Gen Loss: 6.390405654907227   Contr Loss: 3.1148033142089844\n",
      "-----------------------------------------------------------\n",
      "Iter: 592   Total Loss: 10.700233459472656   Gen Loss: 6.648216724395752   Contr Loss: 3.2416138648986816\n",
      "-----------------------------------------------------------\n",
      "Iter: 593   Total Loss: 11.455533981323242   Gen Loss: 7.322862148284912   Contr Loss: 3.3061375617980957\n",
      "-----------------------------------------------------------\n",
      "Iter: 594   Total Loss: 11.542657852172852   Gen Loss: 6.971703052520752   Contr Loss: 3.6567635536193848\n",
      "-----------------------------------------------------------\n",
      "Iter: 595   Total Loss: 10.799960136413574   Gen Loss: 6.7838850021362305   Contr Loss: 3.212860107421875\n",
      "-----------------------------------------------------------\n",
      "Iter: 596   Total Loss: 11.637556076049805   Gen Loss: 7.428768634796143   Contr Loss: 3.3670296669006348\n",
      "-----------------------------------------------------------\n",
      "Iter: 597   Total Loss: 10.728583335876465   Gen Loss: 7.042707443237305   Contr Loss: 2.9487009048461914\n",
      "-----------------------------------------------------------\n",
      "Iter: 598   Total Loss: 11.144405364990234   Gen Loss: 7.169426441192627   Contr Loss: 3.179982900619507\n",
      "-----------------------------------------------------------\n",
      "Iter: 599   Total Loss: 10.489870071411133   Gen Loss: 6.719984531402588   Contr Loss: 3.015908718109131\n",
      "-----------------------------------------------------------\n",
      "Iter: 600   Total Loss: 11.675777435302734   Gen Loss: 7.333613872528076   Contr Loss: 3.47373104095459\n",
      "-----------------------------------------------------------\n",
      "Iter: 601   Total Loss: 10.738826751708984   Gen Loss: 6.788216590881348   Contr Loss: 3.160487651824951\n",
      "-----------------------------------------------------------\n",
      "Iter: 602   Total Loss: 10.894163131713867   Gen Loss: 7.220083236694336   Contr Loss: 2.9392640590667725\n",
      "-----------------------------------------------------------\n",
      "Iter: 603   Total Loss: 11.52731704711914   Gen Loss: 7.123194694519043   Contr Loss: 3.5232975482940674\n",
      "-----------------------------------------------------------\n",
      "Iter: 604   Total Loss: 10.848877906799316   Gen Loss: 7.04232120513916   Contr Loss: 3.0452451705932617\n",
      "-----------------------------------------------------------\n",
      "Iter: 605   Total Loss: 10.593790054321289   Gen Loss: 6.737494945526123   Contr Loss: 3.085035800933838\n",
      "-----------------------------------------------------------\n",
      "Iter: 606   Total Loss: 11.450539588928223   Gen Loss: 7.49869966506958   Contr Loss: 3.1614718437194824\n",
      "-----------------------------------------------------------\n",
      "Iter: 607   Total Loss: 10.667180061340332   Gen Loss: 6.49960470199585   Contr Loss: 3.3340601921081543\n",
      "-----------------------------------------------------------\n",
      "Iter: 608   Total Loss: 11.49600601196289   Gen Loss: 7.041242599487305   Contr Loss: 3.5638110637664795\n",
      "-----------------------------------------------------------\n",
      "Iter: 609   Total Loss: 10.191908836364746   Gen Loss: 6.544093608856201   Contr Loss: 2.9182519912719727\n",
      "-----------------------------------------------------------\n",
      "Iter: 610   Total Loss: 11.230894088745117   Gen Loss: 6.899489879608154   Contr Loss: 3.4651236534118652\n",
      "-----------------------------------------------------------\n",
      "Iter: 611   Total Loss: 10.804329872131348   Gen Loss: 6.801274299621582   Contr Loss: 3.202444553375244\n",
      "-----------------------------------------------------------\n",
      "Iter: 612   Total Loss: 11.60811996459961   Gen Loss: 7.6743364334106445   Contr Loss: 3.147026538848877\n",
      "-----------------------------------------------------------\n",
      "Iter: 613   Total Loss: 10.982081413269043   Gen Loss: 6.735389232635498   Contr Loss: 3.3973536491394043\n",
      "-----------------------------------------------------------\n",
      "Iter: 614   Total Loss: 11.139923095703125   Gen Loss: 7.219284534454346   Contr Loss: 3.1365113258361816\n",
      "-----------------------------------------------------------\n",
      "Iter: 615   Total Loss: 10.936880111694336   Gen Loss: 6.909035682678223   Contr Loss: 3.222275495529175\n",
      "-----------------------------------------------------------\n",
      "Iter: 616   Total Loss: 11.169988632202148   Gen Loss: 7.021396636962891   Contr Loss: 3.318873643875122\n",
      "-----------------------------------------------------------\n",
      "Iter: 617   Total Loss: 10.72677993774414   Gen Loss: 6.986687183380127   Contr Loss: 2.992074489593506\n",
      "-----------------------------------------------------------\n",
      "Iter: 618   Total Loss: 11.203275680541992   Gen Loss: 7.470541000366211   Contr Loss: 2.98618745803833\n",
      "-----------------------------------------------------------\n",
      "Iter: 619   Total Loss: 11.162786483764648   Gen Loss: 7.149341106414795   Contr Loss: 3.210756540298462\n",
      "-----------------------------------------------------------\n",
      "Iter: 620   Total Loss: 10.502345085144043   Gen Loss: 6.635532855987549   Contr Loss: 3.093449592590332\n",
      "-----------------------------------------------------------\n",
      "Iter: 621   Total Loss: 10.995475769042969   Gen Loss: 7.032299041748047   Contr Loss: 3.170541763305664\n",
      "-----------------------------------------------------------\n",
      "Iter: 622   Total Loss: 10.75320816040039   Gen Loss: 6.85851526260376   Contr Loss: 3.1157546043395996\n",
      "-----------------------------------------------------------\n",
      "Iter: 623   Total Loss: 11.350364685058594   Gen Loss: 7.301967144012451   Contr Loss: 3.238718032836914\n",
      "-----------------------------------------------------------\n",
      "Iter: 624   Total Loss: 10.835865020751953   Gen Loss: 6.5709052085876465   Contr Loss: 3.4119672775268555\n",
      "-----------------------------------------------------------\n",
      "Iter: 625   Total Loss: 11.114652633666992   Gen Loss: 7.165359020233154   Contr Loss: 3.1594347953796387\n",
      "-----------------------------------------------------------\n",
      "Iter: 626   Total Loss: 11.22629165649414   Gen Loss: 7.215805530548096   Contr Loss: 3.2083892822265625\n",
      "-----------------------------------------------------------\n",
      "Iter: 627   Total Loss: 12.240641593933105   Gen Loss: 7.764843463897705   Contr Loss: 3.5806384086608887\n",
      "-----------------------------------------------------------\n",
      "Iter: 628   Total Loss: 9.363786697387695   Gen Loss: 6.500461101531982   Contr Loss: 2.290660858154297\n",
      "Epoch 4:  Train loss: 2.7788398265838623   Train Contrastive Loss: 3.2209558486938477   Train Generative Loss: 7.089165687561035]\n",
      "Epoch 4:  Val loss: 10.430991172790527   Val Contrastive Loss: 2.369096517562866   Val Generative Loss: 7.469622611999512]\n",
      "-----------------------------------------------------------\n",
      "Iter: 629   Total Loss: 11.60745906829834   Gen Loss: 7.57421875   Contr Loss: 3.2265923023223877\n",
      "-----------------------------------------------------------\n",
      "Iter: 630   Total Loss: 10.921941757202148   Gen Loss: 7.176866054534912   Contr Loss: 2.9960601329803467\n",
      "-----------------------------------------------------------\n",
      "Iter: 631   Total Loss: 10.84482479095459   Gen Loss: 6.968607425689697   Contr Loss: 3.1009738445281982\n",
      "-----------------------------------------------------------\n",
      "Iter: 632   Total Loss: 10.490500450134277   Gen Loss: 6.9168267250061035   Contr Loss: 2.8589389324188232\n",
      "-----------------------------------------------------------\n",
      "Iter: 633   Total Loss: 11.709722518920898   Gen Loss: 7.673732280731201   Contr Loss: 3.228792190551758\n",
      "-----------------------------------------------------------\n",
      "Iter: 634   Total Loss: 11.394570350646973   Gen Loss: 7.502690315246582   Contr Loss: 3.11350417137146\n",
      "-----------------------------------------------------------\n",
      "Iter: 635   Total Loss: 11.975179672241211   Gen Loss: 7.901922225952148   Contr Loss: 3.258606433868408\n",
      "-----------------------------------------------------------\n",
      "Iter: 636   Total Loss: 11.00375747680664   Gen Loss: 7.151120185852051   Contr Loss: 3.0821101665496826\n",
      "-----------------------------------------------------------\n",
      "Iter: 637   Total Loss: 10.961566925048828   Gen Loss: 7.0144572257995605   Contr Loss: 3.1576881408691406\n",
      "-----------------------------------------------------------\n",
      "Iter: 638   Total Loss: 11.095758438110352   Gen Loss: 7.139257907867432   Contr Loss: 3.1651999950408936\n",
      "-----------------------------------------------------------\n",
      "Iter: 639   Total Loss: 10.975589752197266   Gen Loss: 6.998050212860107   Contr Loss: 3.1820316314697266\n",
      "-----------------------------------------------------------\n",
      "Iter: 640   Total Loss: 11.036478996276855   Gen Loss: 7.150738716125488   Contr Loss: 3.1085920333862305\n",
      "-----------------------------------------------------------\n",
      "Iter: 641   Total Loss: 12.19063663482666   Gen Loss: 6.857030868530273   Contr Loss: 4.266884803771973\n",
      "-----------------------------------------------------------\n",
      "Iter: 642   Total Loss: 11.193185806274414   Gen Loss: 7.5729217529296875   Contr Loss: 2.8962111473083496\n",
      "-----------------------------------------------------------\n",
      "Iter: 643   Total Loss: 10.79463005065918   Gen Loss: 6.928876876831055   Contr Loss: 3.092602252960205\n",
      "-----------------------------------------------------------\n",
      "Iter: 644   Total Loss: 10.90672492980957   Gen Loss: 6.863270282745361   Contr Loss: 3.2347640991210938\n",
      "-----------------------------------------------------------\n",
      "Iter: 645   Total Loss: 10.868675231933594   Gen Loss: 6.800845146179199   Contr Loss: 3.2542643547058105\n",
      "-----------------------------------------------------------\n",
      "Iter: 646   Total Loss: 10.838937759399414   Gen Loss: 6.949331283569336   Contr Loss: 3.111685276031494\n",
      "-----------------------------------------------------------\n",
      "Iter: 647   Total Loss: 11.031999588012695   Gen Loss: 7.012261867523193   Contr Loss: 3.215790271759033\n",
      "-----------------------------------------------------------\n",
      "Iter: 648   Total Loss: 11.642098426818848   Gen Loss: 7.586059093475342   Contr Loss: 3.2448315620422363\n",
      "-----------------------------------------------------------\n",
      "Iter: 649   Total Loss: 10.389769554138184   Gen Loss: 6.6061787605285645   Contr Loss: 3.026872396469116\n",
      "-----------------------------------------------------------\n",
      "Iter: 650   Total Loss: 11.719082832336426   Gen Loss: 7.590538501739502   Contr Loss: 3.302835464477539\n",
      "-----------------------------------------------------------\n",
      "Iter: 651   Total Loss: 10.197834014892578   Gen Loss: 6.180347442626953   Contr Loss: 3.2139892578125\n",
      "-----------------------------------------------------------\n",
      "Iter: 652   Total Loss: 10.671337127685547   Gen Loss: 6.85731840133667   Contr Loss: 3.0512146949768066\n",
      "-----------------------------------------------------------\n",
      "Iter: 653   Total Loss: 10.84807014465332   Gen Loss: 6.798529624938965   Contr Loss: 3.2396328449249268\n",
      "-----------------------------------------------------------\n",
      "Iter: 654   Total Loss: 11.433708190917969   Gen Loss: 7.619039058685303   Contr Loss: 3.0517354011535645\n",
      "-----------------------------------------------------------\n",
      "Iter: 655   Total Loss: 10.77254581451416   Gen Loss: 6.779047012329102   Contr Loss: 3.1947989463806152\n",
      "-----------------------------------------------------------\n",
      "Iter: 656   Total Loss: 11.377151489257812   Gen Loss: 7.166989326477051   Contr Loss: 3.368129253387451\n",
      "-----------------------------------------------------------\n",
      "Iter: 657   Total Loss: 10.8602933883667   Gen Loss: 6.73084020614624   Contr Loss: 3.303562641143799\n",
      "-----------------------------------------------------------\n",
      "Iter: 658   Total Loss: 10.8062744140625   Gen Loss: 6.8047709465026855   Contr Loss: 3.2012031078338623\n",
      "-----------------------------------------------------------\n",
      "Iter: 659   Total Loss: 10.481243133544922   Gen Loss: 6.669468402862549   Contr Loss: 3.049419403076172\n",
      "-----------------------------------------------------------\n",
      "Iter: 660   Total Loss: 11.065801620483398   Gen Loss: 7.139984607696533   Contr Loss: 3.1406538486480713\n",
      "-----------------------------------------------------------\n",
      "Iter: 661   Total Loss: 10.563018798828125   Gen Loss: 6.802921295166016   Contr Loss: 3.008077621459961\n",
      "-----------------------------------------------------------\n",
      "Iter: 662   Total Loss: 11.015244483947754   Gen Loss: 7.175436019897461   Contr Loss: 3.0718469619750977\n",
      "-----------------------------------------------------------\n",
      "Iter: 663   Total Loss: 11.593116760253906   Gen Loss: 7.355006694793701   Contr Loss: 3.3904881477355957\n",
      "-----------------------------------------------------------\n",
      "Iter: 664   Total Loss: 10.806903839111328   Gen Loss: 7.087210655212402   Contr Loss: 2.975754737854004\n",
      "-----------------------------------------------------------\n",
      "Iter: 665   Total Loss: 11.353433609008789   Gen Loss: 7.283567428588867   Contr Loss: 3.2558929920196533\n",
      "-----------------------------------------------------------\n",
      "Iter: 666   Total Loss: 11.397113800048828   Gen Loss: 7.023410797119141   Contr Loss: 3.498962640762329\n",
      "-----------------------------------------------------------\n",
      "Iter: 667   Total Loss: 11.660197257995605   Gen Loss: 7.512698650360107   Contr Loss: 3.3179988861083984\n",
      "-----------------------------------------------------------\n",
      "Iter: 668   Total Loss: 10.699440002441406   Gen Loss: 6.963581085205078   Contr Loss: 2.988687515258789\n",
      "-----------------------------------------------------------\n",
      "Iter: 669   Total Loss: 10.943479537963867   Gen Loss: 6.972297191619873   Contr Loss: 3.1769461631774902\n",
      "-----------------------------------------------------------\n",
      "Iter: 670   Total Loss: 10.717597961425781   Gen Loss: 6.864294052124023   Contr Loss: 3.0826427936553955\n",
      "-----------------------------------------------------------\n",
      "Iter: 671   Total Loss: 11.462278366088867   Gen Loss: 7.485966205596924   Contr Loss: 3.1810498237609863\n",
      "-----------------------------------------------------------\n",
      "Iter: 672   Total Loss: 10.83415412902832   Gen Loss: 7.21735143661499   Contr Loss: 2.893442153930664\n",
      "-----------------------------------------------------------\n",
      "Iter: 673   Total Loss: 10.904476165771484   Gen Loss: 7.276123523712158   Contr Loss: 2.902681827545166\n",
      "-----------------------------------------------------------\n",
      "Iter: 674   Total Loss: 10.68224811553955   Gen Loss: 6.562548637390137   Contr Loss: 3.295759677886963\n",
      "-----------------------------------------------------------\n",
      "Iter: 675   Total Loss: 10.946295738220215   Gen Loss: 7.095312595367432   Contr Loss: 3.0807864665985107\n",
      "-----------------------------------------------------------\n",
      "Iter: 676   Total Loss: 10.887031555175781   Gen Loss: 6.8623785972595215   Contr Loss: 3.219722270965576\n",
      "-----------------------------------------------------------\n",
      "Iter: 677   Total Loss: 10.685768127441406   Gen Loss: 6.893171310424805   Contr Loss: 3.0340771675109863\n",
      "-----------------------------------------------------------\n",
      "Iter: 678   Total Loss: 11.21246337890625   Gen Loss: 7.115487098693848   Contr Loss: 3.277580976486206\n",
      "-----------------------------------------------------------\n",
      "Iter: 679   Total Loss: 11.131425857543945   Gen Loss: 7.167234420776367   Contr Loss: 3.1713531017303467\n",
      "-----------------------------------------------------------\n",
      "Iter: 680   Total Loss: 11.000665664672852   Gen Loss: 7.107392311096191   Contr Loss: 3.1146187782287598\n",
      "-----------------------------------------------------------\n",
      "Iter: 681   Total Loss: 11.327152252197266   Gen Loss: 7.350520610809326   Contr Loss: 3.181304931640625\n",
      "-----------------------------------------------------------\n",
      "Iter: 682   Total Loss: 10.586692810058594   Gen Loss: 6.5384039878845215   Contr Loss: 3.238631248474121\n",
      "-----------------------------------------------------------\n",
      "Iter: 683   Total Loss: 10.536819458007812   Gen Loss: 6.50180196762085   Contr Loss: 3.228013515472412\n",
      "-----------------------------------------------------------\n",
      "Iter: 684   Total Loss: 10.912269592285156   Gen Loss: 6.83724308013916   Contr Loss: 3.260021209716797\n",
      "-----------------------------------------------------------\n",
      "Iter: 685   Total Loss: 11.107305526733398   Gen Loss: 7.179842948913574   Contr Loss: 3.141969680786133\n",
      "-----------------------------------------------------------\n",
      "Iter: 686   Total Loss: 11.02797794342041   Gen Loss: 6.880012512207031   Contr Loss: 3.3183722496032715\n",
      "-----------------------------------------------------------\n",
      "Iter: 687   Total Loss: 11.513530731201172   Gen Loss: 7.452203273773193   Contr Loss: 3.2490625381469727\n",
      "-----------------------------------------------------------\n",
      "Iter: 688   Total Loss: 11.641706466674805   Gen Loss: 7.485151290893555   Contr Loss: 3.325244665145874\n",
      "-----------------------------------------------------------\n",
      "Iter: 689   Total Loss: 11.56087875366211   Gen Loss: 7.7525529861450195   Contr Loss: 3.0466606616973877\n",
      "-----------------------------------------------------------\n",
      "Iter: 690   Total Loss: 10.988972663879395   Gen Loss: 7.273332595825195   Contr Loss: 2.9725120067596436\n",
      "-----------------------------------------------------------\n",
      "Iter: 691   Total Loss: 10.88249683380127   Gen Loss: 6.891080856323242   Contr Loss: 3.1931328773498535\n",
      "-----------------------------------------------------------\n",
      "Iter: 692   Total Loss: 11.126739501953125   Gen Loss: 7.18782377243042   Contr Loss: 3.151132106781006\n",
      "-----------------------------------------------------------\n",
      "Iter: 693   Total Loss: 10.660383224487305   Gen Loss: 6.668683052062988   Contr Loss: 3.193359851837158\n",
      "-----------------------------------------------------------\n",
      "Iter: 694   Total Loss: 11.435029029846191   Gen Loss: 7.412825584411621   Contr Loss: 3.2177627086639404\n",
      "-----------------------------------------------------------\n",
      "Iter: 695   Total Loss: 10.515405654907227   Gen Loss: 7.047281742095947   Contr Loss: 2.7744994163513184\n",
      "-----------------------------------------------------------\n",
      "Iter: 696   Total Loss: 10.752175331115723   Gen Loss: 6.879928112030029   Contr Loss: 3.0977978706359863\n",
      "-----------------------------------------------------------\n",
      "Iter: 697   Total Loss: 11.336376190185547   Gen Loss: 7.210373401641846   Contr Loss: 3.3008017539978027\n",
      "-----------------------------------------------------------\n",
      "Iter: 698   Total Loss: 11.303414344787598   Gen Loss: 7.534715175628662   Contr Loss: 3.0149593353271484\n",
      "-----------------------------------------------------------\n",
      "Iter: 699   Total Loss: 10.932149887084961   Gen Loss: 7.149885177612305   Contr Loss: 3.0258114337921143\n",
      "-----------------------------------------------------------\n",
      "Iter: 700   Total Loss: 10.853333473205566   Gen Loss: 6.9204487800598145   Contr Loss: 3.146307945251465\n",
      "-----------------------------------------------------------\n",
      "Iter: 701   Total Loss: 11.14324951171875   Gen Loss: 6.99750280380249   Contr Loss: 3.3165972232818604\n",
      "-----------------------------------------------------------\n",
      "Iter: 702   Total Loss: 10.667049407958984   Gen Loss: 6.694760799407959   Contr Loss: 3.177830457687378\n",
      "-----------------------------------------------------------\n",
      "Iter: 703   Total Loss: 11.265936851501465   Gen Loss: 7.565990924835205   Contr Loss: 2.959956645965576\n",
      "-----------------------------------------------------------\n",
      "Iter: 704   Total Loss: 9.955587387084961   Gen Loss: 5.765835285186768   Contr Loss: 3.3518013954162598\n",
      "-----------------------------------------------------------\n",
      "Iter: 705   Total Loss: 10.352119445800781   Gen Loss: 6.603983402252197   Contr Loss: 2.9985086917877197\n",
      "-----------------------------------------------------------\n",
      "Iter: 706   Total Loss: 10.473478317260742   Gen Loss: 6.463949203491211   Contr Loss: 3.2076234817504883\n",
      "-----------------------------------------------------------\n",
      "Iter: 707   Total Loss: 10.54814338684082   Gen Loss: 6.898247241973877   Contr Loss: 2.919917345046997\n",
      "-----------------------------------------------------------\n",
      "Iter: 708   Total Loss: 11.017803192138672   Gen Loss: 7.006621360778809   Contr Loss: 3.2089457511901855\n",
      "-----------------------------------------------------------\n",
      "Iter: 709   Total Loss: 10.480453491210938   Gen Loss: 6.71361780166626   Contr Loss: 3.0134685039520264\n",
      "-----------------------------------------------------------\n",
      "Iter: 710   Total Loss: 10.809096336364746   Gen Loss: 6.994997024536133   Contr Loss: 3.0512795448303223\n",
      "-----------------------------------------------------------\n",
      "Iter: 711   Total Loss: 11.118306159973145   Gen Loss: 7.095300197601318   Contr Loss: 3.218404769897461\n",
      "-----------------------------------------------------------\n",
      "Iter: 712   Total Loss: 10.585906982421875   Gen Loss: 6.953864574432373   Contr Loss: 2.9056336879730225\n",
      "-----------------------------------------------------------\n",
      "Iter: 713   Total Loss: 10.71446418762207   Gen Loss: 6.878005504608154   Contr Loss: 3.069167137145996\n",
      "-----------------------------------------------------------\n",
      "Iter: 714   Total Loss: 10.90359878540039   Gen Loss: 7.32177734375   Contr Loss: 2.865457534790039\n",
      "-----------------------------------------------------------\n",
      "Iter: 715   Total Loss: 11.363019943237305   Gen Loss: 7.315648078918457   Contr Loss: 3.2378978729248047\n",
      "-----------------------------------------------------------\n",
      "Iter: 716   Total Loss: 9.952828407287598   Gen Loss: 6.069130897521973   Contr Loss: 3.1069579124450684\n",
      "-----------------------------------------------------------\n",
      "Iter: 717   Total Loss: 11.264107704162598   Gen Loss: 7.272778034210205   Contr Loss: 3.193063735961914\n",
      "-----------------------------------------------------------\n",
      "Iter: 718   Total Loss: 10.683658599853516   Gen Loss: 6.633076190948486   Contr Loss: 3.2404661178588867\n",
      "-----------------------------------------------------------\n",
      "Iter: 719   Total Loss: 11.122072219848633   Gen Loss: 7.185431957244873   Contr Loss: 3.1493120193481445\n",
      "-----------------------------------------------------------\n",
      "Iter: 720   Total Loss: 11.220222473144531   Gen Loss: 7.158294677734375   Contr Loss: 3.249541759490967\n",
      "-----------------------------------------------------------\n",
      "Iter: 721   Total Loss: 11.134247779846191   Gen Loss: 7.309503078460693   Contr Loss: 3.059795618057251\n",
      "-----------------------------------------------------------\n",
      "Iter: 722   Total Loss: 10.787052154541016   Gen Loss: 6.961162090301514   Contr Loss: 3.060711622238159\n",
      "-----------------------------------------------------------\n",
      "Iter: 723   Total Loss: 10.563796043395996   Gen Loss: 6.834282875061035   Contr Loss: 2.9836106300354004\n",
      "-----------------------------------------------------------\n",
      "Iter: 724   Total Loss: 11.333761215209961   Gen Loss: 7.329706192016602   Contr Loss: 3.2032439708709717\n",
      "-----------------------------------------------------------\n",
      "Iter: 725   Total Loss: 11.217269897460938   Gen Loss: 7.253049850463867   Contr Loss: 3.1713757514953613\n",
      "-----------------------------------------------------------\n",
      "Iter: 726   Total Loss: 11.089658737182617   Gen Loss: 7.205041885375977   Contr Loss: 3.107693910598755\n",
      "-----------------------------------------------------------\n",
      "Iter: 727   Total Loss: 11.319526672363281   Gen Loss: 7.150609493255615   Contr Loss: 3.3351340293884277\n",
      "-----------------------------------------------------------\n",
      "Iter: 728   Total Loss: 10.965209007263184   Gen Loss: 7.240828990936279   Contr Loss: 2.979504108428955\n",
      "-----------------------------------------------------------\n",
      "Iter: 729   Total Loss: 11.161726951599121   Gen Loss: 7.304433345794678   Contr Loss: 3.0858349800109863\n",
      "-----------------------------------------------------------\n",
      "Iter: 730   Total Loss: 10.882404327392578   Gen Loss: 7.039182186126709   Contr Loss: 3.074577808380127\n",
      "-----------------------------------------------------------\n",
      "Iter: 731   Total Loss: 11.27627182006836   Gen Loss: 7.455729961395264   Contr Loss: 3.0564332008361816\n",
      "-----------------------------------------------------------\n",
      "Iter: 732   Total Loss: 11.42246150970459   Gen Loss: 7.545265197753906   Contr Loss: 3.101757287979126\n",
      "-----------------------------------------------------------\n",
      "Iter: 733   Total Loss: 11.209615707397461   Gen Loss: 7.136057376861572   Contr Loss: 3.2588462829589844\n",
      "-----------------------------------------------------------\n",
      "Iter: 734   Total Loss: 11.158197402954102   Gen Loss: 7.229021072387695   Contr Loss: 3.143341302871704\n",
      "-----------------------------------------------------------\n",
      "Iter: 735   Total Loss: 11.178154945373535   Gen Loss: 7.048988342285156   Contr Loss: 3.303333282470703\n",
      "-----------------------------------------------------------\n",
      "Iter: 736   Total Loss: 11.395294189453125   Gen Loss: 7.337522029876709   Contr Loss: 3.246218204498291\n",
      "-----------------------------------------------------------\n",
      "Iter: 737   Total Loss: 10.591571807861328   Gen Loss: 6.895791530609131   Contr Loss: 2.9566245079040527\n",
      "-----------------------------------------------------------\n",
      "Iter: 738   Total Loss: 10.255677223205566   Gen Loss: 6.615496635437012   Contr Loss: 2.912144660949707\n",
      "-----------------------------------------------------------\n",
      "Iter: 739   Total Loss: 10.814180374145508   Gen Loss: 7.095648288726807   Contr Loss: 2.974825859069824\n",
      "-----------------------------------------------------------\n",
      "Iter: 740   Total Loss: 10.807748794555664   Gen Loss: 6.911800384521484   Contr Loss: 3.1167588233947754\n",
      "-----------------------------------------------------------\n",
      "Iter: 741   Total Loss: 10.623495101928711   Gen Loss: 6.907204627990723   Contr Loss: 2.9730324745178223\n",
      "-----------------------------------------------------------\n",
      "Iter: 742   Total Loss: 10.798358917236328   Gen Loss: 6.9077467918396   Contr Loss: 3.112489700317383\n",
      "-----------------------------------------------------------\n",
      "Iter: 743   Total Loss: 10.838152885437012   Gen Loss: 7.112737655639648   Contr Loss: 2.980332136154175\n",
      "-----------------------------------------------------------\n",
      "Iter: 744   Total Loss: 10.941280364990234   Gen Loss: 6.866011142730713   Contr Loss: 3.2602148056030273\n",
      "-----------------------------------------------------------\n",
      "Iter: 745   Total Loss: 10.412240982055664   Gen Loss: 6.608025074005127   Contr Loss: 3.043372631072998\n",
      "-----------------------------------------------------------\n",
      "Iter: 746   Total Loss: 10.787145614624023   Gen Loss: 6.995753765106201   Contr Loss: 3.033113956451416\n",
      "-----------------------------------------------------------\n",
      "Iter: 747   Total Loss: 11.034601211547852   Gen Loss: 6.944644451141357   Contr Loss: 3.271965503692627\n",
      "-----------------------------------------------------------\n",
      "Iter: 748   Total Loss: 10.638004302978516   Gen Loss: 7.151419162750244   Contr Loss: 2.7892680168151855\n",
      "-----------------------------------------------------------\n",
      "Iter: 749   Total Loss: 11.18733024597168   Gen Loss: 6.766114234924316   Contr Loss: 3.5369722843170166\n",
      "-----------------------------------------------------------\n",
      "Iter: 750   Total Loss: 11.459949493408203   Gen Loss: 7.318326473236084   Contr Loss: 3.3132987022399902\n",
      "-----------------------------------------------------------\n",
      "Iter: 751   Total Loss: 11.202678680419922   Gen Loss: 7.259881973266602   Contr Loss: 3.1542372703552246\n",
      "-----------------------------------------------------------\n",
      "Iter: 752   Total Loss: 11.074799537658691   Gen Loss: 7.247852802276611   Contr Loss: 3.0615575313568115\n",
      "-----------------------------------------------------------\n",
      "Iter: 753   Total Loss: 11.50542163848877   Gen Loss: 6.781659126281738   Contr Loss: 3.779010057449341\n",
      "-----------------------------------------------------------\n",
      "Iter: 754   Total Loss: 10.924392700195312   Gen Loss: 6.842717170715332   Contr Loss: 3.265340566635132\n",
      "-----------------------------------------------------------\n",
      "Iter: 755   Total Loss: 11.030413627624512   Gen Loss: 6.944371223449707   Contr Loss: 3.268833875656128\n",
      "-----------------------------------------------------------\n",
      "Iter: 756   Total Loss: 10.901636123657227   Gen Loss: 6.911420822143555   Contr Loss: 3.1921725273132324\n",
      "-----------------------------------------------------------\n",
      "Iter: 757   Total Loss: 11.387353897094727   Gen Loss: 6.967016220092773   Contr Loss: 3.5362696647644043\n",
      "-----------------------------------------------------------\n",
      "Iter: 758   Total Loss: 10.899304389953613   Gen Loss: 6.80627965927124   Contr Loss: 3.2744197845458984\n",
      "-----------------------------------------------------------\n",
      "Iter: 759   Total Loss: 11.372058868408203   Gen Loss: 7.035862922668457   Contr Loss: 3.468956708908081\n",
      "-----------------------------------------------------------\n",
      "Iter: 760   Total Loss: 11.28515911102295   Gen Loss: 7.245480060577393   Contr Loss: 3.231743335723877\n",
      "-----------------------------------------------------------\n",
      "Iter: 761   Total Loss: 11.529603958129883   Gen Loss: 7.437897205352783   Contr Loss: 3.2733657360076904\n",
      "-----------------------------------------------------------\n",
      "Iter: 762   Total Loss: 11.268211364746094   Gen Loss: 7.175992488861084   Contr Loss: 3.2737746238708496\n",
      "-----------------------------------------------------------\n",
      "Iter: 763   Total Loss: 11.348270416259766   Gen Loss: 7.26253604888916   Contr Loss: 3.268587350845337\n",
      "-----------------------------------------------------------\n",
      "Iter: 764   Total Loss: 10.873604774475098   Gen Loss: 7.393292427062988   Contr Loss: 2.784249782562256\n",
      "-----------------------------------------------------------\n",
      "Iter: 765   Total Loss: 10.54311752319336   Gen Loss: 6.603231906890869   Contr Loss: 3.1519083976745605\n",
      "-----------------------------------------------------------\n",
      "Iter: 766   Total Loss: 11.319267272949219   Gen Loss: 7.071071147918701   Contr Loss: 3.3985564708709717\n",
      "-----------------------------------------------------------\n",
      "Iter: 767   Total Loss: 11.35420036315918   Gen Loss: 7.733879089355469   Contr Loss: 2.896256923675537\n",
      "-----------------------------------------------------------\n",
      "Iter: 768   Total Loss: 10.829479217529297   Gen Loss: 7.042078971862793   Contr Loss: 3.0299198627471924\n",
      "-----------------------------------------------------------\n",
      "Iter: 769   Total Loss: 10.982064247131348   Gen Loss: 7.246835708618164   Contr Loss: 2.988182783126831\n",
      "-----------------------------------------------------------\n",
      "Iter: 770   Total Loss: 11.86363410949707   Gen Loss: 7.524244785308838   Contr Loss: 3.4715113639831543\n",
      "-----------------------------------------------------------\n",
      "Iter: 771   Total Loss: 10.573974609375   Gen Loss: 7.063389301300049   Contr Loss: 2.8084685802459717\n",
      "-----------------------------------------------------------\n",
      "Iter: 772   Total Loss: 11.30073070526123   Gen Loss: 7.49422550201416   Contr Loss: 3.0452041625976562\n",
      "-----------------------------------------------------------\n",
      "Iter: 773   Total Loss: 10.872743606567383   Gen Loss: 6.960779190063477   Contr Loss: 3.1295719146728516\n",
      "-----------------------------------------------------------\n",
      "Iter: 774   Total Loss: 10.83159065246582   Gen Loss: 7.017861843109131   Contr Loss: 3.050983190536499\n",
      "-----------------------------------------------------------\n",
      "Iter: 775   Total Loss: 10.358671188354492   Gen Loss: 6.287296772003174   Contr Loss: 3.2570996284484863\n",
      "-----------------------------------------------------------\n",
      "Iter: 776   Total Loss: 10.801763534545898   Gen Loss: 7.147159576416016   Contr Loss: 2.9236834049224854\n",
      "-----------------------------------------------------------\n",
      "Iter: 777   Total Loss: 10.807134628295898   Gen Loss: 7.1418890953063965   Contr Loss: 2.9321961402893066\n",
      "-----------------------------------------------------------\n",
      "Iter: 778   Total Loss: 10.8785400390625   Gen Loss: 7.273667335510254   Contr Loss: 2.8838984966278076\n",
      "-----------------------------------------------------------\n",
      "Iter: 779   Total Loss: 10.178262710571289   Gen Loss: 6.114363670349121   Contr Loss: 3.251119613647461\n",
      "-----------------------------------------------------------\n",
      "Iter: 780   Total Loss: 11.000323295593262   Gen Loss: 7.067300319671631   Contr Loss: 3.146418571472168\n"
     ]
    }
   ],
   "source": [
    "train(model=model, opt=None, data=train_loader, val_data=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
